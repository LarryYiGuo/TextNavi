#!/usr/bin/env python3
"""
æ·±åº¦åˆ†æStructureæ–‡ä»¶çš„æ£€ç´¢ç´¢å¼•å’ŒåŒ¹é…é—®é¢˜
æ£€æŸ¥æ˜¯å¦ä¸BLIP captionæœ‰è‰¯å¥½çš„åŒ¹é…åº¦
"""

import json
import os
from typing import Dict, List, Any

def load_jsonl_file(filepath: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ–‡ä»¶"""
    try:
        data = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        data.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
        return data
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        return []

def analyze_retrieval_indexing(data: Dict[str, Any], filename: str) -> Dict[str, Any]:
    """æ·±åº¦åˆ†ææ£€ç´¢ç´¢å¼•"""
    print(f"\nğŸ” æ·±åº¦åˆ†ææ£€ç´¢ç´¢å¼•: {filename}")
    print("=" * 80)
    
    analysis = {
        "filename": filename,
        "total_nodes": 0,
        "nodes_with_retrieval": 0,
        "retrieval_coverage": {},
        "index_terms_analysis": {},
        "tags_analysis": {},
        "potential_issues": []
    }
    
    # è°ƒè¯•ï¼šæ‰“å°æ•°æ®ç»“æ„
    print(f"ğŸ” æ•°æ®ç»“æ„é”®: {list(data.keys())}")
    
    if "topology" in data:
        topology = data["topology"]
        print(f"ğŸ” topologyé”®: {list(topology.keys())}")
        
        if "nodes" in topology:
            nodes = topology["nodes"]
            analysis["total_nodes"] = len(nodes)
            print(f"ğŸ“Š æ€»èŠ‚ç‚¹æ•°: {analysis['total_nodes']}")
            
            # åˆ†ææ¯ä¸ªèŠ‚ç‚¹çš„æ£€ç´¢å­—æ®µ
            for i, node in enumerate(nodes):
                node_id = node.get("id", f"node_{i}")
                print(f"\nğŸ“ èŠ‚ç‚¹ {i+1}: {node_id}")
                
                if "retrieval" in node:
                    analysis["nodes_with_retrieval"] += 1
                    retrieval = node["retrieval"]
                    
                    # åˆ†æindex_terms
                    if "index_terms" in retrieval:
                        index_terms = retrieval["index_terms"]
                        print(f"   ğŸ” index_terms ({len(index_terms)}): {index_terms}")
                        
                        # æ£€æŸ¥index_termsçš„è´¨é‡
                        for term in index_terms:
                            if term not in analysis["index_terms_analysis"]:
                                analysis["index_terms_analysis"][term] = 0
                            analysis["index_terms_analysis"][term] += 1
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æˆ–ç©ºå€¼
                        if len(index_terms) != len(set(index_terms)):
                            analysis["potential_issues"].append(f"èŠ‚ç‚¹ {node_id}: index_termsæœ‰é‡å¤")
                        
                        if any(not term.strip() for term in index_terms):
                            analysis["potential_issues"].append(f"èŠ‚ç‚¹ {node_id}: index_termsåŒ…å«ç©ºå€¼")
                    else:
                        print(f"   âš ï¸ ç¼ºå°‘index_terms")
                        analysis["potential_issues"].append(f"èŠ‚ç‚¹ {node_id}: ç¼ºå°‘index_terms")
                    
                    # åˆ†ætags
                    if "tags" in retrieval:
                        tags = retrieval["tags"]
                        print(f"   ğŸ·ï¸ tags ({len(tags)}): {tags}")
                        
                        # æ£€æŸ¥tagsçš„è´¨é‡
                        for tag in tags:
                            if tag not in analysis["tags_analysis"]:
                                analysis["tags_analysis"][tag] = 0
                            analysis["tags_analysis"][tag] += 1
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æˆ–ç©ºå€¼
                        if len(tags) != len(set(tags)):
                            analysis["potential_issues"].append(f"èŠ‚ç‚¹ {node_id}: tagsæœ‰é‡å¤")
                        
                        if any(not tag.strip() for tag in tags):
                            analysis["potential_issues"].append(f"èŠ‚ç‚¹ {node_id}: tagsåŒ…å«ç©ºå€¼")
                    else:
                        print(f"   âš ï¸ ç¼ºå°‘tags")
                        analysis["potential_issues"].append(f"èŠ‚ç‚¹ {node_id}: ç¼ºå°‘tags")
                    
                    # æ£€æŸ¥æ£€ç´¢å­—æ®µçš„å®Œæ•´æ€§
                    expected_fields = ["index_terms", "tags"]
                    missing_fields = [field for field in expected_fields if field not in retrieval]
                    if missing_fields:
                        analysis["potential_issues"].append(f"èŠ‚ç‚¹ {node_id}: ç¼ºå°‘å­—æ®µ {missing_fields}")
                else:
                    print(f"   âŒ ç¼ºå°‘retrievalå­—æ®µ")
                    analysis["potential_issues"].append(f"èŠ‚ç‚¹ {node_id}: ç¼ºå°‘retrievalå­—æ®µ")
        else:
            print("âŒ topologyä¸­ç¼ºå°‘nodeså­—æ®µ")
            analysis["potential_issues"].append("topologyä¸­ç¼ºå°‘nodeså­—æ®µ")
    else:
        print("âŒ æ–‡ä»¶ç¼ºå°‘topologyå­—æ®µ")
        analysis["potential_issues"].append("æ–‡ä»¶ç¼ºå°‘topologyå­—æ®µ")
    
    # è®¡ç®—è¦†ç›–ç‡
    if analysis["total_nodes"] > 0:
        retrieval_coverage = analysis["nodes_with_retrieval"] / analysis["total_nodes"]
        analysis["retrieval_coverage"] = {
            "nodes_with_retrieval": analysis["nodes_with_retrieval"],
            "total_nodes": analysis["total_nodes"],
            "coverage_percentage": retrieval_coverage * 100
        }
        print(f"\nğŸ“Š æ£€ç´¢å­—æ®µè¦†ç›–ç‡: {retrieval_coverage:.1%} ({analysis['nodes_with_retrieval']}/{analysis['total_nodes']})")
    
    return analysis

def analyze_global_retrieval(data: Dict[str, Any], filename: str) -> Dict[str, Any]:
    """åˆ†æå…¨å±€æ£€ç´¢å­—æ®µ"""
    print(f"\nğŸŒ åˆ†æå…¨å±€æ£€ç´¢å­—æ®µ: {filename}")
    print("=" * 60)
    
    analysis = {
        "filename": filename,
        "has_global_retrieval": False,
        "global_fields": {},
        "cnl_index_analysis": {},
        "keywords_analysis": {},
        "potential_issues": []
    }
    
    if "retrieval" in data:
        analysis["has_global_retrieval"] = True
        global_retrieval = data["retrieval"]
        analysis["global_fields"] = list(global_retrieval.keys())
        
        print(f"ğŸ” å…¨å±€æ£€ç´¢å­—æ®µ: {analysis['global_fields']}")
        
        # åˆ†æcnl_index
        if "cnl_index" in global_retrieval:
            cnl_index = global_retrieval["cnl_index"]
            print(f"ğŸ“ cnl_index ({len(cnl_index)} é¡¹):")
            for i, item in enumerate(cnl_index):
                print(f"   {i+1}. {item[:100]}...")
                analysis["cnl_index_analysis"][f"item_{i+1}"] = len(item)
        else:
            print("âš ï¸ ç¼ºå°‘cnl_index")
            analysis["potential_issues"].append("ç¼ºå°‘cnl_indexå­—æ®µ")
        
        # åˆ†ækeywords
        if "keywords" in global_retrieval:
            keywords = global_retrieval["keywords"]
            print(f"ğŸ”‘ keywords ({len(keywords)}): {keywords}")
            analysis["keywords_analysis"] = {
                "count": len(keywords),
                "keywords": keywords
            }
        else:
            print("âš ï¸ ç¼ºå°‘keywords")
            analysis["potential_issues"].append("ç¼ºå°‘keywordså­—æ®µ")
    else:
        print("âŒ ç¼ºå°‘å…¨å±€retrievalå­—æ®µ")
        analysis["potential_issues"].append("ç¼ºå°‘å…¨å±€retrievalå­—æ®µ")
    
    return analysis

def simulate_blip_matching(analysis: Dict[str, Any], filename: str) -> Dict[str, Any]:
    """æ¨¡æ‹ŸBLIP captionä¸æ£€ç´¢ç´¢å¼•çš„åŒ¹é…"""
    print(f"\nğŸ¤– æ¨¡æ‹ŸBLIP captionåŒ¹é…: {filename}")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿä¸€äº›å…¸å‹çš„BLIP caption
    test_captions = [
        "I am at the Maker Space entrance with glass doors behind me",
        "There is a yellow line on the floor starting from the entrance",
        "I can see a brown chair on the yellow line",
        "The yellow line bends left toward the windows",
        "I am near large windows with soft seats",
        "There is a TV screen and storage shelves",
        "I can see a small table with purple chairs",
        "There is an orange sofa against the wall"
    ]
    
    matching_results = []
    
    for caption in test_captions:
        caption_lower = caption.lower()
        print(f"\nğŸ“ æµ‹è¯•caption: {caption}")
        
        # æ£€æŸ¥ä¸index_termsçš„åŒ¹é…
        index_matches = []
        for term, count in analysis.get("index_terms_analysis", {}).items():
            if term.lower() in caption_lower:
                index_matches.append((term, count))
        
        # æ£€æŸ¥ä¸tagsçš„åŒ¹é…
        tag_matches = []
        for tag, count in analysis.get("tags_analysis", {}).items():
            if tag.lower() in caption_lower:
                tag_matches.append((tag, count))
        
        # æ£€æŸ¥ä¸keywordsçš„åŒ¹é…
        keyword_matches = []
        keywords = analysis.get("keywords_analysis", {}).get("keywords", [])
        for keyword in keywords:
            if keyword.lower() in caption_lower:
                keyword_matches.append(keyword)
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°
        total_matches = len(index_matches) + len(tag_matches) + len(keyword_matches)
        match_score = total_matches / 10.0  # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        
        print(f"   ğŸ” index_termsåŒ¹é…: {len(index_matches)} é¡¹")
        if index_matches:
            for term, count in index_matches[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"      - {term} (å‡ºç°{count}æ¬¡)")
        
        print(f"   ğŸ·ï¸ tagsåŒ¹é…: {len(tag_matches)} é¡¹")
        if tag_matches:
            for tag, count in tag_matches[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"      - {tag} (å‡ºç°{count}æ¬¡)")
        
        print(f"   ğŸ”‘ keywordsåŒ¹é…: {len(keyword_matches)} é¡¹")
        if keyword_matches:
            for keyword in keyword_matches[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"      - {keyword}")
        
        print(f"   ğŸ“Š æ€»åŒ¹é…åˆ†æ•°: {match_score:.3f}")
        
        matching_results.append({
            "caption": caption,
            "index_matches": len(index_matches),
            "tag_matches": len(tag_matches),
            "keyword_matches": len(keyword_matches),
            "total_matches": total_matches,
            "match_score": match_score
        })
    
    # è®¡ç®—å¹³å‡åŒ¹é…åˆ†æ•°
    if matching_results:
        avg_score = sum(r["match_score"] for r in matching_results) / len(matching_results)
        print(f"\nğŸ“Š å¹³å‡åŒ¹é…åˆ†æ•°: {avg_score:.3f}")
        
        if avg_score < 0.3:
            analysis["potential_issues"].append("BLIP captionåŒ¹é…åˆ†æ•°è¾ƒä½ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–æ£€ç´¢ç´¢å¼•")
    
    return matching_results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ·±åº¦åˆ†æStructureæ–‡ä»¶çš„æ£€ç´¢ç´¢å¼•")
    print("=" * 80)
    
    data_dir = "data"
    structure_files = [
        "Sense_A_Finetuned.fixed.jsonl",
        "Sense_B_Finetuned.fixed.jsonl"
    ]
    
    all_analyses = {}
    
    for filename in structure_files:
        filepath = os.path.join(data_dir, filename)
        if os.path.exists(filepath):
            data = load_jsonl_file(filepath)
            if data:
                print(f"\n{'='*80}")
                print(f"ğŸ“ åˆ†ææ–‡ä»¶: {filename}")
                print(f"{'='*80}")
                
                # åˆ†ææ£€ç´¢ç´¢å¼•
                retrieval_analysis = analyze_retrieval_indexing(data[0], filename)
                
                # åˆ†æå…¨å±€æ£€ç´¢å­—æ®µ
                global_analysis = analyze_global_retrieval(data[0], filename)
                
                # æ¨¡æ‹ŸBLIPåŒ¹é…
                matching_results = simulate_blip_matching(retrieval_analysis, filename)
                
                # åˆå¹¶åˆ†æç»“æœ
                all_analyses[filename] = {
                    "retrieval": retrieval_analysis,
                    "global": global_analysis,
                    "matching": matching_results
                }
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    
    # ç”Ÿæˆæ€»ç»“å’Œå»ºè®®
    print(f"\n{'='*80}")
    print("ğŸ“‹ åˆ†ææ€»ç»“å’Œå»ºè®®")
    print(f"{'='*80}")
    
    for filename, analysis in all_analyses.items():
        print(f"\nğŸ“ {filename}:")
        
        # æ£€ç´¢å­—æ®µè¦†ç›–ç‡
        retrieval = analysis["retrieval"]
        if "retrieval_coverage" in retrieval and retrieval["retrieval_coverage"]:
            coverage = retrieval["retrieval_coverage"]
            if "coverage_percentage" in coverage:
                print(f"   ğŸ“Š æ£€ç´¢å­—æ®µè¦†ç›–ç‡: {coverage['coverage_percentage']:.1f}%")
            else:
                print(f"   ğŸ“Š æ£€ç´¢å­—æ®µè¦†ç›–ç‡: æœªè®¡ç®—")
        else:
            print(f"   ğŸ“Š æ£€ç´¢å­—æ®µè¦†ç›–ç‡: æ— æ³•è®¡ç®—")
        
        # æ½œåœ¨é—®é¢˜
        all_issues = []
        if "potential_issues" in retrieval:
            all_issues.extend(retrieval["potential_issues"])
        if "potential_issues" in analysis["global"]:
            all_issues.extend(analysis["global"]["potential_issues"])
        
        if all_issues:
            print(f"   âš ï¸ å‘ç° {len(all_issues)} ä¸ªæ½œåœ¨é—®é¢˜:")
            for issue in all_issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"      - {issue}")
            if len(all_issues) > 5:
                print(f"      ... è¿˜æœ‰ {len(all_issues) - 5} ä¸ªé—®é¢˜")
        else:
            print(f"   âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
    
    print(f"\nğŸ¯ æ”¹è¿›å»ºè®®:")
    print("   1. ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰å®Œæ•´çš„retrievalå­—æ®µ")
    print("   2. ä¼˜åŒ–index_termsï¼Œä½¿å…¶æ›´è´´è¿‘BLIP caption")
    print("   3. å¢åŠ tagsçš„å¤šæ ·æ€§å’Œç›¸å…³æ€§")
    print("   4. å®šæœŸæ›´æ–°keywordsä»¥åŒ¹é…æ–°çš„ç”¨æˆ·æè¿°")
    
    print(f"\nğŸ“Š æ·±åº¦åˆ†æå®Œæˆï¼å…±åˆ†æäº† {len(all_analyses)} ä¸ªStructureæ–‡ä»¶")

if __name__ == "__main__":
    main()
