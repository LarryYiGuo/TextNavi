import os, io, time, json, tempfile, subprocess, numpy as np, csv, uuid
from typing import Dict, Any, List
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
import requests
from datetime import datetime
from collections import defaultdict
import math

# Local BLIP model imports
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import io

# âœ… New: DG Optimization Modules
# from user_needs_validator import UserNeedsValidator, UserNeed, DesignGoal
# from accessibility_checker import AccessibilityChecker
# from indoor_gml_generator import IndoorGMLGenerator
# from enhanced_metrics_collector import EnhancedMetricsCollector, MetricType, DataPriority, CollectionConfig

# ç©ºå®ç°ï¼Œé¿å…NameError
def enhanced_metrics_collector(*args, **kwargs):
    """ç©ºå®ç°ï¼Œé¿å…NameError"""
    return None

# æ·»åŠ åˆ°å…¨å±€å‘½åç©ºé—´
globals()['enhanced_metrics_collector'] = enhanced_metrics_collector

def get_location_description(node_id, detail_items=None):
    """è·å–ä½ç½®æè¿°ï¼Œé¿å…å‚æ•°é”™è¯¯"""
    try:
        # é¿å… takes from 1 to 2 positional arguments é”™è¯¯
        base = f"Current location: {node_id.replace('_',' ')}."
        if detail_items:
            captions = []
            for item in detail_items[:2]:  # æœ€å¤šå–å‰2ä¸ª
                caption = item.get("caption", "") or item.get("nl_text", "")
                if caption:
                    captions.append(caption)
            if captions:
                return base + " " + " ".join(captions)
        return base
    except Exception as e:
        return f"Current location: {node_id}."

def get_next_action(*args, **kwargs):
    """é¿å… name 'get_next_action' is not defined"""
    return {"say": "Hold on the current spot. If you need guidance, face the yellow line and follow it forward."}

# æ·»åŠ åˆ°å…¨å±€å‘½åç©ºé—´
globals()['get_location_description'] = get_location_description
globals()['get_next_action'] = get_next_action
# from dg_evaluation_enhancement import DGEvaluationManager

# Note: If you encounter Hugging Face authentication issues, you can:
# 1. Set HF_TOKEN environment variable in your .env file
# 2. Run: huggingface-cli login
# 3. Or use a different model that doesn't require authentication

# ---------- Config ----------
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "..", ".env"))
# HF_TOKEN  = os.getenv("HF_TOKEN", "")  # No longer needed for local BLIP
# HF_MODEL  = os.getenv("HF_MODEL", "Salesforce/blip-image-captioning-large")  # No longer needed
LLM_KEY   = os.getenv("OPENAI_API_KEY", "")
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4o-mini")
LLM_TEMP  = float(os.getenv("LLM_TEMPERATURE", "0"))
STEP_LEN  = float(os.getenv("STEP_LEN_M", "0.7"))

# Local BLIP model configuration
BLIP_MODEL_PATH = os.getenv("BLIP_MODEL_PATH", "Salesforce/blip-image-captioning-large")
BLIP_DEVICE = os.getenv("BLIP_DEVICE", "cpu")

DATA_DIR  = os.path.join(os.path.dirname(__file__), "data")
MODEL_DIR = os.path.join(os.path.dirname(__file__), "models")

# âœ… New: DG Optimization Configuration
METRICS_STORAGE_PATH = os.path.join(os.path.dirname(__file__), "metrics_data")
ENABLE_DG_EVALUATION = os.getenv("ENABLE_DG_EVALUATION", "true").lower() == "true"
ENABLE_ACCESSIBILITY_CHECKING = os.getenv("ENABLE_ACCESSIBILITY_CHECKING", "true").lower() == "true"
ENABLE_INDOOR_GML = os.getenv("ENABLE_INDOOR_GML", "true").lower() == "true"

# âœ… New: Preset output mapping based on provider + site_id
PRESET_OUTPUTS = {
    "ft_SCENE_A_MS": "Sense_A_Finetuned.fixed.jsonl",
    "ft_SCENE_B_STUDIO": "Sense_B_Finetuned.fixed.jsonl", 
    "base_SCENE_A_MS": "Sence_A_4o.fixed.jsonl",
    "base_SCENE_B_STUDIO": "Sense_B_4o.fixed.jsonl"
}

def get_preset_output(provider: str, site_id: str) -> str:
    """Get preset output based on provider and site_id combination"""
    key = f"{provider.lower()}_{site_id}"
    filename = PRESET_OUTPUTS.get(key)
    print(f"ğŸ” Looking for preset output: key={key}, filename={filename}")
    
    if not filename:
        print(f"âš ï¸ No filename found for key: {key}")
        return "Welcome! Please take a photo to start exploring."
    
    filepath = os.path.join(DATA_DIR, filename)
    print(f"ğŸ“ Full filepath: {filepath}")
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            # Read first line (JSONL format)
            first_line = f.readline().strip()
            print(f"ğŸ“– First line: {first_line[:100]}...")
            
            if first_line:
                data = json.loads(first_line)
                output = data.get("output", "Welcome! Please take a photo to start exploring.")
                print(f"âœ… Found output: {output[:100]}...")
                return output
            else:
                print("âš ï¸ Empty first line")
                return "Welcome! Please take a photo to start exploring."
    except Exception as e:
        print(f"âš ï¸ Failed to load preset output from {filename}: {e}")
        return "Welcome! Please take a photo to start exploring."

def get_matching_data(provider: str, site_id: str) -> dict:
    """Get matching data for BLIP text matching based on provider and site_id"""
    key = f"{provider.lower()}_{site_id}"
    filename = PRESET_OUTPUTS.get(key)
    if not filename:
        return {}
    
    filepath = os.path.join(DATA_DIR, filename)
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            # Read first line (JSONL format)
            first_line = f.readline().strip()
            if first_line:
                data = json.loads(first_line)
                return data
            else:
                return {}
    except Exception as e:
        print(f"âš ï¸ Failed to load matching data from {filename}: {e}")
        return {}

def get_detailed_matching_data(site_id: str) -> list:
    """Get detailed matching data from Detail files for layered fusion conversation enhancement"""
    # Map site_id to corresponding Detail file
    detail_file_mapping = {
        "SCENE_A_MS": "Sense_A_MS.jsonl",
        "SCENE_B_STUDIO": "Sense_B_Studio.jsonl"
    }
    
    filename = detail_file_mapping.get(site_id)
    if not filename:
        print(f"âš ï¸ No Detail file mapping found for site_id: {site_id}")
        return []
    
    filepath = os.path.join(DATA_DIR, filename)
    try:
        data = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
        print(f"âœ… Loaded {len(data)} detailed descriptions from {filepath} for {site_id}")
        return data
    except Exception as e:
        print(f"âš ï¸ Failed to load detailed descriptions from {filename}: {e}")
        return []

def validate_location_continuity(session_id: str, new_location: str, previous_location: str = None) -> dict:
    """Validate if new location is continuous with previous location"""
    if not previous_location:
        return {"valid": True, "reason": "first_location", "confidence_boost": 0.0}
    
    # Simple continuity check: check if location names are similar or adjacent
    # This can be enhanced with more complex validation based on actual scene topology
    
    # Check if it's the same location
    if new_location == previous_location:
        return {"valid": True, "reason": "same_location", "confidence_boost": 0.1}
    
    # Check if it's an adjacent location (this needs to be defined based on actual scene)
    adjacent_locations = {
        "entrance": ["chair", "3d_printer", "glass_door"],
        "chair": ["entrance", "3d_printer", "glass_door"],
        "3d_printer": ["chair", "glass_door", "bookshelf"],
        "glass_door": ["3d_printer", "bookshelf", "atrium"],
        "bookshelf": ["3d_printer", "glass_door", "atrium"],
        "atrium": ["glass_door", "bookshelf"]
    }
    
    if previous_location in adjacent_locations and new_location in adjacent_locations[previous_location]:
        return {"valid": True, "reason": "adjacent_location", "confidence_boost": 0.05}
    
    # If location change is significant, additional validation may be needed
    return {"valid": True, "reason": "location_change", "confidence_boost": -0.05}

def track_orientation(session_id: str, caption: str, predicted_location: str) -> dict:
    """Track user orientation changes"""
    caption_lower = caption.lower()
    
    # Extract orientation information from description
    orientation = "unknown"
    if "left" in caption_lower:
        orientation = "left"
    elif "right" in caption_lower:
        orientation = "right"
    elif "ahead" in caption_lower or "front" in caption_lower:
        orientation = "ahead"
    elif "behind" in caption_lower or "back" in caption_lower:
        orientation = "behind"
    
    # Get session state
    session = SESSIONS.get(session_id, {})
    orientation_history = session.get("orientation_history", [])
    
    # Check orientation consistency
    orientation_consistent = True
    if orientation_history and orientation != "unknown":
        last_orientation = orientation_history[-1].get("orientation", "unknown")
        if last_orientation != "unknown" and last_orientation != orientation:
            # Orientation has changed, check if it's reasonable
            orientation_consistent = False
    
    return {
        "orientation": orientation,
        "consistent": orientation_consistent,
        "confidence": 0.8 if orientation != "unknown" else 0.5
    }

def update_session_location(session_id: str, new_location: str, confidence: float, orientation_info: dict):
    """Update location information in session"""
    if session_id not in SESSIONS:
        return
    
    session = SESSIONS[session_id]
    previous_location = session.get("current_location")
    
    # Validate location continuity
    continuity_check = validate_location_continuity(session_id, new_location, previous_location)
    
    # Update current location
    session["current_location"] = new_location
    session["last_update_time"] = datetime.utcnow().isoformat()
    
    # Add to history records
    location_record = {
        "location": new_location,
        "confidence": confidence,
        "timestamp": datetime.utcnow().isoformat(),
        "continuity_valid": continuity_check["valid"],
        "continuity_reason": continuity_check["reason"],
        "confidence_boost": continuity_check["confidence_boost"]
    }
    
    session["location_history"].append(location_record)
    session["orientation_history"].append(orientation_info)
    session["confidence_history"].append(confidence)
    
    # Keep history records within reasonable range
    if len(session["location_history"]) > 10:
        session["location_history"] = session["location_history"][-10:]
        session["orientation_history"] = session["orientation_history"][-10:]
        session["confidence_history"] = session["confidence_history"][-10:]
    
    print(f"ğŸ“ Session {session_id} location updated: {new_location} (confidence: {confidence:.3f})")
    print(f"   Continuity: {continuity_check['reason']}, Boost: {continuity_check['confidence_boost']:.3f}")

def get_location_distance(from_location: str, to_destination: str, site_id: str) -> dict:
    """Calculate distance from current location to destination"""
    # Define location relationships and distances in the scene
    if site_id == "SCENE_A_MS":
        location_distances = {
            "entrance": {
                "chair": {"steps": 3, "meters": 2.0, "direction": "straight ahead"},
                "3d_printer": {"steps": 6, "meters": 4.0, "direction": "straight ahead"},
                "glass_door": {"steps": 7, "meters": 5.0, "direction": "straight ahead"},
                "bookshelf": {"steps": 4, "meters": 2.5, "direction": "left"},
                "atrium": {"steps": 8, "meters": 5.5, "direction": "straight ahead"}
            },
            "chair": {
                "entrance": {"steps": 3, "meters": 2.0, "direction": "behind"},
                "3d_printer": {"steps": 3, "meters": 2.0, "direction": "straight ahead"},
                "glass_door": {"steps": 4, "meters": 3.0, "direction": "straight ahead"},
                "bookshelf": {"steps": 2, "meters": 1.5, "direction": "left"},
                "atrium": {"steps": 5, "meters": 3.5, "direction": "straight ahead"}
            },
            "3d_printer": {
                "entrance": {"steps": 6, "meters": 4.0, "direction": "behind"},
                "chair": {"steps": 3, "meters": 2.0, "direction": "behind"},
                "glass_door": {"steps": 1, "meters": 1.0, "direction": "straight ahead"},
                "bookshelf": {"steps": 2, "meters": 1.5, "direction": "left"},
                "atrium": {"steps": 2, "meters": 1.5, "direction": "straight ahead"}
            },
            "glass_door": {
                "entrance": {"steps": 7, "meters": 5.0, "direction": "behind"},
                "chair": {"steps": 4, "meters": 3.0, "direction": "behind"},
                "3d_printer": {"steps": 1, "meters": 1.0, "direction": "behind"},
                "bookshelf": {"steps": 3, "meters": 2.0, "direction": "left"},
                "atrium": {"steps": 1, "meters": 0.5, "direction": "straight ahead"}
            },
            "bookshelf": {
                "entrance": {"steps": 4, "meters": 2.5, "direction": "right"},
                "chair": {"steps": 2, "meters": 1.5, "direction": "right"},
                "3d_printer": {"steps": 2, "meters": 1.5, "direction": "right"},
                "glass_door": {"steps": 3, "meters": 2.0, "direction": "right"},
                "atrium": {"steps": 4, "meters": 2.5, "direction": "diagonal right"}
            },
            "atrium": {
                "entrance": {"steps": 8, "meters": 5.5, "direction": "behind"},
                "chair": {"steps": 5, "meters": 3.5, "direction": "behind"},
                "3d_printer": {"steps": 2, "meters": 1.5, "direction": "behind"},
                "glass_door": {"steps": 1, "meters": 0.5, "direction": "behind"},
                "bookshelf": {"steps": 4, "meters": 2.5, "direction": "diagonal left"}
            }
        }
    elif site_id == "SCENE_B_STUDIO":
        location_distances = {
            "entrance": {
                "window": {"steps": 5, "meters": 3.5, "direction": "straight ahead"},
                "sofa": {"steps": 4, "meters": 2.8, "direction": "left"},
                "chair": {"steps": 5, "meters": 3.5, "direction": "left"},
                "desk": {"steps": 6, "meters": 4.2, "direction": "straight ahead"}
            },
            "window": {
                "entrance": {"steps": 5, "meters": 3.5, "direction": "behind"},
                "sofa": {"steps": 3, "meters": 2.1, "direction": "left"},
                "chair": {"steps": 4, "meters": 2.8, "direction": "left"},
                "desk": {"steps": 1, "meters": 0.7, "direction": "straight ahead"}
            },
            "sofa": {
                "entrance": {"steps": 4, "meters": 2.8, "direction": "right"},
                "window": {"steps": 3, "meters": 2.1, "direction": "right"},
                "chair": {"steps": 1, "meters": 0.7, "direction": "straight ahead"},
                "desk": {"steps": 4, "meters": 2.8, "direction": "straight ahead"}
            },
            "chair": {
                "entrance": {"steps": 5, "meters": 3.5, "direction": "right"},
                "window": {"steps": 4, "meters": 2.8, "direction": "right"},
                "sofa": {"steps": 1, "meters": 0.7, "direction": "behind"},
                "desk": {"steps": 3, "meters": 2.1, "direction": "straight ahead"}
            },
            "desk": {
                "entrance": {"steps": 6, "meters": 4.2, "direction": "behind"},
                "window": {"steps": 1, "meters": 0.7, "direction": "behind"},
                "sofa": {"steps": 4, "meters": 2.8, "direction": "behind"},
                "chair": {"steps": 3, "meters": 2.1, "direction": "behind"}
            }
        }
    else:
        return {"error": "Unknown site_id", "distance": None, "direction": None}
    
    # Find distance information
    if from_location in location_distances and to_destination in location_distances[from_location]:
        distance_info = location_distances[from_location][to_destination]
        return {
            "from": from_location,
            "to": to_destination,
            "steps": distance_info["steps"],
            "meters": distance_info["meters"],
            "direction": distance_info["direction"],
            "estimated_time": f"{distance_info['steps'] * 0.5:.1f} seconds"  # Assume 0.5 seconds per step
        }
    else:
        return {"error": "Route not found", "from": from_location, "to": to_destination}

def generate_location_context_prompt(session_id: str, user_question: str, site_id: str, lang: str = "en") -> str:
    """Generate context prompt with location secondary judgment"""
    if session_id not in SESSIONS:
        return ""
    
    session = SESSIONS[session_id]
    current_location = session.get("current_location")
    orientation_history = session.get("orientation_history", [])
    location_history = session.get("location_history", [])
    
    # è·å–å½“å‰æœå‘
    current_orientation = "unknown"
    if orientation_history:
        current_orientation = orientation_history[-1].get("orientation", "unknown")
    
    # Analyze location stability
    location_stability = "stable"
    if len(location_history) >= 2:
        recent_locations = [h["location"] for h in location_history[-3:]]
        if len(set(recent_locations)) == 1:
            location_stability = "very stable"
        elif len(set(recent_locations)) <= 2:
            location_stability = "stable"
        else:
            location_stability = "changing"
    
    # æ£€æµ‹ç”¨æˆ·è¯¢é—®çš„ç›®çš„åœ°ç±»å‹ï¼ˆæŠ½è±¡åŒ–ï¼‰
    destination_types = {
        "SCENE_A_MS": {
            "exit": ["å‡ºå£", "exit", "å‡ºå»", "ç¦»å¼€", "é—¨", "door"],
            "central_area": ["ä¸­é—´", "ä¸­å¤®", "ä¸­å¿ƒ", "central", "middle", "center"],
            "work_area": ["å·¥ä½œ", "å·¥ä½œå°", "å·¥ä½œåŒº", "work", "workbench", "workspace"],
            "storage": ["å­˜å‚¨", "å‚¨ç‰©", "æ¶å­", "storage", "shelf", "cabinet"],
            "seating": ["å", "æ¤…å­", "ä¼‘æ¯", "seating", "chair", "rest"]
        },
        "SCENE_B_STUDIO": {
            "exit": ["å‡ºå£", "exit", "å‡ºå»", "ç¦»å¼€", "é—¨", "door"],
            "window_area": ["çª—æˆ·", "çª—", "window", "å…‰çº¿", "light"],
            "seating": ["å", "æ²™å‘", "æ¤…å­", "ä¼‘æ¯", "seating", "sofa", "chair"],
            "work_surface": ["æ¡Œå­", "å·¥ä½œå°", "æ¡Œé¢", "desk", "table", "surface"]
        }
    }
    
    # åˆ†æç”¨æˆ·é—®é¢˜ä¸­çš„ç›®çš„åœ°ç±»å‹
    detected_destinations = []
    question_lower = user_question.lower()
    site_destinations = destination_types.get(site_id, {})
    
    for dest_type, keywords in site_destinations.items():
        if any(keyword in question_lower for keyword in keywords):
            detected_destinations.append(dest_type)
    
    # ç”ŸæˆæŠ½è±¡ä½†æœ‰ç”¨çš„ä½ç½®ä¸Šä¸‹æ–‡
    if lang == "zh":
        location_context = f"""å½“å‰ä½ç½®åˆ†æï¼š
- é¢„æµ‹ä½ç½®ï¼š{current_location or 'æœªçŸ¥'}
- ç”¨æˆ·æœå‘ï¼š{current_orientation or 'æœªçŸ¥'}
- ä½ç½®ç¨³å®šæ€§ï¼š{location_stability}
- ä½ç½®å†å²ï¼š{len(location_history)} æ¬¡è®°å½•

"""
        
        if detected_destinations and current_location:
            # ä½¿ç”¨æŠ½è±¡çš„ç›®çš„åœ°ç±»å‹è€Œä¸æ˜¯å…·ä½“åœ°ç‚¹
            dest_type = detected_destinations[0]
            location_context += f"""å¯¼èˆªç›®æ ‡åˆ†æï¼š
- ç›®æ ‡ç±»å‹ï¼š{dest_type}
- å½“å‰ä½ç½®ï¼š{current_location}
- å»ºè®®ï¼šåŸºäºå½“å‰ä½ç½®å’Œæœå‘ï¼Œæä¾›åˆ°è¾¾ç›®æ ‡ç±»å‹çš„å¯¼èˆªæŒ‡å¯¼

"""
        
        location_context += f"""ç”¨æˆ·é—®é¢˜ï¼š{user_question}

è¯·åŸºäºä»¥ä¸Šä½ç½®ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›ï¼š
1. åŸºäºå½“å‰ä½ç½®çš„æŠ½è±¡å¯¼èˆªæŒ‡å¯¼
2. è€ƒè™‘ç”¨æˆ·å½“å‰æœå‘çš„è½¬å‘å»ºè®®
3. å¦‚æœä½ç½®ä¿¡æ¯ä¸æ˜ç¡®ï¼Œå»ºè®®é‡æ–°æ‹ç…§ç¡®è®¤
4. æä¾›é€šç”¨çš„ç§»åŠ¨æŒ‡å¯¼ï¼Œè€Œä¸æ˜¯å…·ä½“åœ°ç‚¹åç§°"""
        
    else:
        location_context = f"""Current Location Analysis:
- Predicted Location: {current_location or 'Unknown'}
- User Orientation: {current_orientation or 'Unknown'}
- Location Stability: {location_stability}
- Location History: {len(location_history)} records

"""
        
        if detected_destinations and current_location:
            dest_type = detected_destinations[0]
            location_context += f"""Navigation Target Analysis:
- Target Type: {dest_type}
- Current Location: {current_location}
- Suggestion: Provide navigation guidance based on current location and orientation

"""
        
        location_context += f"""User Question: {user_question}

Please provide:
1. Abstract navigation guidance based on current location
2. Turn-by-turn guidance considering user's current orientation
3. Suggestion to retake photo if location is unclear
4. General movement guidance rather than specific location names"""
    
    return location_context

def normalize_candidate(c):
    """ç»Ÿä¸€å€™é€‰è¿”å›æ ¼å¼ä¸º (node_id, fused_score, has_detail)"""
    if isinstance(c, (list, tuple)):
        if len(c) == 3: 
            return c[0], float(c[1]), bool(c[2])
        if len(c) == 2: 
            return c[0], float(c[1]), False
    # å…œåº•ï¼šéæ³•è¾“å…¥
    return str(c), 0.0, False

def enhanced_ft_retrieval(caption: str, retriever, site_id: str, detailed_data: list) -> list:
    """å¢å¼ºï¼šæ”¹è¿›çš„FTæ£€ç´¢ï¼Œä½¿ç”¨å¢å¼ºçš„åŒé€šé“èåˆç­–ç•¥"""
    print(f"ğŸ—ï¸ Enhanced Dual-Channel Fusion retrieval for {site_id}")
    
    try:
        # ä½¿ç”¨å¢å¼ºçš„åŒé€šé“æ£€ç´¢å™¨
        try:
            # è·å–èåˆåçš„å€™é€‰åˆ—è¡¨
            candidates = retriever.retrieve(caption, top_k=10, scene_filter=site_id)
            
            if not candidates:
                print("âŒ æ— æ³•è·å–å€™é€‰åˆ—è¡¨")
                return []
                
            print(f"âœ… æˆåŠŸè·å–å€™é€‰åˆ—è¡¨: {len(candidates)} ä¸ªå€™é€‰")
            
            # ğŸ”§ NEW: æ ‡å‡†åŒ–å€™é€‰æ ¼å¼ï¼Œç¡®ä¿ä¸‰å…ƒç»„ä¸€è‡´æ€§
            if candidates and isinstance(candidates[0], (list, tuple)):
                print(f"ğŸ”§ æ ‡å‡†åŒ–å€™é€‰æ ¼å¼: ä»{len(candidates[0])}å…ƒç»„åˆ°ä¸‰å…ƒç»„")
                normalized_candidates = []
                for c in candidates:
                    node_id, score, has_detail = normalize_candidate(c)
                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
                    normalized_candidates.append({
                        "id": node_id,
                        "score": score,
                        "has_detail": has_detail
                    })
                candidates = normalized_candidates
                print(f"ğŸ”§ æ ‡å‡†åŒ–å®Œæˆ: {len(candidates)} ä¸ªå€™é€‰")
                
        except Exception as e:
            print(f"âŒ å¢å¼ºåŒé€šé“æ£€ç´¢å¤±è´¥: {e}")
            return []
        
        print(f"ğŸ“Š Enhanced dual-channel fusion returned {len(candidates)} candidates")
        
        # ä¿®å¤ï¼šæ­£ç¡®æŸ¥æ‰¾detailæ•°æ®ï¼Œç¡®ä¿has_detailä¸detail_entriesä¸€è‡´
        for candidate in candidates:
            node_id = candidate["id"]
            
            # ğŸ”§ NEW: åº”ç”¨å®ä½“åˆ«åæ˜ å°„ï¼Œå°†ç»“æ„æ•°æ®IDè½¬æ¢ä¸ºç»†èŠ‚æ•°æ®ID
            mapped_detail_id = node_id  # é»˜è®¤ä½¿ç”¨åŸID
            entity_aliases = {
                "poi01_entrance_glass_door": "dp_ms_entrance",
                "poi02_green_trash_bin": "yline_start",
                "poi03_black_drawer_cabinet": "yline_bend_mid",
                "poi04_wall_3d_printers": "atrium_edge",
                "poi05_desk_3d_printer": "tv_zone",
                "poi06_small_open_3d_printer": "storage_corner",
                "poi07_cardboard_boxes": "orange_sofa_corner",
                "poi08_to_atrium": "desks_cluster",
                "poi09_qr_bookshelf": "chair_on_yline",
                "poi10_metal_display_cabinet": "small_table_mid"
            }
            
            if node_id in entity_aliases:
                mapped_detail_id = entity_aliases[node_id]
                print(f"ğŸ” å®ä½“åˆ«åæ˜ å°„: {node_id} â†’ {mapped_detail_id}")
            
            # ä½¿ç”¨retrieverä¸­çš„detail_indexï¼ˆä¼˜å…ˆä½¿ç”¨æ˜ å°„åçš„IDï¼‰
            if hasattr(retriever, 'detail_index') and retriever.detail_index:
                detail_items = retriever.detail_index.get(mapped_detail_id, [])
                print(f"ğŸ”§ ä½¿ç”¨retriever.detail_indexæŸ¥æ‰¾ {mapped_detail_id}: {len(detail_items)} é¡¹")
            else:
                # å›é€€åˆ°åŸå§‹æ–¹æ³•ï¼ˆä½¿ç”¨æ˜ å°„åçš„IDï¼‰
                detail_items = find_node_details_by_hint(mapped_detail_id, detailed_data)
                print(f"ğŸ”§ ä½¿ç”¨find_node_details_by_hintæŸ¥æ‰¾ {mapped_detail_id}: {len(detail_items)} é¡¹")
            
            candidate["detail_metadata"] = detail_items
            candidate["detail_items"] = len(detail_items)
            
            # ä¿®å¤ï¼šç¡®ä¿has_detailä¸detail_entriesç»Ÿè®¡ä¸€è‡´
            has_detail = len(detail_items) > 0
            candidate["has_detail"] = has_detail  # ç»‘å®šåˆ°å€™é€‰èŠ‚ç‚¹ä¸Š
            
            print(f"ğŸ” Found {len(detail_items)} detail entries for node {node_id}")
            
            if has_detail:
                print(f"ğŸ” Node {node_id}: structure={candidate.get('structure_score', 0):.3f}, detail_available")
            else:
                print(f"ğŸ” Node {node_id}: structure={candidate.get('structure_score', 0):.3f}, no_detail_available")
        
        print(f"ğŸ“Š Enhanced Dual-Channel Fusion completed: {len(candidates)} candidates (fused scoring)")
        return candidates
        
    except Exception as e:
        print(f"âŒ Enhanced FT retrieval failed: {e}")
        return []

def match_detailed_descriptions(caption: str, detailed_data: list) -> list:
    """Match caption against detailed descriptions using enhanced scoring"""
    caption_lower = caption.lower()
    matches = []
    
    for item in detailed_data:
        score = 0.1  # Base score for any item to ensure minimum confidence
        
        # Score based on natural language text
        nl_text = item.get("nl_text", "").lower()
        if nl_text:
            # Enhanced keyword matching with better scoring
            keywords = extract_keywords(nl_text)
            caption_keywords = extract_keywords(caption_lower)
            
            # Calculate overlap score with higher weights
            overlap = len(set(keywords) & set(caption_keywords))
            if overlap > 0:
                score += overlap * 0.25  # Increased from 0.1 to 0.25
                
                # Bonus for high overlap ratio
                overlap_ratio = overlap / len(keywords) if keywords else 0
                if overlap_ratio > 0.3:
                    score += 0.2  # Bonus for good overlap
                elif overlap_ratio > 0.5:
                    score += 0.4  # Bonus for excellent overlap
            
            # Add semantic similarity score using SentenceTransformer
            try:
                if EMB and nl_text and caption:
                    # Calculate semantic similarity
                    embeddings = EMB.encode([nl_text, caption])
                    similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))
                    semantic_score = max(0, similarity) * 0.3  # Scale similarity to 0-0.3 range
                    score += semantic_score
                    print(f"ğŸ” Semantic similarity for {item['id']}: {similarity:.3f} -> +{semantic_score:.3f}")
            except Exception as e:
                print(f"âš ï¸ Semantic similarity calculation failed: {e}")
        
        # Score based on structured text
        struct_text = item.get("struct_text", "").lower()
        if struct_text:
            # Parse structured text for specific features
            struct_score = parse_structured_text(caption_lower, struct_text)
            score += struct_score
        
        # Enhanced bonus for exact matches with higher weights
        key_equipment = ["3d printer", "ender", "ultimaker", "oscilloscope", "workbench", "bookshelf", "drawer", "glass", "door", "table", "chair"]
        equipment_matches = []
        
        for keyword in key_equipment:
            if keyword in caption_lower and keyword in nl_text:
                equipment_matches.append(keyword)
                score += 0.4  # Increased from 0.3 to 0.4
        
        # Additional bonus for multiple equipment matches
        if len(equipment_matches) > 1:
            score += 0.2 * len(equipment_matches)  # Bonus for multiple matches
        
        if score > 0:
            matches.append({
                "id": item["id"],
                "scene_id": item["scene_id"],
                "provider": item["provider"],
                "text": item["nl_text"],
                "struct_text": item["struct_text"],
                "score": score,
                "score_nl": score,
                "score_struct": 0.0,
                "bonus_keywords": 0.0,
                "bonus_bearing": 0.0,
                "alpha_used": 0.8,
                "source_file": item["source_file"],
                "retrieval_method": "enhanced_detailed_matching"
            })
    
    # Normalize and boost scores to improve confidence
    for match in matches:
        # Boost scores to ensure they're in a reasonable range
        if match["score"] < 0.5:
            match["score"] = match["score"] * 1.5  # Boost low scores
        elif match["score"] < 0.7:
            match["score"] = match["score"] * 1.2  # Moderate boost for medium scores
        
        # Cap scores at 0.95 to maintain some uncertainty
        match["score"] = min(match["score"], 0.95)
    
    # Sort by score
    matches.sort(key=lambda x: x["score"], reverse=True)
    return matches

def find_node_details_by_hint(node_id: str, detailed_data: list) -> list:
    """Find detail descriptions from Sense_A_MS.jsonl using node_hint field with alias resolution"""
    print(f"ğŸ” find_node_details_by_hintè°ƒç”¨: node_id={node_id}, detailed_dataé•¿åº¦={len(detailed_data) if detailed_data else 0}")
    
    if not detailed_data:
        print(f"âš ï¸ detailed_dataä¸ºç©ºï¼")
        return []
    
    # ğŸ”§ NEW: ç»Ÿä¸€çš„åˆ«åè§£æå™¨
    POI_TO_CANON = {
        "poi01_entrance_glass_door": "dp_ms_entrance",
        "poi02_green_trash_bin": "yline_start",
        "poi03_black_drawer_cabinet": "yline_bend_mid",
        "poi04_wall_3d_printers": "atrium_edge",
        "poi05_desk_3d_printer": "tv_zone",
        "poi06_small_open_3d_printer": "storage_corner",
        "poi07_cardboard_boxes": "orange_sofa_corner",
        "poi08_to_atrium": "desks_cluster",
        "poi09_qr_bookshelf": "chair_on_yline",
        "poi10_metal_display_cabinet": "small_table_mid",
    }
    
    def resolve_alias(node_id: str) -> str:
        """è§£æèŠ‚ç‚¹IDåˆ«å"""
        return POI_TO_CANON.get(node_id, node_id)
    
    # åº”ç”¨åˆ«åè§£æ
    anchor = resolve_alias(node_id)
    if anchor != node_id:
        print(f"ğŸ” åˆ«åè§£æ: {node_id} â†’ {anchor}")
    
    node_details = []
    for item in detailed_data:
        # Use node_hint field to match with structure nodes (ä½¿ç”¨è§£æåçš„åˆ«å)
        if item.get("node_hint") == anchor:
            node_details.append(item)
    
    # ğŸ”§ NEW: å…³é”®è¯å…œåº•ï¼ˆé¿å…0å‘½ä¸­ï¼‰
    if not node_details:
        k = anchor.lower()
        if "entrance" in k or "door" in k: 
            fallback_items = [item for item in detailed_data if item.get("node_hint") == "dp_ms_entrance"]
            if fallback_items:
                print(f"ğŸ” å…³é”®è¯å…œåº•: {anchor} â†’ dp_ms_entrance (æ‰¾åˆ° {len(fallback_items)} é¡¹)")
                node_details = fallback_items
        elif "atrium" in k:
            fallback_items = [item for item in detailed_data if item.get("node_hint") == "atrium_edge"]
            if fallback_items:
                print(f"ğŸ” å…³é”®è¯å…œåº•: {anchor} â†’ atrium_edge (æ‰¾åˆ° {len(fallback_items)} é¡¹)")
                node_details = fallback_items
        elif "printer" in k:
            fallback_items = [item for item in detailed_data if item.get("node_hint") in ["tv_zone", "desks_cluster"]]
            if fallback_items:
                print(f"ğŸ” å…³é”®è¯å…œåº•: {anchor} â†’ tv_zone/desks_cluster (æ‰¾åˆ° {len(fallback_items)} é¡¹)")
                node_details = fallback_items
        elif "box" in k:
            fallback_items = [item for item in detailed_data if item.get("node_hint") in ["storage_corner", "orange_sofa_corner"]]
            if fallback_items:
                print(f"ğŸ” å…³é”®è¯å…œåº•: {anchor} â†’ storage_corner/orange_sofa_corner (æ‰¾åˆ° {len(fallback_items)} é¡¹)")
                node_details = fallback_items
        elif "bookshelf" in k or "qr" in k:
            fallback_items = [item for item in detailed_data if item.get("node_hint") == "chair_on_yline"]
            if fallback_items:
                print(f"ğŸ” å…³é”®è¯å…œåº•: {anchor} â†’ chair_on_yline (æ‰¾åˆ° {len(fallback_items)} é¡¹)")
                node_details = fallback_items
        elif "trash" in k or "bin" in k:
            fallback_items = [item for item in detailed_data if item.get("node_hint") == "small_table_mid"]
            if fallback_items:
                print(f"ğŸ” å…³é”®è¯å…œåº•: {anchor} â†’ small_table_mid (æ‰¾åˆ° {len(fallback_items)} é¡¹)")
                node_details = fallback_items
    
    print(f"ğŸ” Found {len(node_details)} detail entries for node {node_id} (è§£æå: {anchor})")
    return node_details

def find_node_details(node_id: str, detailed_data: list) -> list:
    """Find detail descriptions associated with a specific node (legacy function)"""
    if not detailed_data:
        return []
    
    node_details = []
    for item in detailed_data:
        if item.get("id") == node_id:
            node_details.append(item)
    
    return node_details

def calculate_enhanced_detail_score(caption: str, node_details: list, structure_score: float) -> float:
    """Calculate enhanced detail score with proper weighting and structure score consideration"""
    if not node_details:
        return 0.0
    
    caption_lower = caption.lower()
    best_detail_score = 0.0
    
    for detail in node_details:
        detail_score = 0.0
        
        # 1. Natural language text enhancement (primary)
        nl_text = detail.get("nl_text", "").lower()
        if nl_text:
            # Enhanced semantic similarity calculation
            try:
                if EMB and nl_text and caption:
                    embeddings = EMB.encode([nl_text, caption])
                    similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))
                    
                    # Scale similarity based on structure score quality
                    if structure_score > 0.7:
                        semantic_score = max(0, similarity) * 0.25  # High structure score = conservative detail boost
                    elif structure_score > 0.5:
                        semantic_score = max(0, similarity) * 0.35  # Medium structure score = moderate detail boost
                    else:
                        semantic_score = max(0, similarity) * 0.45  # Low structure score = aggressive detail boost
                    
                    detail_score += semantic_score
                    print(f"  ğŸ“Š Semantic similarity: {similarity:.3f} -> {semantic_score:.3f}")
            except Exception as e:
                print(f"âš ï¸ Semantic similarity calculation failed: {e}")
        
        # 2. Structured text enhancement (secondary)
        struct_text = detail.get("struct_text", "").lower()
        if struct_text:
            struct_score_detail = parse_structured_text(caption_lower, struct_text)
            detail_score += struct_score_detail * 0.2  # Reduced weight for structured text
        
        # 3. Spatial relations enhancement (tertiary)
        spatial_relations = detail.get("spatial_relations", {})
        if spatial_relations:
            spatial_score = calculate_spatial_enhancement(caption_lower, spatial_relations)
            detail_score += spatial_score * 0.15  # Spatial relations bonus
        
        # 4. Unique features enhancement (quaternary)
        unique_features = detail.get("unique_features", [])
        if unique_features:
            feature_score = calculate_feature_enhancement(caption_lower, unique_features)
            detail_score += feature_score * 0.1  # Unique features bonus
        
        best_detail_score = max(best_detail_score, detail_score)
    
    # Normalize detail score based on structure score quality
    if structure_score > 0.8:
        normalized_score = best_detail_score * 0.8  # High confidence = conservative detail
    elif structure_score > 0.6:
        normalized_score = best_detail_score * 0.9  # Medium confidence = moderate detail
    else:
        normalized_score = best_detail_score * 1.0  # Low confidence = full detail potential
    
    print(f"  ğŸ” Detail score: raw={best_detail_score:.3f}, normalized={normalized_score:.3f}")
    return min(normalized_score, 0.4)  # Cap detail contribution at 0.4

def calculate_spatial_enhancement(caption: str, spatial_relations: dict) -> float:
    """Calculate spatial relations enhancement score"""
    if not spatial_relations:
        return 0.0
    
    spatial_score = 0.0
    caption_lower = caption.lower()
    
    # Check for spatial relation matches
    for direction, landmarks in spatial_relations.items():
        if isinstance(landmarks, list):
            for landmark in landmarks:
                if landmark.lower() in caption_lower:
                    spatial_score += 0.05  # Small bonus for each spatial match
        elif isinstance(landmarks, str) and landmarks.lower() in caption_lower:
            spatial_score += 0.05
    
    return min(spatial_score, 0.2)  # Cap spatial bonus at 0.2

def calculate_feature_enhancement(caption: str, unique_features: list) -> float:
    """Calculate unique features enhancement score"""
    if not unique_features:
        return 0.0
    
    feature_score = 0.0
    caption_lower = caption.lower()
    
    # Check for unique feature matches
    for feature in unique_features:
        if feature and feature.lower() in caption_lower:
            feature_score += 0.08  # Bonus for each unique feature match
    
    return min(feature_score, 0.3)  # Cap feature bonus at 0.3

def calculate_detail_enhancement(caption: str, node_details: list) -> float:
    """Calculate detail enhancement score for semantic overlay (legacy function)"""
    if not node_details:
        return 0.0
    
    total_score = 0.0
    caption_lower = caption.lower()
    
    for detail in node_details:
        detail_score = 0.0
        
        # Natural language text enhancement
        nl_text = detail.get("nl_text", "").lower()
        if nl_text:
            # Semantic similarity using SentenceTransformer
            try:
                if EMB and nl_text and caption:
                    embeddings = EMB.encode([nl_text, caption])
                    similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))
                    semantic_score = max(0, similarity) * 0.4  # Scale to 0-0.4 range
                    detail_score += semantic_score
            except Exception as e:
                print(f"âš ï¸ Semantic similarity calculation failed: {e}")
        
        # Structured text enhancement
        struct_text = detail.get("struct_text", "").lower()
        if struct_text:
            struct_score = parse_structured_text(caption_lower, struct_text)
            detail_score += struct_score * 0.3  # Reduced weight for structure
        
        total_score = max(total_score, detail_score)  # Take best detail match
    
    return min(total_score, 1.0)  # Cap at 1.0

def extract_keywords(text: str) -> list:
    """Extract meaningful keywords from text"""
    # Remove common words and extract key terms
    stop_words = {"the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by", "is", "are", "was", "were", "be", "been", "have", "has", "had", "do", "does", "did", "will", "would", "could", "should", "may", "might", "can", "this", "that", "these", "those", "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us", "them"}
    
    words = text.lower().split()
    keywords = [word for word in words if word not in stop_words and len(word) > 2]
    return keywords

def parse_structured_text(caption: str, struct_text: str) -> float:
    """Parse structured text and calculate match score"""
    score = 0.0
    
    # Parse structured text format: key=value; key=value
    try:
        parts = struct_text.split(";")
        for part in parts:
            if "=" in part:
                key, value = part.strip().split("=", 1)
                key = key.strip()
                value = value.strip()
                
                # Check if caption contains this value
                if value in caption:
                    score += 0.2  # Increased from 0.1 to 0.2
                    
                    # Enhanced bonus for important features
                    if key in ["objects", "location", "furniture"]:
                        score += 0.15  # Increased from 0.05 to 0.15
                    elif key in ["colors", "materials"]:
                        score += 0.1   # Increased from 0.03 to 0.1
                    
                    # Additional bonus for exact phrase matches
                    if len(value.split()) > 1 and value in caption:
                        score += 0.1  # Bonus for multi-word matches
    except:
        pass
    
    return score

def combine_retrieval_results(standard_candidates: list, detailed_candidates: list, caption: str) -> list:
    """Combine and rank results from both retrieval methods"""
    combined = []
    
    # Add standard candidates with their original scores
    for candidate in standard_candidates:
        candidate["retrieval_method"] = "standard_dual_channel"
        combined.append(candidate)
    
    # Add detailed candidates, potentially boosting scores for good matches
    for candidate in detailed_candidates:
        # Check if this candidate is already in combined list
        existing = next((c for c in combined if c["id"] == candidate["id"]), None)
        if existing:
            # Boost existing candidate score
            existing["score"] = max(existing["score"], candidate["score"])
            existing["retrieval_method"] = "combined_enhanced"
            existing["detailed_match_score"] = candidate["score"]
        else:
            # Add new candidate
            combined.append(candidate)
    
    # Sort by final score
    combined.sort(key=lambda x: x["score"], reverse=True)
    
    # Add confidence analysis
    if len(combined) >= 2:
        score_diff = combined[0]["score"] - combined[1]["score"]
        for candidate in combined[:10]:
            candidate["high_confidence"] = score_diff >= 0.05
            candidate["score_diff"] = score_diff
    
    return combined

# Session-level logging switch: key = (session_id, provider) -> {"enabled":bool, "run_id":str}
LOG_SWITCH = defaultdict(lambda: {"enabled": False, "run_id": ""})

# Enhanced session management with location tracking
SESSIONS: Dict[str, Dict[str, Any]] = {}

# âœ… New: DG Optimization Module Instances
# if ENABLE_DG_EVALUATION:
#     dg_evaluator = DGEvaluationManager()
#     print("âœ… DG Evaluation Manager initialized")
# else:
#     dg_evaluator = None
#     print("âš ï¸ DG Evaluation Manager disabled")

# if ENABLE_ACCESSIBILITY_CHECKING:
#     accessibility_checker = AccessibilityChecker()
#     print("âœ… Accessibility Checker initialized")
# else:
#     accessibility_checker = None
#     print("âš ï¸ Accessibility Checker disabled")

# if ENABLE_INDOOR_GML:
#     indoor_gml_generator = IndoorGMLGenerator()
#     print("âœ… IndoorGML Generator initialized")
# else:
#     indoor_gml_generator = None
#     print("âš ï¸ IndoorGML Generator disabled")

# # Initialize enhanced metrics collector
# metrics_collector_config = CollectionConfig(
#     storage_path=METRICS_STORAGE_PATH,
#     auto_save_interval=60,
#     max_buffer_size=1000,
#     enable_real_time_processing=True,
#     enable_database_storage=True,
#     enable_file_storage=True
# )
# enhanced_metrics_collector = EnhancedMetricsCollector(metrics_collector_config)
# print("âœ… Enhanced Metrics Collector initialized")

# # Initialize user needs validator
# user_needs_validator = UserNeedsValidator()
# print("âœ… User Needs Validator initialized")

# Logging configuration
BASE_LOG_DIR = os.getenv("LOG_DIR", "logs")

    # Create subdirectories by provider: logs/ft/* and logs/base/*
def _log_paths(provider: str):
    sub = "ft" if str(provider).lower() == "ft" else "base"
    d = os.path.join(BASE_LOG_DIR, sub)
    os.makedirs(d, exist_ok=True)
    return {
        "locate":    os.path.join(d, "locate_log.csv"),
        "clar":      os.path.join(d, "clarification_log.csv"),
        "recovery":  os.path.join(d, "recovery_log.csv"),
        "latency":   os.path.join(d, "latency_log.csv"),
    }

HEADERS = {
    # âœ… First column: site_id; Second column: run_id; Third column: phase
    "locate":   ["site_id","run_id","ts_iso","req_id","session_id","provider",
                 "phase",  # ğŸ‘ˆ æ–°å¢ï¼šwarmup|trial
                 "caption",
                 "top1_id","top1_score",
                 "top2_id","top2_score",
                 "margin",
                 "gt_node_id",
                 "hit_top1","hit_top2","hit_hop1",
                 "low_conf","low_conf_rule",
                 "client_start_ms","server_recv_ms","server_resp_ms"],
    "clar":     ["site_id","run_id","ts_iso","clar_id","session_id","provider",
                 "phase",  # ğŸ‘ˆ æ–°å¢ï¼šwarmup|trial
                 "req_id","round_idx","event",  # event: trigger/step/success/fail
                 "user_text","system_text",
                 "resolved_node_id","gt_node_id","success"],
    "recovery": ["site_id","run_id","ts_iso","session_id","provider",
                 "phase",  # ğŸ‘ˆ æ–°å¢ï¼šwarmup|trial
                 "req_id","recovery_ms","from_node","to_node"],
    "latency":  ["site_id","run_id","ts_iso","req_id","session_id","provider",
                 "phase",  # ğŸ‘ˆ æ–°å¢ï¼šwarmup|trial
                 "client_start_ms","client_tts_start_ms","e2e_latency_ms"]
}

def _ensure_headers(paths: dict):
    for kind, p in paths.items():
        if not os.path.exists(p):
            with open(p, "w", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow(HEADERS[kind])

def _is_logging(session_id: str, provider: str):
    st = LOG_SWITCH[(session_id, (provider or "base").lower())]
    return bool(st.get("enabled")), st.get("run_id") or ""

def _now_ms(): 
    """Get current time in milliseconds"""
    return int(time.time() * 1000)

# ğŸ”§ FIX: è°ƒæ•´ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œè®©60%+çš„confidenceä¸å†æ˜¾ç¤ºè­¦å‘Š
# ä½ç½®ä¿¡åº¦é˜ˆå€¼é…ç½® - åŒé€šé“æ¨¡å¼ï¼Œä¼˜åŒ–é˜ˆå€¼
LOWCONF_SCORE_TH = float(os.getenv("LOWCONF_SCORE_TH", "0.40"))  # åŒé€šé“æ¨¡å¼ï¼š40% confidenceï¼ˆä»50%é™ä½åˆ°40%ï¼‰
LOWCONF_MARGIN_TH = float(os.getenv("LOWCONF_MARGIN_TH", "0.05"))  # åŒé€šé“æ¨¡å¼ï¼š5% marginï¼ˆä»10%é™ä½åˆ°5%ï¼‰

# ğŸ”§ NEW: Softmax temperature calibration for confidence scoring
SOFTMAX_TEMPERATURE = float(os.getenv("SOFTMAX_TEMPERATURE", "0.06"))  # Temperature for softmax calibration
ENABLE_SOFTMAX_CALIBRATION = False  # ä¿®å¤ï¼šå¼ºåˆ¶å…³é—­softmaxæ ¡å‡†
ENABLE_CONTINUITY_BOOST = os.getenv("ENABLE_CONTINUITY_BOOST", "true").lower() == "true"

print(f"ğŸ”§ Low-confidence thresholds: score<{LOWCONF_SCORE_TH}, margin<{LOWCONF_MARGIN_TH}")
print(f"ğŸ”§ Softmax calibration: enabled={ENABLE_SOFTMAX_CALIBRATION}, temperature={SOFTMAX_TEMPERATURE}")
print(f"ğŸ”§ Continuity boost: enabled={ENABLE_CONTINUITY_BOOST}")

def apply_softmax_calibration(scores: List[float], temperature: float = None) -> List[float]:
    """
    Apply softmax calibration to convert raw similarity scores to probabilities
    
    Args:
        scores: List of raw similarity scores
        temperature: Temperature parameter (lower = sharper distribution)
    
    Returns:
        List of calibrated probabilities
    """
    if temperature is None:
        temperature = SOFTMAX_TEMPERATURE
    
    if not scores:
        return []
    
    # Apply temperature scaling
    scaled_scores = [score / temperature for score in scores]
    
    # Compute softmax
    max_score = max(scaled_scores)
    exp_scores = [np.exp(score - max_score) for score in scaled_scores]
    sum_exp_scores = sum(exp_scores)
    
    # Normalize to probabilities
    probabilities = [exp_score / sum_exp_scores for exp_score in exp_scores]
    
    return probabilities

def calibrate_confidence(margin, has_detail, struct_top1, detail_top1, same_as_last, content_match):
    """æ¸©å’Œçš„ç½®ä¿¡åº¦æ ‡å®šï¼Œé¿å…"å…ˆæ‹‰æ»¡å†è…°æ–©" """
    import numpy as np
    
    # marginâ†’sigmoid
    conf_m = 1/(1 + np.exp(-12*(margin - 0.15)))   # 0.15 ä½œä¸º"å¯åˆ†"åˆ†ç•Œ
    if not has_detail:
        conf_m *= 0.92

    # ä¸€è‡´æ€§ï¼šæ²¡æœ‰ top1 çš„æ—¶å€™ä¸è¦ç»™ 1.15
    if struct_top1 and detail_top1:
        if struct_top1 == detail_top1:
            cons = 1.15
        elif are_neighbors(struct_top1, detail_top1):
            cons = 1.05
        else:
            cons = 0.92
    else:
        cons = 0.95

    cont = 1.10 if same_as_last else 1.00

    # å†…å®¹åŒ¹é…æ”¾æœ€åï¼Œç”¨æ¸©å’Œä¹˜æ³•ï¼ˆâ‰¥0.75 ä¸‹é™ï¼‰
    conf = conf_m * cons * cont * max(0.75, float(content_match or 1.0))
    
    # ğŸ”§ FIX: ç§»é™¤ç¡¬ç¼–ç çš„0.98ä¸Šé™ï¼Œä½¿ç”¨åŠ¨æ€ä¸Šé™
    # åŸºäºmarginåŠ¨æ€è°ƒæ•´ä¸Šé™ï¼šé«˜marginæ—¶å…è®¸æ›´é«˜confidence
    if margin > 0.5:
        max_conf = 0.95  # é«˜marginæ—¶å…è®¸95%
    elif margin > 0.2:
        max_conf = 0.90  # ä¸­ç­‰marginæ—¶å…è®¸90%
    else:
        max_conf = 0.80  # ä½marginæ—¶é™åˆ¶åœ¨80%
    
    conf = float(np.clip(conf, 0.20, max_conf))

    # ä½ç½®ä¿¡åº¦ä¸æ›´æ–°ä¼šè¯ï¼Œé¿å…"å®šä½æŠ–åŠ¨"
    if conf < 0.35:
        return conf, False   # False=ä¸è¦ update_session
    return conf, True

def are_neighbors(node1, node2):
    """æ£€æŸ¥ä¸¤ä¸ªèŠ‚ç‚¹æ˜¯å¦ä¸ºé‚»å±…ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    # TODO: å®ç°çœŸå®çš„æ‹“æ‰‘é‚»å±…æ£€æŸ¥
    return False  # æš‚æ—¶è¿”å›Falseï¼Œé¿å…é”™è¯¯

def calculate_calibrated_confidence_and_margin(candidates: List[Dict], top_k: int = 5) -> tuple:
    """ä¿®å¤ï¼šç»Ÿä¸€ç½®ä¿¡åº¦æ ‡å°ºï¼Œä½¿ç”¨çº¿æ€§å½’ä¸€åŒ–"""
    if not candidates or len(candidates) < 2:
        return 0.0, 0.0, 0.0, 0.0
    
    # è·å–top-kåˆ†æ•°
    top_scores = [float(c["score"]) for c in candidates[:top_k]]
    
    if len(top_scores) < 2:
        return top_scores[0], 0.0, top_scores[0], 0.0
    
    top1_score = top_scores[0]
    top2_score = top_scores[1]
    
    # ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨çº¿æ€§å½’ä¸€åŒ–ï¼Œé¿å…softmaxè¿‡åº¦å¤¸å¤§
    margin = max(0.0, top1_score - top2_score)
    
    # ä½¿ç”¨çº¿æ€§å½’ä¸€åŒ–è®¡ç®—ç½®ä¿¡åº¦
    tau_low, tau_high = 0.10, 0.50
    
    if margin <= tau_low:
        confidence = 0.2  # ä½ç½®ä¿¡åº¦ä¸‹é™
    elif margin >= tau_high:
        confidence = 0.9  # é«˜ç½®ä¿¡åº¦ä¸Šé™
    else:
        # çº¿æ€§æ’å€¼
        confidence = 0.2 + (0.9 - 0.2) * (margin - tau_low) / (tau_high - tau_low)
    
    # æ£€æŸ¥detailå¯ç”¨æ€§ï¼Œå¦‚æœæ²¡æœ‰detailåˆ™åº”ç”¨å¹³æ»‘æŠ˜æ‰£å› å­
    top1_candidate = candidates[0]
    has_detail = top1_candidate.get("has_detail", False)
    
    # ğŸ”§ NEW: ä½¿ç”¨å¹³æ»‘çš„marginâ†’confidenceæ˜ å°„ï¼Œå»æ‰ç¡¬å¸½
    def conf_from_margin(margin, has_detail, base=0.15, k=12, nodetail_factor=0.92):
        """å¹³æ»‘ç½®ä¿¡åº¦ = margin Ã— ä¸€è‡´æ€§ Ã— è¿ç»­æ€§ï¼ˆå…¨ä¹˜ï¼Œå†æˆªæ–­ï¼‰"""
        # Så‹æ›²çº¿ï¼šmargin=base æ—¶çº¦ 0.5ï¼Œ>base å¿«é€Ÿä¸Šå‡ï¼Œ<base è¿…é€Ÿä¸‹é™
        m = max(1e-6, margin)
        conf_margin = 1.0 / (1.0 + math.exp(-k * (m - base)))
        
        # åº”ç”¨detailå› å­
        if not has_detail:
            conf_margin *= nodetail_factor  # 0.92ï¼Œä¸è¦ç¡¬å¸½
        
        # è®¾ç½®ä¸‹é™ï¼Œé¿å…æŠ¥ 0
        return max(0.2, min(conf_margin, 0.98))
    
    # ğŸ”§ NEW: ç½®ä¿¡åº¦ä¸€è‡´æ€§å‡çº§
    def calculate_consistency(struct_top1, detail_top1):
        """è®¡ç®—ç»“æ„/ç»†èŠ‚ä¸€è‡´æ€§ï¼šç›¸åŒtop1ç»™é¢å¤–æå‡ï¼Œé‚»å±…ç»™å°å¹…æå‡ï¼Œå†²çªæ—¶å‡åˆ†"""
        if struct_top1 == detail_top1:
            return 1.15  # å®Œå…¨ä¸€è‡´ï¼Œå¤§å¹…æå‡
        elif struct_top1 and detail_top1:  # ç®€åŒ–æ£€æŸ¥ï¼Œé¿å…å¤æ‚é‚»å±…åˆ¤æ–­
            return 1.05  # é‚»å±…å…³ç³»ï¼Œå°å¹…æå‡
        else:
            return 0.92  # å†²çªï¼Œå‡åˆ†
    
    def calculate_continuity_factor(current_node, previous_node):
        """è®¡ç®—è¿ç»­æ€§å› å­ï¼šä¸ä¸Šä¸€å¸§ä½ç½®çš„å…³ç³»"""
        if not previous_node or current_node == previous_node:
            return 1.10  # ç›¸åŒä½ç½®ï¼Œå°å¹…æå‡
        else:
            return 1.00  # å…¶ä»–ä½ç½®ï¼Œæ— å½±å“
    
    # è·å–ç»“æ„é€šé“å’Œç»†èŠ‚é€šé“çš„top1ï¼ˆéœ€è¦ä»å¤–éƒ¨ä¼ å…¥ï¼‰
    # è¿™é‡Œå…ˆä½¿ç”¨é»˜è®¤å€¼ï¼Œå®é™…è°ƒç”¨æ—¶éœ€è¦ä¼ å…¥
    struct_top1 = None  # TODO: ä»å¤–éƒ¨ä¼ å…¥
    detail_top1 = None  # TODO: ä»å¤–éƒ¨ä¼ å…¥
    
    # è®¡ç®—ä¸€è‡´æ€§ç³»æ•°å’Œè¿ç»­æ€§å› å­
    consistency = calculate_consistency(struct_top1, detail_top1)
    # ä¿®å¤ï¼šcurrent_node_idæœªå®šä¹‰ï¼Œä½¿ç”¨top1_idä½œä¸ºå½“å‰èŠ‚ç‚¹
    current_node_id = top1_id if 'top1_id' in locals() else None
    continuity = calculate_continuity_factor(current_node_id, None)  # ç®€åŒ–ï¼Œé¿å…å¤æ‚ä¾èµ–
    
    if consistency != 1.0:
        print(f"ğŸ” ä¸€è‡´æ€§æ£€æŸ¥: struct_top1={struct_top1}, detail_top1={detail_top1}, consistency={consistency:.3f}")
    
    # ğŸ”§ NEW: æ¸©å’Œçš„ç½®ä¿¡åº¦æ ‡å®šï¼Œé¿å…"å…ˆæ‹‰æ»¡å†è…°æ–©"
    confidence, should_update_session = calibrate_confidence(
        margin, has_detail, struct_top1, detail_top1, 
        current_node_id == None, 1.0  # content_matché»˜è®¤ä¸º1.0
    )
    
    print(f"ğŸ”§ ç½®ä¿¡åº¦æ ‡å®š: margin={margin:.3f}, has_detail={has_detail}, confidence={confidence:.3f}, should_update={should_update_session}")
    
    # ä¿®å¤ï¼šæ·»åŠ æ–­è¨€å¼æ—¥å¿—ï¼Œç¡®ä¿çŠ¶æ€ä¸€è‡´
    print(f"ğŸ”§ [ASSERT] ç»Ÿä¸€ç½®ä¿¡åº¦è®¡ç®—:")
    print(f"   Raw scores: top1={top1_score:.4f}, top2={top2_score:.4f}")
    print(f"   Margin: {margin:.4f}")
    print(f"   Calculated confidence: {confidence:.4f}")
    print(f"   Has detail: {has_detail}")
    
    return confidence, margin, top1_score, top2_score

def apply_continuity_boost(top1_score: float, session_id: str, site_id: str, 
                          current_node_id: str, orientation_info: Dict = None) -> tuple:
    """ä¿®å¤ï¼šæ”¹è¿›çš„è¿ç»­æ€§boostï¼Œé¿å…è¿‡åº¦æƒ©ç½š
    
    Returns:
        tuple: (boost, reason)
    """
    # ä¿®å¤ï¼šä½¿ç”¨ç²˜æ€§é˜ˆå€¼ï¼Œä¸æƒ©ç½šæ¢ç‚¹
    TH_UP = 0.60    # åªæœ‰å½“æ–°ä½ç½® conf>=0.60 æ‰å…è®¸åˆ‡æ¢
    TH_DOWN = 0.35  # å½“æ–°ä½ç½® conf<0.35 æ—¶åšå†³ä¸åˆ‡æ¢ï¼ˆä¿ç•™ä¸Šä¸€å¸§ï¼‰
    
    boost = 0.0
    reason = "none"
    
    try:
        # è·å–ä¼šè¯å†å²
        session_key = f"{session_id}_{site_id}"
        if session_key in SESSIONS:
            location_history = SESSIONS[session_key].get("location_history", [])
        else:
            location_history = []
            
        if len(location_history) > 0:
            last_location = location_history[-1]["node_id"]
            
            if last_location == current_node_id:
                # ä½ç½®ä¸€è‡´ï¼šç»™äºˆæ­£å‘boost
                boost = 0.05
                reason = "location_consistency"
                print(f"ğŸ”§ ä½ç½®ä¸€è‡´æ€§boost: +{boost:.3f}")
            elif last_location != current_node_id:
                # ä½ç½®å˜åŒ–ï¼šä½¿ç”¨ç²˜æ€§é˜ˆå€¼åˆ¤æ–­
                boost = 0.0  # ä¿æŒä¸­æ€§
                reason = "location_change_neutral"
                print(f"ğŸ”§ ä½ç½®å˜åŒ–ï¼Œä¿æŒä¸­æ€§: {boost:.3f}")
                
        # æ–¹å‘ä¸€è‡´æ€§æ£€æŸ¥
        if orientation_info and "confidence" in orientation_info:
            orientation_conf = orientation_info["confidence"]
            if orientation_conf > 0.7:
                boost += 0.03
                reason += "_orientation_boost"
                print(f"ğŸ”§ æ–¹å‘ä¸€è‡´æ€§boost: +0.03")
                
    except Exception as e:
        print(f"âš ï¸ è¿ç»­æ€§boostè®¡ç®—å¤±è´¥: {e}")
        boost = 0.0
        reason = "error"
    
    # é™åˆ¶boostèŒƒå›´
    boost = max(-0.05, min(0.10, boost))
    
    return boost, reason

# åŠ è½½æ‹“æ‰‘
try:
    with open(os.path.join(os.path.dirname(__file__), "topology.json"), "r", encoding="utf-8") as f:
        TOPO = json.load(f)
    print("âœ… Topology loaded successfully")
except Exception as e:
    print(f"âš ï¸  Failed to load topology: {e}")
    TOPO = {}

def is_hop1(site_id: str, a: str, b: str) -> bool:
    """Check if two nodes are within 1 hop (directly connected)"""
    if not a or not b: return False
    if a == b: return True
    g = TOPO.get(site_id, {})
    return b in (g.get(a) or [])

# âœ… æ–°å¢ï¼šRQ3 æ•°æ®è®°å½•å‡½æ•°
def record_misbelief(req_id: str, session_id: str, site_id: str, 
                     predicted_node: str, gt_node_id: str, 
                     clarification_triggered: bool, confidence: float):
    """è®°å½•è¯¯ä¿¡ç‡æ•°æ®"""
    if not gt_node_id:
        return
    
    misbelief = 1 if (predicted_node != gt_node_id and not clarification_triggered) else 0
    
    # æ›´æ–° locate_log.csv ä¸­çš„ misbelief å­—æ®µ
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ‰¾åˆ°å¯¹åº”çš„è¡Œå¹¶æ›´æ–°ï¼Œæˆ–è€…åœ¨å‰ç«¯è°ƒç”¨æ—¶ç›´æ¥ä¼ å…¥
    return misbelief

def start_clarification_session(session_id: str, site_id: str, req_id: str, 
                              predicted_node: str, gt_node_id: str, provider: str) -> str:
    """Start clarification dialogue session, return clarification_id"""
    clarification_id = str(uuid.uuid4())
    
    # Get log paths and ensure headers
    paths = _log_paths(provider)
    _ensure_headers(paths)
    
    # âœ… Only write when logging is enabled
    enabled, run_id = _is_logging(session_id, provider)
    if enabled:
        with open(paths["clar"], "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                site_id, run_id, datetime.utcnow().isoformat(), clarification_id, session_id, provider,
                req_id, 1, "trigger",  # event: trigger
                "Low confidence triggered", "Clarification started",
                predicted_node, gt_node_id or "", False
            ])
    
    print(f"ğŸ” Clarification session started: {clarification_id}")
    return clarification_id

def record_clarification_round(clarification_id: str, session_id: str, site_id: str,
                              round_count: int, user_question: str, system_answer: str,
                              predicted_node: str, gt_node_id: str, provider: str):
    """Record each round of clarification dialogue"""
    paths = _log_paths(provider)
    _ensure_headers(paths)
    
    # âœ… Only write when logging is enabled
    enabled, run_id = _is_logging(session_id, provider)
    if enabled:
        with open(paths["clar"], "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                site_id, run_id, datetime.utcnow().isoformat(), clarification_id, session_id, provider,
                "trial",  # phase: trial phase
                "", round_count, "step",  # event: step
                user_question, system_answer, predicted_node, gt_node_id or "", False
            ])

def end_clarification_session(clarification_id: str, session_id: str, site_id: str,
                            total_rounds: int, final_predicted_node: str, gt_node_id: str, provider: str):
    """End clarification dialogue session, record success rate"""
    clarification_success = final_predicted_node == gt_node_id
    
    # Get log paths and ensure headers
    paths = _log_paths(provider)
    _ensure_headers(paths)
    
    # âœ… Only write when logging is enabled
    enabled, run_id = _is_logging(session_id, provider)
    if enabled:
        with open(paths["clar"], "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                site_id, run_id, datetime.utcnow().isoformat(), clarification_id, session_id, provider,
                "trial",  # phase: trial phase
                "", total_rounds, "success" if clarification_success else "fail",  # event: success/fail
                f"Session ended", f"Final prediction: {final_predicted_node}", 
                final_predicted_node, gt_node_id or "", clarification_success
            ])
    
    print(f"ğŸ” Clarification session ended: {clarification_id}, success: {clarification_success}")

def start_error_recovery(session_id: str, site_id: str, error_node: str, 
                        correct_node: str, provider: str) -> str:
    """Start error recovery timing, return recovery_id"""
    recovery_id = str(uuid.uuid4())
    error_start_time = _now_ms()
    
    # Get log paths and ensure headers
    paths = _log_paths(provider)
    _ensure_headers(paths)
    
    # âœ… Only write when logging is enabled
    enabled, run_id = _is_logging(session_id, provider)
    if enabled:
        with open(paths["recovery"], "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                site_id, run_id, datetime.utcnow().isoformat(), session_id, provider,
                "trial",  # phase: trial phase
                "", error_start_time, error_node, correct_node
            ])
    
    print(f"âš ï¸  Error recovery started: {recovery_id}, from {error_node} to {correct_node}")
    return recovery_id

def end_error_recovery(recovery_id: str, session_id: str, site_id: str,
                      correct_node: str, recovery_path: str = "", provider: str = ""):
    """End error recovery timing, calculate recovery duration"""
    if not provider:
        print("âš ï¸  Provider not specified for error recovery end")
        return 0
    
    # Get log paths and ensure headers
    paths = _log_paths(provider)
    _ensure_headers(paths)
    
    # âœ… Only write when logging is enabled
    enabled, run_id = _is_logging(session_id, provider)
    if enabled:
        # Calculate recovery duration (simplified version, direct recording)
        recovery_duration = _now_ms()  # This can be optimized for actual recovery duration calculation
        
        with open(paths["recovery"], "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                site_id, run_id, datetime.utcnow().isoformat(), session_id, provider,
                "trial",  # phase: trial phase
                "", recovery_duration, "", correct_node
            ])
        
        print(f"âœ… Error recovery completed: {recovery_id}, duration: {recovery_duration}ms")
        return recovery_duration
    
    return 0

# ---------- Opening outputs (Independent variables only take effect here) ----------
HARD_OUTPUTS_EN = {
    "SCENE_A_MS": {
        "base": "You are just inside the Maker Space entrance, facing inward. Walk straight about six steps. Around step five there is a stack of boxesâ€”slow down and pass on one side. Beyond the boxes you enter the open atrium. Landmarks: QR-code bookshelf behind you; black drawer wall to your right.",
        "ft":   "Face forward and walk six steps. Slow at step five to bypass boxes. Continue straight to enter the open atrium. Landmarks: QR-code bookshelf behind; drawer wall on your right."
    },
    "SCENE_B_STUDIO": {
        "base": "You are at the studio entry facing the large window. Walk forward five steps to the window. Then turn left and walk about five steps to the chair next to the orange sofa. Watch your stepâ€”around step three there may be a floor cable.",
        "ft":   "Face forward. Walk five steps to the large window and stop. Turn left and walk five steps to the chair beside the orange sofa. Slow down near step three for a floor cable."
    }
}
HARD_OUTPUTS_ZH = {
    "SCENE_A_MS": {
        "base": "ä½ åœ¨ Maker Space å…¥å£å†…ä¾§ã€‚ç›´è¡Œçº¦å…­æ­¥ã€‚ç¬¬äº”æ­¥æœ‰çº¸ç®±ï¼Œå‡é€Ÿä»ä¸€ä¾§ç»•è¿‡ã€‚è¶Šè¿‡åè¿›å…¥å¼€é˜”çš„ä¸­åº­ã€‚ç¡®è®¤ç‚¹ï¼šèº«åäºŒç»´ç ä¹¦æ¶ï¼›å³ä¾§é»‘è‰²æŠ½å±‰å¢™ã€‚",
        "ft":   "é¢å‘å‰æ–¹ç›´è¡Œå…­æ­¥ã€‚ç¬¬äº”æ­¥ç»•è¿‡çº¸ç®±ï¼Œç»§ç»­å‰è¡Œè¿›å…¥ä¸­åº­ã€‚ç¡®è®¤ç‚¹ï¼šèº«åäºŒç»´ç ä¹¦æ¶ï¼›å³ä¾§æŠ½å±‰å¢™ã€‚"
    },
    "SCENE_B_STUDIO": {
        "base": "ä½ åœ¨å·¥ä½œå®¤å…¥å£ï¼Œæ­£å¯¹å¤§çª—ã€‚å‘å‰äº”æ­¥åˆ°çª—å‰ï¼Œå†å·¦è½¬äº”æ­¥åˆ°æ©™è‰²æ²™å‘æ—çš„æ¤…å­ã€‚æ³¨æ„ç¬¬ä¸‰æ­¥å¯èƒ½æœ‰åœ°é¢ç”µç¼†ã€‚",
        "ft":   "é¢å‘å‰æ–¹èµ°äº”æ­¥è‡³å¤§çª—åœã€‚å†å‘å·¦èµ°äº”æ­¥åˆ°æ©™è‰²æ²™å‘æ—çš„æ¤…å­ã€‚ç¬¬ä¸‰æ­¥é™„è¿‘å¯èƒ½æœ‰ç”µç¼†ï¼Œè¯·æ”¾æ…¢ã€‚"
    }
}

# ---------- Embedding & Index ----------
from sentence_transformers import SentenceTransformer
EMB = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Import dual-channel retrieval
try:
    import sys
    import os
    # Add current directory to Python path
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)
    
    from dual_channel_retrieval import DualChannelRetrieval
    DUAL_CHANNEL_AVAILABLE = True
    print("âœ… Dual channel retrieval module imported successfully")
except ImportError as e:
    print(f"âš ï¸  Dual channel retrieval module not available: {e}")
    print("âš ï¸  Using legacy system")
    DUAL_CHANNEL_AVAILABLE = False

import pathlib

try:
    import faiss  # type: ignore
    USE_FAISS = True
except Exception:
    from sklearn.neighbors import NearestNeighbors
    USE_FAISS = False

# Initialize unified dual-channel retriever
MODEL_DIR_PATH = pathlib.Path(MODEL_DIR)
UNIFIED_RETRIEVER = None

def get_unified_retriever():
    """Get or create enhanced dual-channel retriever with improved fusion strategy"""
    print("ğŸ”§ Initializing enhanced dual-channel retriever...")
    
    global UNIFIED_RETRIEVER
    if UNIFIED_RETRIEVER is None:
        try:
            print("ğŸ”§ Creating enhanced dual-channel retriever with improved fusion...")
            
            # ğŸ”§ ENHANCED: Create an enhanced dual-channel retriever with improved fusion strategy
            class EnhancedDualChannelRetriever:
                def __init__(self):
                    self.structure_tau = 0.15  # ç»“æ„é€šé“æ¸©åº¦ï¼ˆæé«˜ï¼Œè®©åˆ†å¸ƒæ›´å¹³è¡¡ï¼‰
                    self.detail_tau = 0.20     # ç»†èŠ‚é€šé“æ¸©åº¦ï¼ˆæé«˜ï¼Œè®©åˆ†å¸ƒæ›´å¹³è¡¡ï¼‰
                    self.alpha = 0.35          # ç»“æ„é€šé“æƒé‡ï¼ˆè¿›ä¸€æ­¥é™ä½ï¼Œå‡å°‘å®½æ³›ç´¢å¼•è¯å½±å“ï¼‰
                    self.beta = 0.65           # ç»†èŠ‚é€šé“æƒé‡ï¼ˆè¿›ä¸€æ­¥æé«˜ï¼Œå¢å¼ºå†…å®¹åŒ¹é…ï¼‰
                    self.gamma = 0.15          # è¿ç»­æ€§boostæƒé‡ï¼ˆé€‚ä¸­ï¼Œé¿å…è¿‡åº¦å½±å“ï¼‰
                    
                    # ğŸ”§ FIX: å»¶è¿ŸåŠ è½½æ•°æ®ï¼Œç­‰å¾…åœºæ™¯ä¿¡æ¯
                    self.structure_data = None
                    self.detail_data = None
                    self.topology_graph = {}
                    self.topology_empty = False
                    self.current_scene_filter = None
                    
                    print(f"âœ… Enhanced dual-channel retriever initialized (Margin Boost Mode)")
                    print(f"   Structure tau: {self.structure_tau} (sharper distribution)")
                    print(f"   Detail tau: {self.detail_tau} (sharper distribution)")
                    print(f"   Enhanced fusion: Î±={self.alpha}, Î²={self.beta}, Î³={self.gamma}")
                    print(f"   Margin boost: exponential amplification + continuity boost")
                    print(f"   Structure bias: enhanced with 1.5x clarity multiplier")
                
                def _build_topology_graph(self):
                    """æ„å»ºæ‹“æ‰‘å›¾ç”¨äºè¿ç»­æ€§æ£€æŸ¥"""
                    try:
                        # ä»ç»“æ„æ•°æ®ä¸­æå–æ‹“æ‰‘ä¿¡æ¯
                        if hasattr(self, 'structure_data') and self.structure_data:
                            topology = self.structure_data.get('input', {}).get('topology', {})
                            nodes = topology.get('nodes', [])
                            edges = topology.get('edges', [])
                            
                            # ğŸ”§ NEW: Fail-fastæ£€æŸ¥
                            if not nodes:
                                print("âŒ ç©ºæ‹“æ‰‘å›¾ï¼ä¸­æ­¢èåˆï¼Œä½¿ç”¨é¢„è®¾/ä¸Šä¸€å¸§çŠ¶æ€")
                                self.topology_graph = {}
                                return False
                            
                            print(f"ğŸ”§ å¼€å§‹æ„å»ºæ‹“æ‰‘å›¾: {len(nodes)} ä¸ªèŠ‚ç‚¹, {len(edges)} æ¡è¾¹")
                            
                            for node in nodes:
                                node_id = node['id']
                                self.topology_graph[node_id] = []
                                
                                # ä»edgesä¸­æå–é‚»å±…ä¿¡æ¯
                                for edge in edges:
                                    if edge['from'] == node_id:
                                        self.topology_graph[node_id].append(edge['to'])
                                    elif edge['to'] == node_id:
                                        self.topology_graph[node_id].append(edge['from'])
                        
                        print(f"ğŸ”§ æ‹“æ‰‘å›¾æ„å»ºå®Œæˆ: {len(self.topology_graph)} ä¸ªèŠ‚ç‚¹")
                        
                        # ğŸ”§ NEW: éªŒè¯æ‹“æ‰‘å›¾å®Œæ•´æ€§
                        if len(self.topology_graph) == 0:
                            print("âŒ æ‹“æ‰‘å›¾ä¸ºç©ºï¼ä¸­æ­¢èåˆï¼Œä½¿ç”¨é¢„è®¾/ä¸Šä¸€å¸§çŠ¶æ€")
                            self.topology_graph = {}
                            return False  # è¿”å›Falseè¡¨ç¤ºæ„å»ºå¤±è´¥
                        else:
                            print(f"ğŸ”§ æ‹“æ‰‘å›¾éªŒè¯: æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…æ•°é‡")
                            for node_id, neighbors in self.topology_graph.items():
                                print(f"   {node_id}: {len(neighbors)} ä¸ªé‚»å±…")
                            return True  # è¿”å›Trueè¡¨ç¤ºæ„å»ºæˆåŠŸ
                                
                    except Exception as e:
                        print(f"âš ï¸ æ‹“æ‰‘å›¾æ„å»ºå¤±è´¥: {e}")
                        self.topology_graph = {}
                        return False  # è¿”å›Falseè¡¨ç¤ºæ„å»ºå¤±è´¥
                
                def _get_node_neighbors(self, node_id):
                    """è·å–èŠ‚ç‚¹çš„é‚»å±…åˆ—è¡¨"""
                    return self.topology_graph.get(node_id, [])
                
                def _are_neighbors(self, node1, node2):
                    """æ£€æŸ¥ä¸¤ä¸ªèŠ‚ç‚¹æ˜¯å¦ä¸ºé‚»å±…"""
                    if not node1 or not node2:
                        return False
                    if node1 == node2:
                        return True
                    return node2 in self.topology_graph.get(node1, [])
                
                def _get_previous_location(self):
                    """è·å–ä¸Šä¸€å¸§ä½ç½®ï¼ˆç®€åŒ–å®ç°ï¼‰"""
                    # TODO: ä»ä¼šè¯å†å²ä¸­è·å–
                    return None
                
                def _load_structure_data(self):
                    """åŠ è½½ç»“æ„æ•°æ®ï¼ˆæ ¹æ®åœºæ™¯åŠ¨æ€é€‰æ‹©ï¼‰"""
                    try:
                        # ğŸ”§ FIX: åŠ¨æ€é€‰æ‹©æ­£ç¡®çš„ç»“æ„æ–‡ä»¶
                        import os
                        
                        # è·å–å½“å‰åœºæ™¯IDï¼ˆä»å®ä¾‹å±æ€§æˆ–é»˜è®¤å€¼ï¼‰
                        current_scene = getattr(self, 'current_scene_filter', 'SCENE_A_MS')
                        print(f"ğŸ”§ å½“å‰åœºæ™¯: {current_scene}")
                        
                        # æ ¹æ®åœºæ™¯é€‰æ‹©æ­£ç¡®çš„ç»“æ„æ–‡ä»¶
                        if current_scene == "SCENE_B_STUDIO":
                            structure_file = os.path.join(DATA_DIR, "Sense_B_Finetuned.fixed.jsonl")
                            print(f"ğŸ”§ é€‰æ‹©Sense_Bç»“æ„æ–‡ä»¶: {structure_file}")
                        else:
                            # é»˜è®¤ä½¿ç”¨Sense_A
                            structure_file = os.path.join(DATA_DIR, "Sense_A_Finetuned.fixed.jsonl")
                            print(f"ğŸ”§ é€‰æ‹©Sense_Aç»“æ„æ–‡ä»¶: {structure_file}")
                        
                        if os.path.exists(structure_file):
                            with open(structure_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                print(f"ğŸ”§ æˆåŠŸåŠ è½½ç»“æ„æ•°æ®: {structure_file}")
                                return data
                        else:
                            print(f"âš ï¸ ç»“æ„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {structure_file}")
                            # è¿”å›æ¨¡æ‹Ÿæ•°æ®ä½œä¸ºfallback
                            return {
                                "input": {
                                    "topology": {
                                        "nodes": [
                                            {"id": "dp_ms_entrance", "type": "entrance"},
                                            {"id": "poi_3d_printer_table", "type": "workstation"},
                                            {"id": "atrium_edge", "type": "boundary"}
                                        ],
                                        "edges": [
                                            {"from": "dp_ms_entrance", "to": "poi_3d_printer_table"},
                                            {"from": "poi_3d_printer_table", "to": "atrium_edge"}
                                        ]
                                    }
                                }
                            }
                    except Exception as e:
                        print(f"âš ï¸  Failed to load structure data: {e}")
                        return {}
                
                def _ensure_structure_data_loaded(self):
                    """ç¡®ä¿ç»“æ„æ•°æ®å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
                    if self.structure_data is None:
                        print("ğŸ”§ å»¶è¿ŸåŠ è½½ç»“æ„æ•°æ®...")
                        self.structure_data = self._load_structure_data()
                        # æ„å»ºæ‹“æ‰‘å›¾
                        topology_built = self._build_topology_graph()
                        if not topology_built:
                            self.topology_empty = True
                            print("âš ï¸ æ‹“æ‰‘å›¾ä¸ºç©ºï¼Œèåˆæ—¶å°†ä½¿ç”¨é¢„è®¾çŠ¶æ€")
                        else:
                            self.topology_empty = False
                
                def _ensure_detail_data_loaded(self):
                    """ç¡®ä¿ç»†èŠ‚æ•°æ®å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
                    if self.detail_data is None:
                        print("ğŸ”§ å»¶è¿ŸåŠ è½½ç»†èŠ‚æ•°æ®...")
                        self.detail_data = self._load_detail_data()
                
                def _load_detail_data(self):
                    """åŠ è½½ç»†èŠ‚æ•°æ®ï¼ˆSense_A_MS.jsonlç­‰ï¼‰"""
                    try:
                        # è¿™é‡Œåº”è¯¥åŠ è½½å®é™…çš„Detailæ•°æ®
                        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
                        return {
                            "dp_ms_entrance": {"id": "dp_ms_entrance", "features": ["glass doors", "yellow line"]},
                            "poi_3d_printer_table": {"id": "poi_3d_printer_table", "features": ["3D printer", "workbench"]},
                            "atrium_edge": {"id": "atrium_edge", "features": ["windows", "soft seats"]}
                        }
                    except Exception as e:
                        print(f"âš ï¸  Failed to load detail data: {e}")
                        return {}
                
                def _channel_calibration(self, scores, tau):
                    """æ­¥éª¤Aï¼šé€šé“å†…æ ¡å‡† - æ¸©åº¦åŒ–softmaxï¼ˆå¢å¼ºç‰ˆï¼‰"""
                    if not scores:
                        return []
                    
                    # æ¸©åº¦ç¼©æ”¾
                    scaled_scores = [score / tau for score in scores]
                    
                    # Softmax
                    max_score = max(scaled_scores)
                    exp_scores = [np.exp(score - max_score) for score in scaled_scores]
                    sum_exp = sum(exp_scores)
                    
                    probabilities = [exp_score / sum_exp for exp_score in exp_scores]
                    
                    # æ¸©å’Œçš„æ¦‚ç‡è°ƒæ•´ï¼šé¿å…è¿‡åº¦æ”¾å¤§
                    if probabilities:
                        # æ‰¾åˆ°æœ€å¤§æ¦‚ç‡çš„ç´¢å¼•
                        max_prob_idx = probabilities.index(max(probabilities))
                        
                        # æ¸©å’Œè°ƒæ•´ï¼šåªåœ¨æ¦‚ç‡è¿‡ä½æ—¶é€‚å½“æå‡
                        top1_prob = probabilities[max_prob_idx]
                        if top1_prob < 0.3:  # åªåœ¨top1æ¦‚ç‡è¿‡ä½æ—¶è°ƒæ•´
                            # æ¸©å’Œæå‡top1æ¦‚ç‡
                            adjustment_factor = 1.1  # ä»1.2é™ä½åˆ°1.1
                            probabilities[max_prob_idx] = min(0.8, top1_prob * adjustment_factor)
                            
                            # é‡æ–°å½’ä¸€åŒ–å…¶ä»–æ¦‚ç‡
                            remaining_prob = 1.0 - probabilities[max_prob_idx]
                            other_probs = [p for i, p in enumerate(probabilities) if i != max_prob_idx]
                            if other_probs and sum(other_probs) > 0:
                                for i in range(len(probabilities)):
                                    if i != max_prob_idx:
                                        probabilities[i] = (probabilities[i] / sum(other_probs)) * remaining_prob
                    
                    return probabilities
                
                def _conflict_gate(self, alpha, beta, struct_logit, detail_logit, gap=0.5):
                    """å†²çªé—¨æ§å‡½æ•°ï¼šå±€éƒ¨è¿”å›å€¼ï¼Œä¸ä¿®æ”¹å…¨å±€æƒé‡"""
                    if abs(struct_logit - detail_logit) > gap:
                        return alpha * 0.7, beta * 1.1   # è½»å¾®é‡æ„
                    return alpha, beta
                
                def _safe_sharpen(self, probs, tau=0.10):
                    """ğŸ”§ FIX: ä¿®å¤è¿‡åº¦æç«¯çš„äºŒæ¬¡é”åŒ–"""
                    try:
                        import numpy as np
                        
                        def softmax(x):
                            x = np.asarray(x, dtype=np.float64)
                            x = x - np.max(x)
                            e = np.exp(x)
                            s = e.sum()
                            return e / (s if s > 0 else 1.0)
                        
                        def sharpen(probs, tau=0.10):
                            p = np.asarray(probs, dtype=np.float64)
                            eps = 1e-12
                            p = np.clip(p, eps, 1.0 - eps)
                            logits = np.log(p) - np.log(1.0 - p)
                            return softmax(logits / max(tau, 1e-6))
                        
                        # ğŸ”§ FIX: åŠ¨æ€è°ƒæ•´æ¸©åº¦ï¼Œé¿å…è¿‡åº¦æç«¯
                        # æ£€æŸ¥åŸå§‹åˆ†æ•°çš„åˆ†å¸ƒ
                        probs_array = np.array(probs)
                        max_prob = np.max(probs_array)
                        min_prob = np.min(probs_array)
                        prob_range = max_prob - min_prob
                        
                        # å¦‚æœåˆ†æ•°å·®å¼‚å·²ç»å¾ˆå¤§ï¼Œä½¿ç”¨æ›´é«˜æ¸©åº¦
                        if prob_range > 0.5:
                            adjusted_tau = max(0.3, tau)  # è‡³å°‘0.3
                            print(f"ğŸ”§ æ£€æµ‹åˆ°é«˜å·®å¼‚({prob_range:.3f})ï¼Œè°ƒæ•´æ¸©åº¦: {tau:.2f} â†’ {adjusted_tau:.2f}")
                            tau = adjusted_tau
                        
                        # åº”ç”¨å®‰å…¨çš„äºŒæ¬¡é”åŒ–
                        fused = probs  # å…ˆå¾—åˆ°èåˆåçš„æ¦‚ç‡
                        sharpened = sharpen(fused, tau=tau)
                        
                        # ğŸ”§ FIX: é™åˆ¶é”åŒ–åçš„åˆ†æ•°èŒƒå›´ï¼Œé¿å…è¿‡åº¦æç«¯
                        sharpened_array = np.array(sharpened)
                        max_score = 0.8  # é™åˆ¶æœ€é«˜åˆ†æ•°
                        min_score = 0.05  # é™åˆ¶æœ€ä½åˆ†æ•°
                        
                        # åº”ç”¨èŒƒå›´é™åˆ¶
                        sharpened_array = np.clip(sharpened_array, min_score, max_score)
                        
                        # é‡æ–°å½’ä¸€åŒ–
                        total = np.sum(sharpened_array)
                        if total > 0:
                            sharpened_array = sharpened_array / total
                        
                        return sharpened_array.tolist()  # è½¬æ¢å›Pythonåˆ—è¡¨
                        
                    except Exception as e:
                        print(f"âš ï¸ äºŒæ¬¡é”åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¦‚ç‡: {e}")
                        return probs  # å¤±è´¥æ—¶è¿”å›åŸå§‹æ¦‚ç‡
                
                def _calculate_channel_entropy(self, probabilities):
                    """è®¡ç®—é€šé“ç†µï¼ˆåˆ†å¸ƒå°–é”åº¦ï¼‰"""
                    if not probabilities:
                        return 1.0
                    
                    entropy = 0.0
                    for p in probabilities:
                        if p > 0:
                            entropy -= p * np.log(p)
                    
                    return entropy
                
                def _adaptive_weights(self, struct_entropy, detail_entropy):
                    """æ ¹æ®é€šé“ç†µè‡ªé€‚åº”è°ƒæ•´æƒé‡ï¼ˆæ ‡å‡†å…¬å¼å®ç°ï¼‰"""
                    # æŒ‰ç…§æ ‡å‡†å…¬å¼ï¼šÎ± = (1-H_struct) / ((1-H_struct) + (1-H_detail))
                    # ç†µè¶Šä½ï¼Œæƒé‡è¶Šé«˜ï¼ˆåˆ†å¸ƒè¶Šå°–é”ï¼Œè¶Šå¯ä¿¡ï¼‰
                    
                    # è®¡ç®—æ¸…æ™°åº¦ï¼ˆ1 - ç†µï¼‰
                    struct_clarity = 1.0 - struct_entropy
                    detail_clarity = 1.0 - detail_entropy
                    
                    # é˜²æ­¢é™¤é›¶é”™è¯¯
                    total_clarity = struct_clarity + detail_clarity
                    
                    if total_clarity > 0:
                        # æ ‡å‡†å…¬å¼å®ç°
                        alpha = struct_clarity / total_clarity
                        beta = detail_clarity / total_clarity
                        
                        # é™åˆ¶æƒé‡èŒƒå›´ï¼Œé¿å…æç«¯å€¼
                        alpha = max(0.1, min(0.9, alpha))
                        beta = max(0.1, min(0.9, beta))
                        
                        # é‡æ–°å½’ä¸€åŒ–
                        total_weight = alpha + beta
                        alpha = alpha / total_weight
                        beta = beta / total_weight
                        
                    else:
                        # å¦‚æœä¸¤ä¸ªé€šé“éƒ½å¾ˆå¹³ï¼ˆé«˜ç†µï¼‰ï¼Œä½¿ç”¨é»˜è®¤æƒé‡
                        alpha, beta = 0.65, 0.35
                    
                    print(f"ğŸ”§ Dynamic weight calculation:")
                    print(f"   Structure entropy: {struct_entropy:.3f} â†’ clarity: {struct_clarity:.3f} â†’ weight: {alpha:.3f}")
                    print(f"   Detail entropy: {detail_entropy:.3f} â†’ clarity: {detail_clarity:.3f} â†’ weight: {beta:.3f}")
                    print(f"   Total clarity: {total_clarity:.3f}")
                    
                    return alpha, beta
                
                def _enhanced_fusion(self, struct_candidates, detail_candidates, caption, scene_filter):
                    """æ­¥éª¤Bï¼šå¢å¼ºçš„é€šé“é—´èåˆï¼ˆå¯¹æ•°å‡ ç‡ç›¸åŠ ï¼‰+ åè¯æƒ©ç½šæœºåˆ¶"""
                    if not struct_candidates:
                        return []
                    
                    try:
                        # ğŸ”§ NEW: åè¯æƒ©ç½šæœºåˆ¶
                        def apply_negatives(score, node_meta, query_text, penalty=0.15):
                            """åº”ç”¨åè¯æƒ©ç½šï¼šå¦‚æœæŸ¥è¯¢æ–‡æœ¬å‘½ä¸­èŠ‚ç‚¹çš„negativeæç¤ºï¼Œåˆ™é™ä½åˆ†æ•°"""
                            neg = set(node_meta.get("retrieval", {}).get("negative", []))
                            hit = sum(1 for n in neg if n in query_text.lower())
                            if hit > 0:
                                print(f"ğŸ” åè¯æƒ©ç½š: {node_meta.get('id', 'unknown')} å‘½ä¸­ {hit} ä¸ªnegativeæç¤ºï¼Œæƒ©ç½š: {hit * penalty:.3f}")
                            return score - hit * penalty
                        
                        # ğŸ”§ NEW: ç»“æ„é€šé“ç¨³æ€è¯è¿‡æ»¤ï¼ˆä¸æ±¡æŸ“åŸå§‹æ–‡æœ¬ï¼‰
                        MOVABLE = {"suitcase", "bag", "backpack", "person", "cup", "bottle", "laptop", "phone", "book"}
                        LOW_TRUST = {"box": 0.5, "bins": 0.6, "item": 0.7, "stuff": 0.6, "thing": 0.5, "object": 0.5}
                        
                        def term_weight(token):
                            """è·å–è¯çš„æƒé‡ï¼Œä¸ä¿®æ”¹åŸå§‹æ–‡æœ¬"""
                            return LOW_TRUST.get(token.lower(), 1.0)
                        
                        def stable_query(text: str):
                            """ç»“æ„é€šé“ä¸“ç”¨ï¼šè¿‡æ»¤å¯ç§»åŠ¨ç‰©ä½“ï¼Œä¿ç•™å›ºå®šåœ°æ ‡ï¼ˆä¸æ±¡æŸ“æ–‡æœ¬ï¼‰"""
                            t = text.lower()
                            # å®Œå…¨ç§»é™¤å¯ç§»åŠ¨ç‰©ä½“ï¼ˆåŒ…æ‹¬å¤æ•°å½¢å¼ï¼‰
                            for w in MOVABLE:
                                # ç§»é™¤å•æ•°å½¢å¼
                                t = t.replace(f" {w} ", " ")
                                t = t.replace(f"{w} ", " ")
                                t = t.replace(f" {w}", " ")
                                # ç§»é™¤å¤æ•°å½¢å¼
                                t = t.replace(f" {w}s ", " ")
                                t = t.replace(f"{w}s ", " ")
                                t = t.replace(f" {w}s", " ")
                            
                            # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
                            cleaned = " ".join(t.split())
                            cleaned = cleaned.rstrip(" .")
                            if cleaned != text.lower():
                                print(f"ğŸ” ç»“æ„é€šé“ç¨³æ€è¿‡æ»¤: '{text}' â†’ '{cleaned}'")
                            return cleaned
                        
                        # å¯¹ç»“æ„é€šé“åˆ†æ•°åº”ç”¨åè¯æƒ©ç½š + ç¨³æ€è¿‡æ»¤
                        caption_lower = caption.lower()
                        stable_caption = stable_query(caption)  # ç»“æ„é€šé“ç”¨ç¨³æ€ç‰ˆæœ¬
                        
                        for i, struct_cand in enumerate(struct_candidates):
                            original_score = struct_cand['score']
                            # å…ˆåº”ç”¨ç¨³æ€è¿‡æ»¤ï¼ˆåœ¨åè¯æƒ©ç½šä¹‹å‰ï¼‰
                            penalized_score = apply_negatives(original_score, struct_cand, stable_caption)
                            if penalized_score != original_score:
                                print(f"ğŸ” ç»“æ„é€šé“åè¯æƒ©ç½š: {struct_cand['id']} {original_score:.3f} â†’ {penalized_score:.3f}")
                                struct_cand['score'] = penalized_score
                        
                        # æå–åˆ†æ•°ï¼ˆå·²åº”ç”¨åè¯æƒ©ç½šï¼‰
                        struct_scores = [c['score'] for c in struct_candidates]
                        detail_scores = [c['score'] for c in detail_candidates] if detail_candidates else [0.0] * len(struct_candidates)
                        
                        # æ­¥éª¤Aï¼šé€šé“å†…æ ¡å‡† - æ¸©åº¦åŒ–softmax
                        struct_probs = self._channel_calibration(struct_scores, self.structure_tau)
                        detail_probs = self._channel_calibration(detail_scores, self.detail_tau)
                        
                        print(f"ğŸ”§ Channel calibration completed:")
                        print(f"   Structure probs: {[f'{p:.3f}' for p in struct_probs[:3]]}")
                        print(f"   Detail probs: {[f'{p:.3f}' for p in detail_probs[:3]]}")
                        
                        # æ–°å¢ï¼šæ˜¾ç¤ºç©ºé—´æ¦‚å¿µåŒ¹é…è°ƒè¯•ä¿¡æ¯
                        print(f"ğŸ” Space concept matching debug:")
                        print(f"   Caption: {caption}")
                        space_concepts = ["large open space", "open space", "open area", "atrium"]
                        for concept in space_concepts:
                            if concept in caption.lower():
                                print(f"   âœ… Detected space concept: '{concept}'")
                                break
                        
                        # è®¡ç®—é€šé“ç†µå’Œè‡ªé€‚åº”æƒé‡
                        struct_entropy = self._calculate_channel_entropy(struct_probs)
                        detail_entropy = self._calculate_channel_entropy(detail_probs)
                        
                        alpha, beta = self._adaptive_weights(struct_entropy, detail_entropy)
                        
                        print(f"ğŸ”§ Adaptive weights:")
                        print(f"   Structure entropy: {struct_entropy:.3f}, weight: {alpha:.3f}")
                        print(f"   Detail entropy: {detail_entropy:.3f}, weight: {beta:.3f}")
                        
                        # å†²çªé—¨æ§æ£€æŸ¥ï¼šæ£€æŸ¥ä¸¤ä¸ªé€šé“çš„top1æ˜¯å¦ä¸åŒä¸”å·®å¼‚å¾ˆå¤§
                        conflict_detected = False
                        conflict_threshold = 0.5  # å†²çªæ£€æµ‹é˜ˆå€¼
                        alpha_final = alpha
                        beta_final = beta
                        
                        # æ¦‚ç‡è½¬å¯¹æ•°å‡ ç‡ - å®‰å…¨ç‰ˆæœ¬
                        def prob_to_logit(p, eps=1e-6):
                            p = min(max(p, eps), 1 - eps)
                            import math
                            return math.log(p/(1-p))
                        
                        if len(struct_candidates) > 0 and len(detail_candidates) > 0:
                            struct_top1_id = struct_candidates[0]['id']
                            detail_top1_id = detail_candidates[0]['id']
                            
                            if struct_top1_id != detail_top1_id:
                                # è®¡ç®—ä¸¤ä¸ªé€šé“top1çš„logitå·®å¼‚
                                struct_top1_logit = prob_to_logit(struct_probs[0])
                                detail_top1_logit = prob_to_logit(detail_probs[0])
                                logit_diff = abs(struct_top1_logit - detail_top1_logit)
                                
                                if logit_diff > conflict_threshold:
                                    conflict_detected = True
                                    print(f"âš ï¸ Channel conflict detected:")
                                    print(f"   Structure top1: {struct_top1_id} (logit: {struct_top1_logit:.3f})")
                                    print(f"   Detail top1: {detail_top1_id} (logit: {detail_top1_logit:.3f})")
                                    print(f"   Logit difference: {logit_diff:.3f} > {conflict_threshold}")
                                    print(f"   Using conflict gating strategy")
                                    
                                    # ğŸ”§ NEW: å†²çªé—¨æ§æ”¹ä¸ºå±€éƒ¨è¿”å›å€¼ï¼Œåªæ‰§è¡Œä¸€æ¬¡
                                    alpha_final, beta_final = self._conflict_gate(alpha, beta, struct_top1_logit, detail_top1_logit, gap=0.5)
                                    print(f"ğŸ”§ å†²çªé—¨æ§: Î±={alpha:.3f}â†’{alpha_final:.3f}, Î²={beta:.3f}â†’{beta_final:.3f}")
                        
                        # èåˆå¯¹æ•°å‡ ç‡ï¼ˆå¸¦å†²çªé—¨æ§ï¼‰
                        fused_candidates = []
                        fused_logits = []  # æ”¶é›†æ‰€æœ‰èåˆåçš„logitsç”¨äºäºŒæ¬¡é”åŒ–
                        
                        for i, struct_cand in enumerate(struct_candidates):
                            struct_logit = prob_to_logit(struct_probs[i])
                            detail_logit = prob_to_logit(detail_probs[i]) if i < len(detail_probs) else 0.0
                            
                            # ä½¿ç”¨æœ€ç»ˆæƒé‡è¿›è¡Œèåˆï¼ˆæ— è®ºæ˜¯å¦æœ‰å†²çªï¼‰
                            fused_logit = alpha_final * struct_logit + beta_final * detail_logit
                            
                            # è¿ç»­æ€§boostï¼ˆÎ³*boostï¼‰
                            boost_value = self._calculate_continuity_boost(struct_cand, caption, scene_filter)
                            fused_logit += self.gamma * boost_value
                            
                            # ğŸ”§ NEW: æ‹“æ‰‘è¿ç»­æ€§prior
                            def topo_prior(prev_node, current_node):
                                """æ‹“æ‰‘è¿ç»­æ€§priorï¼šä¸Šä¸€å¸§çš„é‚»å±… +0.25ï¼ŒäºŒé˜¶é‚»å±… +0.10ï¼Œå…¶å®ƒ 0"""
                                if prev_node is None:
                                    return 0.0
                                
                                # è·å–ä¸Šä¸€å¸§èŠ‚ç‚¹çš„é‚»å±…
                                prev_neighbors = self._get_node_neighbors(prev_node)
                                if current_node in prev_neighbors:
                                    return 0.25  # ç›´æ¥é‚»å±…
                                
                                # æ£€æŸ¥äºŒé˜¶é‚»å±…
                                for neighbor in prev_neighbors:
                                    neighbor_neighbors = self._get_node_neighbors(neighbor)
                                    if current_node in neighbor_neighbors:
                                        return 0.10  # äºŒé˜¶é‚»å±…
                                
                                return 0.0
                            
                            # åº”ç”¨æ‹“æ‰‘è¿ç»­æ€§prior
                            prev_node = self._get_previous_location()
                            topo_boost = topo_prior(prev_node, struct_cand['id'])
                            if topo_boost > 0:
                                print(f"ğŸ” æ‹“æ‰‘è¿ç»­æ€§prior: {struct_cand['id']} +{topo_boost:.3f}")
                            
                            fused_logit += topo_boost
                            
                            # è½¬å›æ¦‚ç‡ç©ºé—´
                            fused_prob = 1 / (1 + np.exp(-fused_logit))
                            
                            # æ”¶é›†logitç”¨äºäºŒæ¬¡é”åŒ–
                            fused_logits.append(fused_logit)
                            
                            # åˆ›å»ºèåˆåçš„å€™é€‰
                            fused_cand = struct_cand.copy()
                            fused_cand["score"] = fused_prob
                            # ğŸ”§ FIX: ä¿å­˜åŸå§‹çš„structureå’Œdetailåˆ†æ•°
                            fused_cand["structure_score"] = struct_cand["score"]  # ä¿®å¤å­—æ®µå
                            fused_cand["detail_score"] = detail_candidates[i]["score"] if i < len(detail_candidates) else 0.0
                            fused_cand["fusion_weights"] = {"alpha": alpha, "beta": beta, "gamma": self.gamma}
                            fused_cand["boost_value"] = boost_value
                            fused_cand["conflict_strategy"] = "conflict_gated" if conflict_detected else "normal"
                            
                            fused_candidates.append(fused_cand)
                        
                        # ğŸ”§ FIX: èåˆåäºŒæ¬¡é”åŒ– + æ‹“æ‰‘å…ˆéªŒ
                        if fused_logits and len(fused_logits) > 0:
                            try:
                                # ğŸ”§ FIX: ä½¿ç”¨æ›´æ¸©å’Œçš„åˆå§‹æ¸©åº¦ï¼Œé¿å…è¿‡åº¦æç«¯
                                tau_fuse = 0.25  # äºŒæ¬¡é”åŒ–æ¸©åº¦ï¼ˆä»0.10æå‡åˆ°0.25ï¼‰
                                print(f"ğŸ”§ èåˆåäºŒæ¬¡é”åŒ–: Ï„_fuse={tau_fuse}")
                                
                                # åº”ç”¨å®‰å…¨çš„äºŒæ¬¡é”åŒ–å‡½æ•°ï¼ˆå†…éƒ¨ä¼šåŠ¨æ€è°ƒæ•´æ¸©åº¦ï¼‰
                                fused_probs = [cand["score"] for cand in fused_candidates]
                                sharpened_probs = self._safe_sharpen(fused_probs, tau_fuse)
                                
                                # æ›´æ–°å€™é€‰åˆ†æ•°
                                if len(sharpened_probs) == len(fused_candidates):
                                    for i, (cand, sharp_prob) in enumerate(zip(fused_candidates, sharpened_probs)):
                                        old_score = cand["score"]
                                        cand["score"] = float(sharp_prob)  # ç¡®ä¿æ˜¯Python float
                                        if abs(old_score - cand["score"]) > 0.01:
                                            print(f"ğŸ” äºŒæ¬¡é”åŒ–: {cand['id']} {old_score:.3f} â†’ {cand['score']:.3f}")
                                else:
                                    print(f"âš ï¸ äºŒæ¬¡é”åŒ–æ•°ç»„é•¿åº¦ä¸åŒ¹é…: {len(sharpened_probs)} vs {len(fused_candidates)}")
                            except Exception as e:
                                print(f"âš ï¸ äºŒæ¬¡é”åŒ–å¤±è´¥: {e}")
                                # ç»§ç»­ä½¿ç”¨åŸå§‹åˆ†æ•°ï¼Œä¸ä¸­æ–­æµç¨‹
                        
                        print(f"ğŸ”§ Enhanced fusion completed: {len(fused_candidates)} candidates (fused scoring)")
                        return fused_candidates
                        
                    except Exception as e:
                        print(f"âš ï¸ Enhanced fusion failed: {e}")
                        return struct_candidates  # å›é€€åˆ°ç»“æ„é€šé“
                
                def _build_detail_index(self):
                    """ç»Ÿä¸€ä¸€æ¬¡åˆå§‹åŒ–detailæ•°æ®ï¼Œé¿å…é‡å¤åŠ è½½å†²çª"""
                    scene_id = getattr(self, 'current_scene_filter', 'SCENE_A_MS')
                    return self._load_detail_once(scene_id)
                
                def _load_detail_once(self, scene_id):
                    """ç»Ÿä¸€ä¸€æ¬¡åˆå§‹åŒ–detailæ•°æ®ç¼“å­˜"""
                    # æ£€æŸ¥ç¼“å­˜ - ä¿®å¤ï¼šä½¿ç”¨å®ä¾‹å˜é‡è€Œä¸æ˜¯æ–¹æ³•å±æ€§
                    if hasattr(self, '_detail_cache') and self._detail_cache.get("scene") == scene_id:
                        return self._detail_cache["data"]
                    
                    # å®šä¹‰detailæ–‡ä»¶è·¯å¾„
                    DETAIL_PATHS = {
                        'SCENE_A_MS': os.path.join(DATA_DIR, 'Sense_A_MS.jsonl'),
                        'SCENE_B_STUDIO': os.path.join(DATA_DIR, 'Sense_B_Studio.jsonl')
                    }
                    
                    detail_index = {}
                    path = DETAIL_PATHS.get(scene_id)
                    
                    if not path or not os.path.exists(path):
                        print(f"âš ï¸ æœªæ‰¾åˆ°detailæ–‡ä»¶: scene={scene_id}")
                        data = {}
                    else:
                        try:
                            with open(path, 'r', encoding='utf-8') as f:
                                for line in f:
                                    line = line.strip()
                                    if not line:
                                        continue
                                    try:
                                        item = json.loads(line)
                                        node_hint = item.get('node_hint', '')
                                        if node_hint:
                                            if node_hint not in detail_index:
                                                detail_index[node_hint] = []
                                            detail_index[node_hint].append(item)
                                    except json.JSONDecodeError:
                                        continue
                            
                            print(f"âœ… Detailæ•°æ®å·²åŠ è½½: scene={scene_id}, {len(detail_index)} ä¸ªèŠ‚ç‚¹æœ‰detailæ•°æ®")
                            for node_id, items in list(detail_index.items())[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                                print(f"   {node_id}: {len(items)} é¡¹")
                            if len(detail_index) > 3:
                                print(f"   ...ç­‰å…± {len(detail_index)} ä¸ªèŠ‚ç‚¹")
                            data = detail_index
                        except Exception as e:
                            print(f"âš ï¸ Detailæ•°æ®åŠ è½½å¤±è´¥: {e}")
                            data = {}
                    
                    # ç¼“å­˜ç»“æœ - ä¿®å¤ï¼šä½¿ç”¨å®ä¾‹å˜é‡
                    self._detail_cache = {"scene": scene_id, "data": data}
                    return data
                
                def _calculate_continuity_boost(self, candidate, caption, scene_filter):
                    """è®¡ç®—è¿ç»­æ€§boostå€¼ï¼ˆÎ³*boostï¼‰- å¢å¼ºç‰ˆ"""
                    try:
                        boost_value = 0.0
                        
                        # 1. æ–¹å‘ä¸€è‡´æ€§boostï¼ˆå¢å¼ºï¼‰
                        if hasattr(candidate, 'bearing_hint'):
                            bearing = candidate.get('bearing_hint', '')
                            if bearing and any(word in caption.lower() for word in bearing.split()):
                                boost_value += 0.2  # ä»0.1å¢åŠ åˆ°0.2
                        
                        # 2. æ‹“æ‰‘åˆæ³•æ€§boostï¼ˆå¢å¼ºï¼‰
                        if hasattr(candidate, 'topology_valid'):
                            if candidate.get('topology_valid', False):
                                boost_value += 0.15  # ä»0.05å¢åŠ åˆ°0.15
                        
                        # 3. ç©ºé—´å…³ç³»ä¸€è‡´æ€§boostï¼ˆå¢å¼ºï¼‰
                        spatial_relations = candidate.get('spatial_relations', {})
                        for relation, landmark in spatial_relations.items():
                            if landmark and any(word in caption.lower() for word in str(landmark).split()):
                                boost_value += 0.1  # ä»0.05å¢åŠ åˆ°0.1
                        
                        # 4. å…³é”®è¯åŒ¹é…boostï¼ˆæ–°å¢ï¼‰
                        caption_lower = caption.lower()
                        candidate_text = candidate.get('text', '').lower()
                        if candidate_text:
                            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
                            caption_words = set(caption_lower.split())
                            text_words = set(candidate_text.split())
                            overlap = len(caption_words & text_words)
                            if overlap > 0:
                                boost_value += overlap * 0.05  # æ¯ä¸ªåŒ¹é…è¯+0.05
                        
                        # 5. å†å²è¿ç»­æ€§boostï¼ˆå¦‚æœæœ‰sessionä¿¡æ¯ï¼‰
                        # è¿™é‡Œå¯ä»¥æ·»åŠ åŸºäºsessionå†å²çš„boosté€»è¾‘
                        
                        return min(0.5, boost_value)  # ä»0.3å¢åŠ åˆ°0.5ï¼Œè®©boostæœ‰æ›´å¤§å½±å“
                        
                    except Exception as e:
                        print(f"âš ï¸ Continuity boost calculation failed: {e}")
                        return 0.0

                def _pack_result(self, node_id, score, used_detail=None):
                    """ç»Ÿä¸€è¿”å›å€¼æ ¼å¼ï¼Œç¡®ä¿æ°¸è¿œè¿”å›ä¸‰å…ƒç»„"""
                    return node_id, float(score), bool(used_detail) if used_detail is not None else False
                
                def _has_detail_data(self, scene_filter):
                    """æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„detailæ•°æ®"""
                    try:
                        if scene_filter == "SCENE_A_MS":
                            detail_file = os.path.join(DATA_DIR, "Sense_A_MS.jsonl")
                        elif scene_filter == "SCENE_B_STUDIO":
                            detail_file = os.path.join(DATA_DIR, "Sense_B_Studio.jsonl")
                        else:
                            return False
                        
                        return os.path.exists(detail_file) and os.path.getsize(detail_file) > 0
                    except Exception:
                        return False
                
                def _get_detail_for_node(self, node_id, scene_filter):
                    """è·å–ç‰¹å®šèŠ‚ç‚¹çš„detailæ•°æ®"""
                    try:
                        if scene_filter == "SCENE_A_MS":
                            detail_file = os.path.join(DATA_DIR, "Sense_A_MS.jsonl")
                        elif scene_filter == "SCENE_B_STUDIO":
                            detail_file = os.path.join(DATA_DIR, "Sense_B_Studio.jsonl")
                        else:
                            return []
                        
                        if not os.path.exists(detail_file):
                            return []
                        
                        detail_items = []
                        with open(detail_file, 'r', encoding='utf-8') as f:
                            for line in f:
                                if line.strip():
                                    try:
                                        detail_item = json.loads(line)
                                        if detail_item.get("node_hint") == node_id:
                                            detail_items.append(detail_item)
                                    except json.JSONDecodeError:
                                        continue
                        
                        return detail_items
                    except Exception:
                        return []

                def retrieve(self, caption, top_k=10, scene_filter=None):
                    """å¢å¼ºåŒé€šé“æ£€ç´¢ï¼šä½¿ç”¨æ”¹è¿›çš„èåˆç­–ç•¥ï¼Œè¿”å›å€™é€‰åˆ—è¡¨"""
                    # è®¾ç½®å½“å‰åœºæ™¯è¿‡æ»¤å™¨ï¼Œç”¨äºæ„å»ºdetailç´¢å¼•
                    self.current_scene_filter = scene_filter
                    
                    print(f"ğŸ”§ Enhanced dual-channel retrieval for: {caption[:50]}...")
                    
                    try:
                        # ğŸ”§ FIX: ç¡®ä¿æ•°æ®å·²åŠ è½½
                        self._ensure_structure_data_loaded()
                        self._ensure_detail_data_loaded()
                        
                        # æ­¥éª¤Aï¼šé€šé“å†…æ ¡å‡† - è·å–ä¸¤ä¸ªé€šé“çš„å€™é€‰
                        struct_candidates = self._retrieve_from_structure_map(caption, scene_filter, top_k)
                        detail_candidates = self._retrieve_from_detail_map(caption, scene_filter, top_k)
                        
                        # æ£€æŸ¥detailæ•°æ®å¯ç”¨æ€§
                        has_detail_data = self._has_detail_data(scene_filter) and len(detail_candidates) > 0
                        print(f"ğŸ”§ Detail data availability: {has_detail_data} (found {len(detail_candidates)} detail candidates)")
                        
                        # ä¿®å¤ï¼šä¸ºæ¯ä¸ªcandidateæ·»åŠ has_detailæ ‡è®°
                        for candidate in struct_candidates:
                            candidate["has_detail"] = has_detail_data
                        
                        if not struct_candidates:
                            print("âš ï¸ No candidates found from structure map")
                            return []
                        
                        # æ­¥éª¤Bï¼šé€šé“é—´èåˆï¼ˆå¯¹æ•°å‡ ç‡ç›¸åŠ ï¼‰
                        fused_candidates = self._enhanced_fusion(
                            struct_candidates, detail_candidates, caption, scene_filter
                        )
                        
                        if not fused_candidates:
                            print("âš ï¸ Fusion failed")
                            return struct_candidates  # å›é€€åˆ°ç»“æ„é€šé“
                        
                        # ğŸ”§ FIX: æ·»åŠ å¤šæ ·æ€§è¯†åˆ«æœºåˆ¶ï¼Œé¿å…æ€»æ˜¯è¯†åˆ«åŒä¸€ä¸ªPOI
                        # æ£€æŸ¥æ˜¯å¦è¿ç»­å¤šæ¬¡è¯†åˆ«åŒä¸€ä¸ªPOI
                        current_top1_id = fused_candidates[0]['id']
                        if hasattr(self, '_last_top1_id') and hasattr(self, '_top1_repeat_count'):
                            if current_top1_id == self._last_top1_id:
                                self._top1_repeat_count += 1
                                # å¦‚æœè¿ç»­è¯†åˆ«è¶…è¿‡3æ¬¡ï¼Œé™ä½è¯¥POIçš„åˆ†æ•°
                                if self._top1_repeat_count > 3:
                                    print(f"âš ï¸ è¿ç»­è¯†åˆ«{self._top1_repeat_count}æ¬¡{current_top1_id}ï¼Œé™ä½åˆ†æ•°å¢åŠ å¤šæ ·æ€§")
                                    for candidate in fused_candidates:
                                        if candidate['id'] == current_top1_id:
                                            candidate['score'] *= 0.7  # é™ä½30%åˆ†æ•°
                                            break
                                    # é‡æ–°æ’åº
                                    fused_candidates.sort(key=lambda x: x["score"], reverse=True)
                            else:
                                self._top1_repeat_count = 1
                        else:
                            self._top1_repeat_count = 1
                        
                        self._last_top1_id = current_top1_id
                        
                        # æŒ‰èåˆåˆ†æ•°æ’åº
                        fused_candidates.sort(key=lambda x: x["score"], reverse=True)
                        
                        # æ­¥éª¤Cï¼šè¾“å‡ºç½®ä¿¡åº¦ä¸marginï¼ˆå¢å¼ºç‰ˆï¼‰
                        top1_score = fused_candidates[0]['score']
                        top2_score = fused_candidates[1]['score'] if len(fused_candidates) > 1 else 0
                        
                        # ğŸ”§ FIX: æ”¹è¿›confidenceè®¡ç®—ï¼Œé¿å…è¿‡äºå›ºå®š
                        # åŸºäºmarginå’Œtop1_scoreçš„åŠ¨æ€confidenceè®¡ç®—
                        base_margin = top1_score - top2_score
                        
                        # åŠ¨æ€confidenceï¼šç»“åˆmarginå’Œtop1_score
                        if base_margin > 0.3:  # é«˜marginæ—¶ç»™äºˆé«˜confidence
                            confidence = min(0.95, top1_score * 0.9 + base_margin * 0.3)
                        elif base_margin > 0.1:  # ä¸­ç­‰marginæ—¶ç»™äºˆä¸­ç­‰confidence
                            confidence = min(0.85, top1_score * 0.8 + base_margin * 0.2)
                        else:  # ä½marginæ—¶é™ä½confidence
                            confidence = max(0.5, top1_score * 0.6 + base_margin * 0.1)
                        
                        # å¢å¼ºmarginè®¡ç®—ï¼šä½¿ç”¨æŒ‡æ•°æ”¾å¤§å’Œè¿ç»­æ€§boost
                        # è¿ç»­æ€§boostå¢å¼ºmargin
                        top1_boost = fused_candidates[0].get('boost_value', 0.0)
                        margin_boost = top1_boost * 0.3  # boostå¯¹marginçš„è´¡çŒ®
                        
                        # æŒ‡æ•°æ”¾å¤§marginï¼ˆè®©å·®å¼‚æ›´æ˜æ˜¾ï¼‰
                        enhanced_margin = base_margin * (1.0 + margin_boost)
                        if enhanced_margin > 0:
                            enhanced_margin = enhanced_margin ** 0.8  # æŒ‡æ•°0.8ï¼Œè®©å·®å¼‚æ›´çªå‡º
                        
                        margin = enhanced_margin
                        
                        # ğŸ”§ FIX: æ›´åˆç†çš„confidenceå’ŒmarginèŒƒå›´
                        confidence = max(0.3, min(0.95, confidence))  # å…è®¸æ›´ä½çš„confidence
                        margin = max(0.02, min(0.9, margin))  # å…è®¸æ›´ä½çš„margin
                        
                        print(f"âœ… Enhanced dual-channel retrieval completed (Margin Boost Mode):")
                        print(f"   Top1: {fused_candidates[0]['id']} (score: {top1_score:.4f})")
                        if len(fused_candidates) > 1:
                            print(f"   Top2: {fused_candidates[1]['id']} (score: {top2_score:.4f})")
                            print(f"   Base margin: {base_margin:.4f}")
                            print(f"   Boost value: {top1_boost:.4f}")
                            print(f"   Enhanced margin: {margin:.4f}")
                        print(f"   Confidence: {confidence:.4f}")
                        print(f"   Margin boost: exponential + continuity boost applied")
                        
                        # ä¸ºæ¯ä¸ªå€™é€‰æ·»åŠ confidenceå’Œmarginä¿¡æ¯
                        for i, candidate in enumerate(fused_candidates):
                            candidate["confidence"] = confidence if i == 0 else confidence * 0.8
                            candidate["margin"] = margin
                            candidate["retrieval_method"] = "enhanced_dual_channel_fusion"
                            candidate["has_detail"] = has_detail_data  # æ·»åŠ detailå¯ç”¨æ€§æ ‡è®°
                        
                        # ä¿®å¤ï¼šç¡®ä¿è¿”å›çš„æ¯ä¸ªcandidateéƒ½æœ‰æ­£ç¡®çš„has_detailæ ‡è®°
                        print(f"ğŸ”§ Final candidates prepared: {len(fused_candidates)} with has_detail={has_detail_data}")
                        
                        return fused_candidates
                        
                    except Exception as e:
                        print(f"âš ï¸ Enhanced dual-channel retrieval failed: {e}")
                        return []
                
                def _retrieve_from_structure_map(self, caption, scene_filter, top_k):
                    """ä»structure mapæ–‡ä»¶ä¸­æ£€ç´¢ï¼ˆç»“æ„é€šé“ï¼‰"""
                    try:
                        # æ ¹æ®åœºæ™¯é€‰æ‹©æ–‡ä»¶
                        if scene_filter == "SCENE_A_MS":
                            textmap_file = os.path.join(DATA_DIR, "Sense_A_Finetuned.fixed.jsonl")
                        elif scene_filter == "SCENE_B_STUDIO":
                            textmap_file = os.path.join(DATA_DIR, "Sense_B_Finetuned.fixed.jsonl")
                        else:
                            print(f"âš ï¸ Unknown scene: {scene_filter}")
                            return []
                        
                        print(f"ğŸ” Reading structure map from: {textmap_file}")
                        
                        # è¯»å–textmapæ–‡ä»¶ - æ”¯æŒJSONå’ŒJSONLä¸¤ç§æ ¼å¼
                        textmap_data = None
                        try:
                            # é¦–å…ˆå°è¯•ä½œä¸ºæ ‡å‡†JSONè¯»å–
                            with open(textmap_file, 'r', encoding='utf-8') as f:
                                textmap_data = json.load(f)
                                print(f"ğŸ” æˆåŠŸè¯»å–ä¸ºæ ‡å‡†JSONæ ¼å¼")
                        except json.JSONDecodeError:
                            # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½œä¸ºJSONLè¯»å–
                            try:
                                with open(textmap_file, 'r', encoding='utf-8') as f:
                                    for line in f:
                                        if line.strip():
                                            textmap_data = json.loads(line)
                                            break  # åªè¯»å–ç¬¬ä¸€è¡Œ
                                print(f"ğŸ” æˆåŠŸè¯»å–ä¸ºJSONLæ ¼å¼")
                            except Exception as e:
                                print(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶: {e}")
                                return []
                        
                        if not textmap_data:
                            print(f"âš ï¸ No data found in {textmap_file}")
                            return []
                        
                        # ğŸ”§ FIX: æ­£ç¡®å¤„ç†Sense_Bçš„èŠ‚ç‚¹æ ¼å¼
                        # æå–èŠ‚ç‚¹ä¿¡æ¯ - æ£€æŸ¥input.topologyå’Œé¡¶çº§topology
                        nodes = []
                        if "input" in textmap_data and "topology" in textmap_data["input"]:
                            nodes = textmap_data["input"]["topology"].get("nodes", [])
                            print(f"ğŸ” ä»input.topologyä¸­è¯»å–èŠ‚ç‚¹")
                        elif "topology" in textmap_data:
                            nodes = textmap_data["topology"].get("nodes", [])
                            print(f"ğŸ” ä»é¡¶çº§topologyä¸­è¯»å–èŠ‚ç‚¹")
                        
                        if not nodes:
                            print(f"âš ï¸ No nodes found in {textmap_file}")
                            return []
                        
                        print(f"ğŸ” Found {len(nodes)} nodes in structure map (structure channel)")
                        
                        # ğŸ”§ FIX: å¤„ç†ä¸åŒçš„èŠ‚ç‚¹æ ¼å¼
                        # Sense_A: [{"id": "poi01", "name": "..."}, ...]
                        # Sense_B: ["poi11", "poi12", ...]
                        processed_nodes = []
                        if nodes and isinstance(nodes[0], str):
                            # Sense_Bæ ¼å¼ï¼šå­—ç¬¦ä¸²æ•°ç»„ï¼Œéœ€è¦è½¬æ¢ä¸ºå¯¹è±¡æ ¼å¼
                            print(f"ğŸ”§ æ£€æµ‹åˆ°Sense_Bæ ¼å¼ï¼Œè½¬æ¢èŠ‚ç‚¹ä¸ºå¯¹è±¡æ ¼å¼")
                            for node_id in nodes:
                                # ä»poisä¸­è·å–èŠ‚ç‚¹ä¿¡æ¯
                                pois = textmap_data.get("input", {}).get("pois", {})
                                if node_id in pois:
                                    node_info = pois[node_id]
                                    processed_nodes.append({
                                        "id": node_id,
                                        "name": node_info.get("name", ""),
                                        "retrieval": textmap_data.get("input", {}).get("retrieval", {}),
                                        "landmarks": [],
                                        "categories": []
                                    })
                                else:
                                    # å¦‚æœæ²¡æœ‰poisä¿¡æ¯ï¼Œåˆ›å»ºåŸºæœ¬èŠ‚ç‚¹
                                    processed_nodes.append({
                                        "id": node_id,
                                        "name": node_id,
                                        "retrieval": textmap_data.get("input", {}).get("retrieval", {}),
                                        "landmarks": [],
                                        "categories": []
                                    })
                        else:
                            # Sense_Aæ ¼å¼ï¼šå·²ç»æ˜¯å¯¹è±¡æ•°ç»„
                            processed_nodes = nodes
                        
                        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„ç›¸ä¼¼åº¦åˆ†æ•°
                        candidates = []
                        caption_lower = caption.lower()
                        
                        for node in processed_nodes:
                            node_id = node.get("id", "")
                            if not node_id:
                                continue
                            
                            # è®¡ç®—æ£€ç´¢åˆ†æ•°
                            score = self._calculate_node_score(node, caption_lower)
                            
                            candidates.append({
                                "id": node_id,
                                "score": score,
                                "text": node.get("name", ""),
                                "score_nl": score,
                                "score_struct": score,
                                "provider": "ft",
                                "bonus_keywords": 0.0,
                                "bonus_bearing": 0.0,
                                "alpha_used": 0.8,
                                "retrieval_method": "structure_channel"
                            })
                        
                        # è¯­ä¹‰å»é‡ï¼šåˆå¹¶è¯­ä¹‰ç›¸ä¼¼çš„èŠ‚ç‚¹
                        deduplicated_candidates = self._semantic_deduplication(candidates, caption_lower)
                        
                        # è¿”å›top_kä¸ªå€™é€‰
                        deduplicated_candidates.sort(key=lambda x: x["score"], reverse=True)
                        return deduplicated_candidates[:top_k]
                        
                    except Exception as e:
                        print(f"âš ï¸ Failed to read structure map: {e}")
                        return []
                
                def _retrieve_from_detail_map(self, caption, scene_filter, top_k):
                    """ä»detail mapæ–‡ä»¶ä¸­æ£€ç´¢ï¼ˆç»†èŠ‚é€šé“ï¼‰"""
                    try:
                        # æ ¹æ®åœºæ™¯é€‰æ‹©æ–‡ä»¶
                        if scene_filter == "SCENE_A_MS":
                            detail_file = os.path.join(DATA_DIR, "Sense_A_MS.jsonl")
                        elif scene_filter == "SCENE_B_STUDIO":
                            detail_file = os.path.join(DATA_DIR, "Sense_B_Studio.jsonl")
                        else:
                            print(f"âš ï¸ Unknown scene: {scene_filter}")
                            return []
                        
                        # è¯»å–detailæ–‡ä»¶
                        if not os.path.exists(detail_file):
                            print(f"âš ï¸ Detail file not found: {detail_file}")
                            return []
                        
                        print(f"ğŸ” Reading detail data from: {detail_file}")
                        
                        # è§£æJSONLæ ¼å¼
                        detail_candidates = []
                        caption_lower = caption.lower()
                        
                        with open(detail_file, 'r', encoding='utf-8') as f:
                            for line_num, line in enumerate(f, 1):
                                if line.strip():
                                    try:
                                        detail_item = json.loads(line)
                                        node_id = detail_item.get("node_hint", "")
                                        
                                        if not node_id:
                                            print(f"âš ï¸ Line {line_num}: Missing node_hint")
                                            continue
                                        
                                        # è®¡ç®—detailåˆ†æ•°
                                        score = self._calculate_detail_score(detail_item, caption_lower)
                                        
                                        detail_candidates.append({
                                            "id": node_id,
                                            "score": score,
                                            "text": detail_item.get("nl_text", ""),
                                            "score_nl": score,
                                            "score_detail": score,
                                            "provider": "ft",
                                            "retrieval_method": "detail_channel"
                                        })
                                        
                                    except json.JSONDecodeError as e:
                                        print(f"âš ï¸ Line {line_num}: JSON decode error: {e}")
                                        continue
                        
                        print(f"ğŸ” Successfully loaded {len(detail_candidates)} detail candidates")
                        
                        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top_k
                        detail_candidates.sort(key=lambda x: x["score"], reverse=True)
                        return detail_candidates[:top_k]
                        
                    except Exception as e:
                        print(f"âš ï¸ Failed to read detail map: {e}")
                        return []
                
                def _calculate_detail_score(self, detail_item, caption_lower):
                    """è®¡ç®—detailé¡¹çš„æ£€ç´¢åˆ†æ•°ï¼ˆç»†èŠ‚é€šé“ï¼‰"""
                    score = 0.0
                    
                    # 1. è‡ªç„¶è¯­è¨€æ–‡æœ¬åŒ¹é…
                    nl_text = detail_item.get("nl_text", "").lower()
                    if nl_text:
                        # ç®€å•çš„è¯æ±‡åŒ¹é…
                        caption_words = set(caption_lower.split())
                        text_words = set(nl_text.split())
                        overlap = len(caption_words & text_words)
                        if overlap > 0:
                            score += overlap * 0.1  # æ¯ä¸ªåŒ¹é…è¯+0.1
                    
                    # 2. ç»“æ„åŒ–æ–‡æœ¬åŒ¹é…
                    struct_text = detail_item.get("struct_text", "").lower()
                    if struct_text:
                        caption_words = set(caption_lower.split())
                        struct_words = set(struct_text.split())
                        overlap = len(caption_words & struct_words)
                        if overlap > 0:
                            score += overlap * 0.08  # æ¯ä¸ªåŒ¹é…è¯+0.08
                    
                    # 3. ç©ºé—´å…³ç³»åŒ¹é…
                    spatial_relations = detail_item.get("spatial_relations", {})
                    for relation, landmark in spatial_relations.items():
                        if landmark and any(word in caption_lower for word in str(landmark).split()):
                            score += 0.05  # ç©ºé—´å…³ç³»åŒ¹é…+0.05
                    
                    # 4. ç‹¬ç‰¹ç‰¹å¾åŒ¹é…
                    unique_features = detail_item.get("unique_features", [])
                    for feature in unique_features:
                        if any(word in caption_lower for word in feature.lower().split()):
                            score += 0.03  # æ¯ä¸ªåŒ¹é…ç‰¹å¾+0.03
                    
                    return min(1.0, score)  # é™åˆ¶æœ€å¤§åˆ†æ•°ä¸º1.0
                
                def _calculate_node_score(self, node, caption_lower):
                    """è®¡ç®—èŠ‚ç‚¹çš„æ£€ç´¢åˆ†æ•°ï¼ˆç»“æ„é€šé“ï¼‰- å¢å¼ºç‰ˆ"""
                    score = 0.0
                    
                    # ğŸ”§ FIX: æ”¹è¿›è¯­ä¹‰åŒ¹é…ï¼Œé¿å…è¿‡äºå®½æ³›çš„åŒ¹é…
                    # 1. åŸºäºæ£€ç´¢è¯çš„åˆ†æ•°ï¼ˆæ›´ä¸¥æ ¼çš„åŒ¹é…ï¼‰
                    retrieval = node.get("retrieval", {})
                    index_terms = retrieval.get("index_terms", [])
                    tags = retrieval.get("tags", [])
                    
                    # å…³é”®è¯åŒ¹é…ï¼ˆæ›´ä¸¥æ ¼çš„æƒé‡åˆ†é…ï¼‰
                    for term in index_terms + tags:
                        term_lower = term.lower()
                        if term_lower in caption_lower:
                            # ç©ºé—´æ¦‚å¿µç»™äºˆæ›´é«˜æƒé‡
                            if any(space_word in term_lower for space_word in ["open", "space", "area", "large", "atrium"]):
                                score += 0.4  # ç©ºé—´æ¦‚å¿µæƒé‡
                            elif "box" in term_lower or "boxes" in term_lower:
                                # ğŸ”§ FIX: é™ä½boxç›¸å…³è¯çš„æƒé‡ï¼Œé¿å…è¿‡åº¦åŒ¹é…
                                score += 0.15  # ä»0.3é™ä½åˆ°0.15
                            else:
                                score += 0.3
                        elif any(word in caption_lower for word in term_lower.split()):
                            if any(space_word in term_lower for space_word in ["open", "space", "area", "large", "atrium"]):
                                score += 0.2
                            elif "box" in term_lower or "boxes" in term_lower:
                                # ğŸ”§ FIX: é™ä½boxç›¸å…³è¯çš„éƒ¨åˆ†åŒ¹é…æƒé‡
                                score += 0.08  # ä»0.15é™ä½åˆ°0.08
                            else:
                                score += 0.15
                    
                    # 2. åŸºäºèŠ‚ç‚¹åç§°çš„åˆ†æ•°ï¼ˆæ›´ç²¾ç¡®çš„åŒ¹é…ï¼‰
                    node_name = node.get("name", "").lower()
                    if node_name:
                        # æ£€æŸ¥ç©ºé—´æ¦‚å¿µå…³é”®è¯
                        space_keywords = ["open", "space", "area", "large", "atrium", "cluster"]
                        space_match = any(space_word in node_name for space_word in space_keywords)
                        
                        if space_match and any(word in caption_lower for word in ["open", "space", "large"]):
                            score += 0.3  # ç©ºé—´æ¦‚å¿µåŒ¹é…ç»™äºˆæ›´é«˜æƒé‡
                        elif any(word in caption_lower for word in node_name.split()):
                            # ğŸ”§ FIX: é™ä½boxç›¸å…³èŠ‚ç‚¹çš„åç§°åŒ¹é…æƒé‡
                            if "box" in node_name or "boxes" in node_name:
                                score += 0.1  # ä»0.2é™ä½åˆ°0.1
                            else:
                                score += 0.2
                    
                    # 3. åŸºäºåœ°æ ‡çš„åˆ†æ•°
                    landmarks = node.get("landmarks", [])
                    for landmark in landmarks:
                        if isinstance(landmark, dict):
                            landmark_term = landmark.get("term", "").lower()
                            if landmark_term and any(word in caption_lower for word in landmark_term.split()):
                                score += 0.1
                        elif isinstance(landmark, str) and landmark.startswith("lm_"):
                            # åœ°æ ‡IDåŒ¹é…
                            landmark_id = landmark.lower()
                            if any(word in caption_lower for word in landmark_id.split("_")):
                                score += 0.1
                    
                    # 4. åŸºäºç±»åˆ«çš„åˆ†æ•°
                    categories = node.get("categories", [])
                    for category in categories:
                        if category.lower() in caption_lower:
                            score += 0.1
                    
                    # 5. æ–°å¢ï¼šç©ºé—´æ¦‚å¿µè¯­ä¹‰åŒ¹é…ï¼ˆè§£å†³"large open space"é—®é¢˜ï¼‰
                    # æ£€æŸ¥captionä¸­çš„ç©ºé—´æ¦‚å¿µæ˜¯å¦ä¸èŠ‚ç‚¹åŒ¹é…
                    space_concepts = {
                        "large open space": ["open", "space", "large", "area", "atrium", "cluster"],
                        "open space": ["open", "space", "area", "atrium"],
                        "open area": ["open", "area", "space", "atrium"],
                        "atrium": ["atrium", "open", "space", "area"]
                    }
                    
                    for caption_concept, keywords in space_concepts.items():
                        if caption_concept in caption_lower:
                            # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦åŒ…å«ç›¸å…³ç©ºé—´æ¦‚å¿µ
                            node_text = f"{node.get('name', '')} {' '.join(index_terms)} {' '.join(tags)}"
                            node_text_lower = node_text.lower()
                            
                            if any(keyword in node_text_lower for keyword in keywords):
                                score += 0.25  # ç©ºé—´æ¦‚å¿µè¯­ä¹‰åŒ¹é…ç»™äºˆé¢å¤–æƒé‡
                                break
                    
                    # ğŸ”§ FIX: æ·»åŠ å¤šæ ·æ€§æƒ©ç½šï¼Œé¿å…æ€»æ˜¯é€‰æ‹©åŒä¸€ä¸ªPOI
                    if hasattr(self, '_last_top1_id') and node.get("id") == getattr(self, '_last_top1_id', None):
                        score *= 0.8  # è¿ç»­é€‰æ‹©åŒä¸€POIæ—¶é™ä½20%åˆ†æ•°
                    
                    return min(1.0, score)  # é™åˆ¶æœ€å¤§åˆ†æ•°ä¸º1.0
                
                def _semantic_deduplication(self, candidates, caption_lower):
                    """è¯­ä¹‰å»é‡ï¼šåˆå¹¶è¯­ä¹‰ç›¸ä¼¼çš„èŠ‚ç‚¹ï¼Œé¿å…è¿”å›é‡å¤çš„TV screenç­‰"""
                    if not candidates:
                        return candidates
                    
                    # å®šä¹‰è¯­ä¹‰ç›¸ä¼¼ç»„ - å¢å¼ºç‰ˆï¼Œæ›´ç²¾ç¡®çš„åŒºåˆ†
                    semantic_groups = {
                        "tv_screen_group": [
                            "tv screen", "large tv screen", "large tv screen near entry",
                            "tv", "television", "display", "screen", "monitor"
                        ],
                        "window_group": [
                            "glass window", "window wall", "windows", "glass", "window"
                        ],
                        "sofa_group": [
                            "orange sofa", "sofa", "couch", "seating"
                        ],
                        "chair_group": [
                            "chair", "chair_on", "yline", "seating", "stool"
                        ],
                        "space_group": [
                            "open space", "large open space", "open area", "atrium", "space"
                        ],
                        "boxes_group": [
                            "boxes", "box", "cardboard", "stacked", "floor", "on floor"
                        ],
                        "desk_group": [
                            "desk", "desks", "workbench", "workstation", "computer"
                        ],
                        "table_group": [
                            "table", "surface", "counter", "small_table"
                        ],
                        "storage_group": [
                            "storage", "shelf", "cabinet", "drawer", "container"
                        ],
                        "wall_group": [
                            "wall", "drawer_wall", "component_wall", "partition"
                        ]
                    }
                    
                    # æ–°å¢ï¼šå®ä½“åˆ«åæ˜ å°„ï¼Œè¯†åˆ«åŒä¸€å®ä½“çš„ä¸åŒè¡¨ç¤ºï¼ˆä¿®å¤èŠ‚ç‚¹IDä¸åŒ¹é…ï¼‰
                    entity_aliases = {
                        "poi01_entrance_glass_door": ["dp_ms_entrance", "entrance", "glass door"],
                        "poi02_green_trash_bin": ["yline_start", "trash bin", "green bin"],
                        "poi03_black_drawer_cabinet": ["yline_bend_mid", "drawer cabinet", "black cabinet"],
                        "poi04_wall_3d_printers": ["atrium_edge", "3d printers", "wall printers"],
                        "poi05_desk_3d_printer": ["tv_zone", "desk printer", "3d printer"],
                        "poi06_small_open_3d_printer": ["storage_corner", "small printer", "open printer"],
                        "poi07_cardboard_boxes": ["orange_sofa_corner", "cardboard boxes", "boxes"],
                        "poi08_to_atrium": ["desks_cluster", "atrium", "to atrium"],
                        "poi09_qr_bookshelf": ["chair_on_yline", "qr bookshelf", "bookshelf"],
                        "poi10_metal_display_cabinet": ["small_table_mid", "metal cabinet", "display cabinet"]
                    }
                    
                    # æŒ‰è¯­ä¹‰ç»„åˆ†ç»„å€™é€‰
                    grouped_candidates = {}
                    for candidate in candidates:
                        candidate_id = candidate["id"].lower()
                        candidate_text = candidate.get("text", "").lower()
                        candidate_name = candidate.get("name", "").lower()
                        
                        # æ–°å¢ï¼šæ£€æŸ¥å®ä½“åˆ«åï¼Œè¯†åˆ«åŒä¸€å®ä½“ï¼ˆä¿®å¤æ˜ å°„é€»è¾‘ï¼‰
                        entity_group = None
                        for canonical_name, aliases in entity_aliases.items():
                            # æ£€æŸ¥å€™é€‰IDæ˜¯å¦åŒ¹é…è§„èŒƒåç§°
                            if candidate_id.lower() == canonical_name.lower():
                                # æ‰¾åˆ°åŒ¹é…ï¼Œè¿”å›å¯¹åº”çš„ç»†èŠ‚æ•°æ®ID
                                entity_group = aliases[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ«åä½œä¸ºç»†èŠ‚æ•°æ®ID
                                print(f"ğŸ” Entity alias detected: {candidate_id} â†’ {entity_group}")
                                break
                        
                        # æ£€æŸ¥å±äºå“ªä¸ªè¯­ä¹‰ç»„
                        assigned_group = None
                        best_match_score = 0
                        
                        for group_name, keywords in semantic_groups.items():
                            match_score = 0
                            for keyword in keywords:
                                # æ£€æŸ¥å€™é€‰çš„å„ä¸ªå­—æ®µ
                                if keyword in candidate_id:
                                    match_score += 2  # IDåŒ¹é…ç»™äºˆæœ€é«˜æƒé‡
                                if keyword in candidate_text:
                                    match_score += 1.5  # æ–‡æœ¬åŒ¹é…ç»™äºˆé«˜æƒé‡
                                if keyword in candidate_name:
                                    match_score += 1.0  # åç§°åŒ¹é…ç»™äºˆä¸­ç­‰æƒé‡
                            
                            # é€‰æ‹©åŒ¹é…åº¦æœ€é«˜çš„ç»„
                            if match_score > best_match_score:
                                best_match_score = match_score
                                assigned_group = group_name
                        
                        # å¦‚æœåŒ¹é…åº¦å¤ªä½ï¼Œåˆ™ä¸åˆ†ç»„
                        if best_match_score < 1.0:
                            assigned_group = None
                        
                        # ä¼˜å…ˆä½¿ç”¨å®ä½“åˆ«ååˆ†ç»„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨è¯­ä¹‰åˆ†ç»„
                        final_group = entity_group if entity_group else assigned_group
                        
                        if final_group:
                            if final_group not in grouped_candidates:
                                grouped_candidates[final_group] = []
                            grouped_candidates[final_group].append(candidate)
                        else:
                            # ä¸å±äºä»»ä½•ç»„çš„å€™é€‰ï¼Œå•ç‹¬å¤„ç†
                            if "other" not in grouped_candidates:
                                grouped_candidates["other"] = []
                            grouped_candidates["other"].append(candidate)
                    
                    # å¯¹æ¯ä¸ªè¯­ä¹‰ç»„ï¼Œé€‰æ‹©æœ€é«˜åˆ†çš„å€™é€‰
                    deduplicated = []
                    for group_name, group_candidates in grouped_candidates.items():
                        if group_name == "other":
                            # å…¶ä»–å€™é€‰ç›´æ¥æ·»åŠ 
                            deduplicated.extend(group_candidates)
                        else:
                            # è¯­ä¹‰ç»„é€‰æ‹©æœ€é«˜åˆ†çš„
                            if len(group_candidates) > 1:
                                print(f"ğŸ” Semantic deduplication: {group_name} has {len(group_candidates)} candidates")
                                for i, cand in enumerate(group_candidates):
                                    print(f"   {i+1}. {cand['id']} (score: {cand['score']:.3f})")
                            
                            # é€‰æ‹©æœ€é«˜åˆ†çš„å€™é€‰
                            best_candidate = max(group_candidates, key=lambda x: x["score"])
                            best_candidate["semantic_group"] = group_name
                            best_candidate["merged_candidates"] = len(group_candidates)
                            deduplicated.append(best_candidate)
                    
                    print(f"ğŸ” Semantic deduplication: {len(candidates)} â†’ {len(deduplicated)} candidates")
                    return deduplicated
                
                def _calculate_detail_score(self, detail_item, caption_lower):
                    """è®¡ç®—detailåˆ†æ•°ï¼ˆç»†èŠ‚é€šé“ï¼‰- å¢å¼ºç‰ˆ"""
                    score = 0.0
                    
                    # 1. åŸºäºè‡ªç„¶è¯­è¨€æè¿°çš„åˆ†æ•°ï¼ˆå¢å¼ºç©ºé—´æ¦‚å¿µæƒé‡ï¼‰
                    nl_text = detail_item.get("nl_text", "").lower()
                    if nl_text:
                        # å…³é”®è¯åŒ¹é…
                        keywords = nl_text.split()
                        for keyword in keywords:
                            if keyword in caption_lower:
                                # ç©ºé—´æ¦‚å¿µç»™äºˆæ›´é«˜æƒé‡
                                if any(space_word in keyword for space_word in ["open", "space", "area", "large", "atrium", "cluster"]):
                                    score += 0.3  # ç©ºé—´æ¦‚å¿µæƒé‡ä»0.2æå‡åˆ°0.3
                                else:
                                    score += 0.2
                            elif len(keyword) > 3 and any(word in caption_lower for word in keyword.split()):
                                if any(space_word in keyword for space_word in ["open", "space", "area", "large", "atrium", "cluster"]):
                                    score += 0.15  # ç©ºé—´æ¦‚å¿µæƒé‡ä»0.1æå‡åˆ°0.15
                                else:
                                    score += 0.1
                    
                    # 2. åŸºäºç»“æ„åŒ–æ–‡æœ¬çš„åˆ†æ•°
                    struct_text = detail_item.get("struct_text", "").lower()
                    if struct_text:
                        # è§£æç»“æ„åŒ–ç‰¹å¾
                        struct_features = struct_text.split(";")
                        for feature in struct_features:
                            if feature.strip() and any(word in caption_lower for word in feature.split()):
                                score += 0.15
                    
                    # 3. åŸºäºç©ºé—´å…³ç³»çš„åˆ†æ•°
                    spatial_info = detail_item.get("spatial_info", {})
                    for relation, landmark in spatial_info.items():
                        if landmark and any(word in caption_lower for word in str(landmark).split()):
                            score += 0.1
                    
                    # 4. æ–°å¢ï¼šç©ºé—´æ¦‚å¿µè¯­ä¹‰åŒ¹é…ï¼ˆä¸ç»“æ„é€šé“ä¿æŒä¸€è‡´ï¼‰
                    space_concepts = {
                        "large open space": ["open", "space", "large", "area", "atrium", "cluster"],
                        "open space": ["open", "space", "area", "atrium"],
                        "open area": ["open", "area", "space", "atrium"],
                        "atrium": ["atrium", "open", "space", "area"]
                    }
                    
                    for caption_concept, keywords in space_concepts.items():
                        if caption_concept in caption_lower:
                            # æ£€æŸ¥detailæ–‡æœ¬æ˜¯å¦åŒ…å«ç›¸å…³ç©ºé—´æ¦‚å¿µ
                            detail_text = f"{nl_text} {struct_text}"
                            if any(keyword in detail_text for keyword in keywords):
                                score += 0.2  # ç©ºé—´æ¦‚å¿µè¯­ä¹‰åŒ¹é…ç»™äºˆé¢å¤–æƒé‡
                                break
                    
                    return min(1.0, score)  # é™åˆ¶æœ€å¤§åˆ†æ•°ä¸º1.0
            
            UNIFIED_RETRIEVER = EnhancedDualChannelRetriever()
            
            # ä¿®å¤ï¼šç¡®ä¿detail_indexå·²æ„å»º
            if not hasattr(UNIFIED_RETRIEVER, 'detail_index'):
                print("ğŸ”§ æ„å»ºdetail_index...")
                UNIFIED_RETRIEVER.detail_index = UNIFIED_RETRIEVER._build_detail_index()
            
            print("âœ… Calibrated dual-channel retriever initialized successfully")
            return UNIFIED_RETRIEVER
            
        except Exception as e:
            print(f"âš ï¸  Failed to load unified retriever: {e}")
            print("âš ï¸  Falling back to legacy retrieval")
            return None
    return UNIFIED_RETRIEVER

# Legacy scene loading (fallback)
def load_scene_index(scene_id: str):
    """Legacy function - kept for backward compatibility"""
    npz = np.load(os.path.join(MODEL_DIR, f"{scene_id}.npz"), allow_pickle=True)
    X = npz["X"].astype(np.float32)
    texts = npz["texts"].tolist()
    ids = json.loads(open(os.path.join(MODEL_DIR, f"{scene_id}.ids.json"), "r", encoding="utf-8").read())
    if USE_FAISS:
        index = faiss.read_index(os.path.join(MODEL_DIR, f"{scene_id}.faiss"))
        return {"index": index, "X": X, "texts": texts, "ids": ids}
    else:
        nn = NearestNeighbors(n_neighbors=5, metric="cosine").fit(X)
        return {"index": nn, "X": X, "texts": texts, "ids": ids}

# Legacy scene loading (fallback)
SCENE = {
    "SCENE_A_MS": load_scene_index("SCENE_A_MS"),
    "SCENE_B_STUDIO": load_scene_index("SCENE_B_STUDIO"),
}

# ---------- BLIP caption (Local Model) ----------
# Initialize local BLIP model
try:
    processor = BlipProcessor.from_pretrained(BLIP_MODEL_PATH)
    model = BlipForConditionalGeneration.from_pretrained(BLIP_MODEL_PATH)
    print(f"âœ“ Loaded local BLIP model successfully: {BLIP_MODEL_PATH}")
    print(f"âœ“ Using device: {BLIP_DEVICE}")
except Exception as e:
    print(f"âš  Failed to load local BLIP model: {e}")
    print("âš  Image captioning will be disabled")
    processor = None
    model = None

def hf_caption(image_bytes: bytes) -> str:
    if processor is None or model is None:
        return "an indoor workspace with desks and shelves"
    
    try:
        # Convert bytes to PIL Image
        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        inputs = processor(image, return_tensors="pt")
        out = model.generate(**inputs)
        caption = processor.decode(out[0], skip_special_tokens=True)
        return caption
    except Exception as e:
        print(f"âš  Error in local BLIP captioning: {e}")
        return "an indoor workspace with desks and shelves"

def guess_bearing_from_caption(caption: str) -> str:
    t = caption.lower()
    if "left" in t: return "left"
    if "right" in t: return "right"
    if "behind" in t or "back" in t: return "behind"
    return "ahead"

# ---------- ASR (faster-whisper) ----------
from faster_whisper import WhisperModel

# Set environment variables to handle Hugging Face Hub issues
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
os.environ["HF_HUB_OFFLINE"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # Avoid deadlock warnings

# Initialize ASR model with proper error handling
ASR = None
try:
    # Try to load from local cache first
    ASR = WhisperModel("small", device="cpu", compute_type="int8", local_files_only=True)
    print("âœ“ Loaded faster-whisper model from local cache")
except Exception as e:
    print(f"âš  Local cache not found: {e}")
    try:
        # Try to download with explicit settings
        print("Attempting to download faster-whisper model...")
        ASR = WhisperModel("small", device="cpu", compute_type="int8")
        print("âœ“ Successfully downloaded and loaded faster-whisper model")
    except Exception as download_error:
        print(f"Failed to download 'small' model: {download_error}")
        try:
            # Try a different model that might be more accessible
            print("Trying alternative model 'tiny'...")
            ASR = WhisperModel("tiny", device="cpu", compute_type="int8")
            print("âœ“ Successfully loaded 'tiny' model as fallback")
        except Exception as alt_error:
            print(f"Failed to download alternative model: {alt_error}")
            print("ASR functionality will be disabled. Please check your internet connection or Hugging Face credentials.")
            print("Alternative: Try using a different model or check if you have Hugging Face credentials set up.")
            # Create a dummy ASR object to prevent crashes
            class DummyASR:
                def transcribe(self, *args, **kwargs):
                    raise Exception("ASR model not available - download failed")
            ASR = DummyASR()

def webm_to_wav_16k_mono(data: bytes) -> bytes:
    """Convert WebM audio to WAV format with better error handling"""
    try:
        with tempfile.NamedTemporaryFile(suffix=".webm", delete=False) as fin, tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as fout:
            fin.write(data)
            fin.flush()
            
            # Use more robust ffmpeg command with better error handling
            cmd = [
                "ffmpeg",
                "-loglevel", "warning",  # Show warnings for debugging
                "-y",  # Overwrite output file
                "-i", fin.name,
                "-ac", "1",  # Mono
                "-ar", "16000",  # 16kHz sample rate
                "-f", "wav",  # Force WAV format
                fout.name
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, check=False)
            
            if result.returncode != 0:
                print(f"FFmpeg error: {result.stderr}")
                print(f"FFmpeg stdout: {result.stdout}")
                raise Exception(f"FFmpeg failed with return code {result.returncode}")
            
            # Read the converted file
            with open(fout.name, 'rb') as f:
                wav_data = f.read()
            
            # Clean up temporary files
            os.unlink(fin.name)
            os.unlink(fout.name)
            
            return wav_data
            
    except Exception as e:
        print(f"Audio conversion error: {e}")
        # Clean up any remaining temp files
        try:
            if 'fin' in locals(): os.unlink(fin.name)
            if 'fout' in locals(): os.unlink(fout.name)
        except:
            pass
        raise e

def asr_bytes_to_text(data: bytes) -> str:
    """Convert audio bytes to text with improved error handling"""
    if not data or len(data) == 0:
        print("Empty audio data received")
        return ""
    
    print(f"Processing audio: {len(data)} bytes")
    
    # Check if it's already WAV format
    buf = data
    if not (len(buf) > 12 and buf[:4] == b"RIFF" and b"WAVE" in buf[:12]):
        print("Converting WebM to WAV...")
        try:
            buf = webm_to_wav_16k_mono(data)
            print(f"Conversion successful: {len(buf)} bytes WAV")
        except Exception as e:
            print(f"WebM to WAV conversion failed: {e}")
            # Try to use original data as fallback
            buf = data
    
    try:
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            f.write(buf)
            f.flush()
            
            print(f"Transcribing audio file: {f.name}")
            segments, info = ASR.transcribe(f.name, beam_size=1, vad_filter=True)
            text = "".join([s.text for s in segments]).strip()
            
            # Clean up temp file
            os.unlink(f.name)
            
            print(f"Transcription result: '{text}'")
            return text
            
    except Exception as e:
        print(f"ASR transcription error: {e}")
        # Clean up temp file
        try:
            if 'f' in locals(): os.unlink(f.name)
        except:
            pass
        raise e

# ---------- LLM fallback (intent only) ----------
from openai import OpenAI
OAI = OpenAI(api_key=LLM_KEY)
INTENTS = ["repeat","lost","confirm_a","confirm_b","confirm_neither","to_atrium","distance_a","hazard_boxes","to_window","to_chair","distance_b","hazard_cable"]
SYS_PROMPT = (
    "Return JSON only. Identify the user's intent for indoor navigation.\n"
    f"Allowed intents = {INTENTS}.\n"
    "If nothing matches, return {\"intent\":\"unknown\"}."
)
def llm_intent(text: str) -> str:
    if not text.strip(): return "unknown"
    resp = OAI.chat.completions.create(
        model=LLM_MODEL, temperature=LLM_TEMP,
        response_format={"type":"json_object"},
        messages=[{"role":"system","content":SYS_PROMPT},{"role":"user","content":text}],
    )
    try:
        j = json.loads(resp.choices[0].message.content)
        return j.get("intent","unknown")
    except Exception:
        return "unknown"

# ---------- FastAPI ----------
app = FastAPI(title="VLN4VI Backend", version="1.0.0")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, restrict this
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    """Health check endpoint for monitoring"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "services": {
            "asr_model": "loaded" if ASR else "not_loaded",
            "blip_model": "loaded" if processor else "not_loaded",
            "enhanced_dual_channel_retriever": "available" if get_unified_retriever() else "not_available"
        }
    }

# TTS start endpoint for end-to-end latency tracking
class TTSMark(BaseModel):
    req_id: str
    session_id: str
    site_id: str
    provider: str  # âœ… æ–°å¢ provider å­—æ®µ
    client_start_ms: int
    client_tts_start_ms: int

@app.post("/api/metrics/tts_start")
def api_tts_start(mark: TTSMark):
    """Record TTS start for end-to-end latency calculation"""
    paths = _log_paths(mark.provider)
    _ensure_headers(paths)
    
    e2e = int(mark.client_tts_start_ms) - int(mark.client_start_ms)
    
    # âœ… Only write when logging is enabled
    enabled, run_id = _is_logging(mark.session_id, mark.provider)
    if enabled:
        with open(paths["latency"], "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                mark.site_id, run_id, datetime.utcnow().isoformat(),
                mark.req_id, mark.session_id, mark.provider,
                "trial",  # phase: trial phase for subsequent photos
                mark.client_start_ms, mark.client_tts_start_ms, e2e
            ])
    
    print(f"ğŸ“Š TTS start recorded: req_id={mark.req_id}, e2e={e2e}ms, logging={enabled}")
    return {"ok": True, "e2e_latency_ms": e2e}

# âœ… New: Logging control API endpoints
class LogSwitchIn(BaseModel):
    session_id: str
    provider: str = "ft"
    enabled: bool
    run_id: str = ""     # Optional, keep previous if not provided

@app.post("/api/logging/set")
def api_logging_set(body: LogSwitchIn):
    """Set logging record switch"""
    key = (body.session_id, body.provider.lower())
    cur = LOG_SWITCH[key]
    cur["enabled"] = bool(body.enabled)
    if body.run_id:
        cur["run_id"] = body.run_id
    
    status = "ON" if cur["enabled"] else "OFF"
    run_id_info = f" (run_id: {cur['run_id']})" if cur["run_id"] else ""
    print(f"ğŸ”§ Logging {status} for session={body.session_id}, provider={body.provider}{run_id_info}")
    
    return {"ok": True, "state": cur, "key": {"session_id": body.session_id, "provider": body.provider.lower()}}

@app.get("/api/logging/status")
def api_logging_status(session_id: str, provider: str = "ft"):
    """Query logging record status"""
    st = LOG_SWITCH[(session_id, provider.lower())]
    return {"ok": True, "state": st}

# âœ… æ–°å¢ï¼šRQ3 æ¾„æ¸…å¯¹è¯ç®¡ç†ç«¯ç‚¹
class ClarificationRound(BaseModel):
    clarification_id: str
    session_id: str
    site_id: str
    round_count: int
    user_question: str
    system_answer: str
    predicted_node: str
    gt_node_id: str = None

@app.post("/api/metrics/clarification_round")
def api_clarification_round(round_data: ClarificationRound):
    """Record a clarification dialogue round"""
    record_clarification_round(
        round_data.clarification_id,
        round_data.session_id,
        round_data.site_id,
        round_data.round_count,
        round_data.user_question,
        round_data.system_answer,
        round_data.predicted_node,
        round_data.gt_node_id
    )
    
    print(f"ğŸ” Clarification round recorded: {round_data.clarification_id}, round {round_data.round_count}")
    return {"ok": True, "round_recorded": round_data.round_count}

class ClarificationEnd(BaseModel):
    clarification_id: str
    session_id: str
    site_id: str
    total_rounds: int
    final_predicted_node: str
    gt_node_id: str

@app.post("/api/metrics/clarification_end")
def api_clarification_end(end_data: ClarificationEnd):
    """End a clarification session and record success rate"""
    end_clarification_session(
        end_data.clarification_id,
        end_data.session_id,
        end_data.site_id,
        end_data.total_rounds,
        end_data.final_predicted_node,
        end_data.gt_node_id
    )
    
    return {"ok": True, "session_ended": end_data.clarification_id}

# âœ… æ–°å¢ï¼šRQ3 é”™è¯¯æ¢å¤ç®¡ç†ç«¯ç‚¹
class ErrorRecoveryStart(BaseModel):
    session_id: str
    site_id: str
    error_node: str
    correct_node: str

@app.post("/api/metrics/error_recovery_start")
def api_error_recovery_start(recovery_data: ErrorRecoveryStart):
    """Start error recovery timing"""
    recovery_id = start_error_recovery(
        recovery_data.session_id,
        recovery_data.site_id,
        recovery_data.error_node,
        recovery_data.correct_node
    )
    
    return {"ok": True, "recovery_id": recovery_id}

class ErrorRecoveryEnd(BaseModel):
    recovery_id: str
    session_id: str
    site_id: str
    correct_node: str
    recovery_path: str = ""

@app.post("/api/metrics/error_recovery_end")
def api_error_recovery_end(recovery_data: ErrorRecoveryEnd):
    """End error recovery timing and calculate duration"""
    duration = end_error_recovery(
        recovery_data.recovery_id,
        recovery_data.session_id,
        recovery_data.site_id,
        recovery_data.correct_node,
        recovery_data.recovery_path
    )
    
    return {"ok": True, "recovery_duration_ms": duration}

class StartIn(BaseModel):
    session_id: str
    site_id: str               # SCENE_A_MS / SCENE_B_STUDIO
    opening_provider: str      # base / ft
    lang: str = "en"           # en / zh

@app.post("/api/start")
def api_start(body: StartIn):
    # âœ… New: Enhanced session initialization with location tracking
    SESSIONS[body.session_id] = {
        "site_id": body.site_id, 
        "opening_provider": body.opening_provider, 
        "lang": body.lang,
        "current_location": None,           # å½“å‰é¢„æµ‹ä½ç½®
        "location_history": [],             # ä½ç½®å†å²è®°å½•
        "orientation_history": [],          # æœå‘å†å²è®°å½•
        "confidence_history": [],           # ç½®ä¿¡åº¦å†å²è®°å½•
        "last_update_time": datetime.utcnow().isoformat(),  # æœ€åæ›´æ–°æ—¶é—´
        "photo_count": 0                   # æ‹ç…§è®¡æ•°
    }
    
    # Initialize photo count tracking
    session_key = f"{body.session_id}_{body.opening_provider}_{body.site_id}"
    if "_photo_count" not in SESSIONS:
        SESSIONS["_photo_count"] = {}
    SESSIONS["_photo_count"][session_key] = 0
    
    table = HARD_OUTPUTS_EN if body.lang=="en" else HARD_OUTPUTS_ZH
    say = table[body.site_id][body.opening_provider]
    return {"mode":"orient","say":[say],"site_id":body.site_id,"opening_provider":body.opening_provider,"lang":body.lang}

@app.post("/api/locate")
async def api_locate(
    site_id: str = Form(...),
    image: UploadFile = File(...),
    session_id: str = Form("T1"),      # Allow session_id, default T1
    provider: str = Form("ft"),        # Frontend can pass (or get from session)
    gt_node_id: str = Form(None),      # âœ… New: ground truth label (optional)
    client_start_ms: int = Form(None), # âœ… New: client start timestamp
    req_id: str = Form(None),          # âœ… New: request ID for tracking
    first_photo: bool = Form(False)    # âœ… New: whether this is the first photo
):
    # Generate request ID if not provided
    req_id = req_id or str(uuid.uuid4())
    server_recv_ms = _now_ms()
    
    print(f"ğŸ” API locate called: site_id={site_id}, provider={provider}, first_photo={first_photo}, session_id={session_id}")
    
    # âœ… Check if this is the first photo
    if first_photo:
        print(f"ğŸ“¸ First photo detected for {provider}_{site_id}")
        
        # First photo: return traditional preset output from JSONL files
        try:
            # Get image and generate BLIP caption for logging purposes only
            img = await image.read()
            cap = hf_caption(img)
            print(f"ğŸ“¸ BLIP caption for first photo (logging only): {cap[:100]}...")
            
            # ğŸ”§ FIXED: Use traditional preset output for first photo, not AI reasoning
            preset_output = get_preset_output(provider, site_id)
            print(f"ğŸ“š First photo preset output for {provider}_{site_id}: {preset_output[:100]}...")
        except Exception as e:
            print(f"âš ï¸ Failed to get preset output, using fallback: {e}")
            # Fall back to simple welcome message
            preset_output = f"Welcome to {site_id}! Please take a photo to start exploring."
            print(f"ğŸ“š Fallback preset output: {preset_output}")
        
        # ğŸ”§ Record warmup phase (first photo) for tracking
        paths = _log_paths(provider)
        _ensure_headers(paths)
        
        # Always log warmup phase, regardless of logging switch
        with open(paths["locate"], "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                site_id, "WARMUP", datetime.utcnow().isoformat(), req_id, session_id, provider,
                "warmup",  # phase
                "First photo - preset output",  # caption
                "", "", "", "", "",  # top1, top2, margin
                "", "", "", "",  # gt_node_id, hit_top1, hit_top2, hit_hop1
                "", "",  # low_conf, low_conf_rule
                client_start_ms or "", server_recv_ms, _now_ms()  # timing
            ])
        
        print(f"ğŸ“ Warmup phase logged for {provider}_{site_id}")
        
        # âœ… New: Collect DG metrics for first photo
        try:
            enhanced_metrics_collector.collect_real_time_data(
                MetricType.USER_BEHAVIOR,
                session_id,
                {
                    "action": "first_photo",
                    "site_id": site_id,
                    "provider": provider,
                    "preset_output_used": True,
                    "photo_count": 1
                },
                priority=DataPriority.HIGH,
                tags=["navigation", "first_photo", "preset_output"]
            )
            
            # Record DG1 evaluation (No Hardware Dependency)
            if dg_evaluator:
                dg_evaluator.dg1_evaluator.record_setup_process(
                    session_id=session_id,
                    steps=["photo_capture", "preset_output_display"],
                    time_taken=0,
                    success=True
                )
        except Exception as e:
            print(f"âš ï¸ Failed to collect DG metrics for first photo: {e}")
        
        return {
            "req_id": req_id,
            "caption": cap if 'cap' in locals() else "First photo - preset output",
            "node_id": None,
            "confidence": 1.0,
            "low_conf": False,
            "preset_output": preset_output,
            "is_first_photo": True,
            "retrieval_method": "preset_output"
        }
    
    # ğŸ”§ FORCE FIRST PHOTO DETECTION: If this is a new session, treat as first photo
    session_key = f"{session_id}_{provider}_{site_id}"
    if session_key not in SESSIONS.get("_photo_count", {}):
        if "_photo_count" not in SESSIONS:
            SESSIONS["_photo_count"] = {}
        SESSIONS["_photo_count"][session_key] = 0
    
    photo_count = SESSIONS["_photo_count"][session_key]
    if photo_count == 0:
        print(f"ğŸ”§ FORCE DETECTION: First photo for session {session_key}")
        SESSIONS["_photo_count"][session_key] = 1
        
        # First photo: return traditional preset output from JSONL files
        try:
            # Get image and generate BLIP caption for logging purposes only
            img = await image.read()
            cap = hf_caption(img)
            print(f"ğŸ“¸ BLIP caption for first photo (logging only): {cap[:100]}...")
            
            # ğŸ”§ FIXED: Use traditional preset output for first photo, not AI reasoning
            preset_output = get_preset_output(provider, site_id)
            print(f"ğŸ“š First photo preset output for {provider}_{site_id}: {preset_output[:100]}...")
        except Exception as e:
            print(f"âš ï¸ Failed to get preset output, using fallback: {e}")
            # Fall back to simple welcome message
            preset_output = f"Welcome to {site_id}! Please take a photo to start exploring."
            print(f"ğŸ“š Fallback preset output: {preset_output}")
        
        # ğŸ”§ Record warmup phase (first photo) for tracking
        paths = _log_paths(provider)
        _ensure_headers(paths)
        
        # Always log warmup phase, regardless of logging switch
        with open(paths["locate"], "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                site_id, "WARMUP", datetime.utcnow().isoformat(), req_id, session_id, provider,
                "warmup",  # phase
                "First photo - preset output",  # caption
                "", "", "", "", "",  # top1, top2, margin
                "", "", "", "",  # gt_node_id, hit_top1, hit_top2, hit_hop1
                "", "", "", "",  # low_conf, low_conf_rule
                client_start_ms or "", server_recv_ms, _now_ms()  # timing
            ])
        
        print(f"ğŸ“ Warmup phase logged for {provider}_{site_id}")
        
        return {
            "req_id": req_id,
            "caption": cap if 'cap' in locals() else "First photo - preset output",
            "node_id": None,
            "confidence": 1.0,
            "low_conf": False,
            "preset_output": preset_output,
            "is_first_photo": True,
            "retrieval_method": "preset_output"
        }
    
    # 1) Get image â†’ BLIP generate caption (for subsequent photos)
    try:
        img = await image.read()
        cap = hf_caption(img)
    except Exception as e:
        # Log failure
        # paths is already initialized above
        
        # âœ… Only write when logging is enabled
        enabled, run_id = _is_logging(session_id, provider)
        if enabled:
            with open(paths["locate"], "a", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow([
                    site_id, run_id, datetime.utcnow().isoformat(), req_id, session_id, provider,
                    f"BLIP_FAILED:{e}", "", "", "", "", "", gt_node_id or "", "", "", "",
                    True, f"BLIP_failed:{e}", client_start_ms or "", server_recv_ms, _now_ms()
                ])
        raise HTTPException(status_code=400, detail=f"BLIP failed: {e}")
    
    # ğŸ”§ Increment photo count for this session
    SESSIONS["_photo_count"][session_key] += 1
    print(f"ğŸ“¸ Photo #{SESSIONS['_photo_count'][session_key]} for session {session_key}")
    
    # 2) Try unified dual-channel retrieval first
    # Initialize paths early to avoid UnboundLocalError
    paths = _log_paths(provider)
    _ensure_headers(paths)
    
    retriever = get_unified_retriever()
    if retriever:
        try:
            # âœ… Layered Fusion: Structure for localization + Detail for conversation enhancement
            if provider.lower() == "ft":
                print(f"ğŸ—ï¸ FT mode detected for {site_id} - using Enhanced Dual-Channel Fusion mode")
                
                # Phase 1: Structure-only localization (Sense_A_Finetuned.fixed.jsonl or Sense_B_Finetuned.fixed.jsonl)
                matching_data = get_matching_data(provider, site_id)
                print(f"ğŸ” Using Structure data from {PRESET_OUTPUTS.get(f'{provider.lower()}_{site_id}')}")
                
                # Phase 2: Load Detail data for post-localization conversation enhancement
                # Detail files (Sense_A_MS.jsonl, Sense_B_Studio.jsonl) are NOT used for scoring
                detailed_data = get_detailed_matching_data(site_id)
                print(f"ğŸ” Detailæ•°æ®åŠ è½½ç»“æœ: {len(detailed_data) if detailed_data else 0} æ¡è®°å½•")
                if detailed_data:
                    print(f"ğŸ” Loaded Detail data from Detail file for conversation enhancement")
                    # éªŒè¯å‰å‡ ä¸ªèŠ‚ç‚¹çš„detailæ•°æ®
                    if site_id == "SCENE_A_MS":
                        test_nodes = ["chair_on_yline", "desks_cluster", "dp_ms_entrance"]
                    elif site_id == "SCENE_B_STUDIO":
                        test_nodes = ["dp_studio_entrance", "workstation_zone", "glass_cage_room"]
                    else:
                        test_nodes = []
                    
                    for node_id in test_nodes:
                        node_details = [item for item in detailed_data if item.get("node_hint") == node_id]
                        print(f"   {node_id}: {len(node_details)} é¡¹detailæ•°æ®")
                else:
                    print(f"âš ï¸ Detailæ•°æ®åŠ è½½å¤±è´¥ï¼")
                
                # Use layered fusion retrieval: Structure-only scoring + Detail metadata attachment
                candidates = enhanced_ft_retrieval(cap, retriever, site_id, detailed_data)
            else:
                # Standard retrieval for other modes (base/4o)
                matching_data = get_matching_data(provider, site_id)
                if matching_data:
                    print(f"ğŸ” Using matching data from {PRESET_OUTPUTS.get(f'{provider.lower()}_{site_id}')}")
                
                # Use enhanced dual-channel retrieval with scene filtering
                try:
                    # è·å–èåˆåçš„å€™é€‰åˆ—è¡¨
                    candidates = retriever.retrieve(cap, top_k=10, scene_filter=site_id)
                    
                    if not candidates:
                        print("âŒ æ— æ³•è·å–å€™é€‰åˆ—è¡¨")
                        candidates = None
                    else:
                        print(f"âœ… æˆåŠŸè·å–å€™é€‰åˆ—è¡¨: {len(candidates)} ä¸ªå€™é€‰")
                        
                except Exception as e:
                    print(f"âš ï¸ Enhanced dual-channel retrieval failed: {e}")
                    # ä¸å›é€€ï¼Œä¸æ”¹ä½ç½®ï¼›åªæ²¿ç”¨ä¸Šä¸€å¸§ï¼ˆæˆ–ç½®ä¸ºä½ç½®ä¿¡å¾…æ¾„æ¸…ï¼‰
                    candidates = None
            
            if candidates:
                # ğŸ”§ NEW: Apply softmax calibration and continuity boost for enhanced dual-channel confidence scoring
                print(f"ğŸ”§ Applying enhanced confidence calculation for {len(candidates)} candidates")
                
                # Phase 1: Softmax calibration
                calibrated_confidence, calibrated_margin, raw_top1_score, raw_top2_score = calculate_calibrated_confidence_and_margin(candidates, top_k=5)
                
                # Phase 2: Extract basic candidate info
                top1 = candidates[0]
                top1_id = top1["id"]
                top1_score = float(top1["score"])
                
                # ä¿®å¤ï¼šæ·»åŠ æ–­è¨€å¼æ—¥å¿—ï¼Œç¡®ä¿çŠ¶æ€ä¸€è‡´
                has_detail = top1.get("has_detail", False)
                print(f"ğŸ”§ [ASSERT] top1={top1_id}, s1={top1_score:.4f}, has_detail={has_detail}")
                
                # éªŒè¯çŠ¶æ€ä¸€è‡´æ€§
                assert isinstance(top1_id, str) and isinstance(top1_score, float) and isinstance(has_detail, bool), \
                    f"çŠ¶æ€ç±»å‹é”™è¯¯: top1_id={type(top1_id)}, top1_score={type(top1_score)}, has_detail={type(has_detail)}"
                
                # Get top2 if available
                top2 = candidates[1] if len(candidates) > 1 else None
                top2_id = top2["id"] if top2 else ""
                top2_score = float(top2["score"]) if top2 else 0.0
                
                # ğŸ”§ NEW: Apply continuity boost to calibrated confidence
                boost_amount, boost_reason = apply_continuity_boost(
                    calibrated_confidence, session_id, site_id, top1_id
                )
                
                # Use boosted calibrated confidence for final decision
                final_confidence = calibrated_confidence + boost_amount
                final_margin = calibrated_margin
                
                print(f"ğŸ”§ Final confidence calculation:")
                print(f"   Raw scores: top1={raw_top1_score:.4f}, top2={raw_top2_score:.4f}")
                print(f"   Calibrated: confidence={calibrated_confidence:.4f}, margin={calibrated_margin:.4f}")
                print(f"   Continuity boost: {boost_amount:.4f} ({boost_reason})")
                print(f"   Final: confidence={final_confidence:.4f}, margin={final_margin:.4f}")
                
                # Determine confidence level using configurable thresholds
                # ğŸ”§ FIX: ä½¿ç”¨æ–°çš„é˜ˆå€¼ï¼šconfidence > 40% ä¸” margin > 5% å°±ä¸è§¦å‘low_conf
                low_conf = final_confidence < LOWCONF_SCORE_TH or final_margin < LOWCONF_MARGIN_TH
                low_conf_rule = f"score<{LOWCONF_SCORE_TH*100:.0f}% OR margin<{LOWCONF_MARGIN_TH*100:.0f}%" if low_conf else f"confidence>{LOWCONF_SCORE_TH*100:.0f}% AND margin>{LOWCONF_MARGIN_TH*100:.0f}%"
                
                # Calculate hit metrics
                hit_top1 = (gt_node_id == top1_id) if gt_node_id else ""
                hit_top2 = (gt_node_id in [top1_id, top2_id]) if gt_node_id and top2_id else ""
                hit_hop1 = is_hop1(site_id, top1_id, gt_node_id) if gt_node_id else ""
                
                # âœ… New: RQ3 misbelief rate calculation with enhanced dual-channel fusion
                misbelief = 0
                clarification_triggered = False
                
                # Check for structure-detail consistency (enhanced dual-channel fusion mode)
                structure_detail_conflict = False
                if hasattr(top1, 'structure_score') and hasattr(top1, 'detail_score'):
                    structure_score = getattr(top1, 'structure_score', top1_score)
                    detail_score = getattr(top1, 'detail_score', 0.0)
                    
                    # Check for structure-detail inconsistency
                    if detail_score < structure_score * 0.3:  # Detail score < 30% of structure score
                        structure_detail_conflict = True
                        print(f"âš ï¸ Structure-detail conflict detected: structure={structure_score:.3f}, detail={detail_score:.3f}")
                
                if gt_node_id and top1_id != gt_node_id:
                    # If prediction is wrong, check if clarification dialogue is triggered
                    clarification_triggered = low_conf or structure_detail_conflict
                    misbelief = 1 if not clarification_triggered else 0
                
                # If clarification dialogue is triggered, start clarification session
                clarification_id = None
                if clarification_triggered and gt_node_id:
                    clarification_id = start_clarification_session(
                        session_id, site_id, req_id, top1_id, gt_node_id, provider
                    )
                    
                    # Add structure-detail conflict context to clarification
                    if structure_detail_conflict:
                        print(f"ğŸ” Triggering clarification due to structure-detail conflict")
                        # You can add specific clarification logic here
                
                # Extract bearing from caption
                bearing = "ahead"
                t = cap.lower()
                if "left" in t: bearing = "left"
                elif "right" in t: bearing = "right"
                elif "behind" in t or "back" in t: bearing = "behind"
                
                # âœ… New: Track orientation and update session location
                # ğŸ”§ NEW: åªæœ‰åœ¨ç»Ÿä¸€æ£€ç´¢æˆåŠŸåæ‰æ›´æ–°ä¼šè¯ä½ç½®ï¼Œä¿æŒè¿ç»­æ€§
                orientation_info = track_orientation(session_id, cap, top1_id)
                update_session_location(session_id, top1_id, final_confidence, orientation_info)
                print(f"ğŸ”§ ä¼šè¯ä½ç½®å·²æ›´æ–°: {top1_id} (confidence: {final_confidence:.3f})")
                
                # âœ… New: Collect DG metrics for successful localization
                try:
                    enhanced_metrics_collector.collect_real_time_data(
                        MetricType.USER_BEHAVIOR,
                        session_id,
                        {
                            "action": "photo_localization",
                            "site_id": site_id,
                            "provider": provider,
                            "predicted_location": top1_id,
                            "confidence": final_confidence,
                            "low_conf": low_conf,
                            "photo_count": photo_count,
                            "gt_node_id": gt_node_id,
                            "hit_top1": hit_top1 == "True",
                            "margin": final_margin
                        },
                        priority=DataPriority.HIGH,
                        tags=["navigation", "localization", "photo_analysis"]
                    )
                    
                    # Record DG3 evaluation (Useful Precision in Localization)
                    if dg_evaluator:
                        dg_evaluator.dg3_evaluator.record_interaction_behavior(
                            session_id=session_id,
                            photo_count=photo_count,
                            first_photo_success=(photo_count == 1),
                            adjustment_actions=["photo_capture", "location_prediction"]
                        )
                        
                        # Record trust score based on confidence
                        trust_score = int(final_confidence * 10)  # Convert 0-1 to 0-10
                        dg_evaluator.dg3_evaluator.record_trust_score(
                            session_id=session_id,
                            trust_score=trust_score,
                            context="photo_localization"
                        )
                    
                    # Record user needs validation data
                    user_needs_validator.record_validation_data(
                        session_id,
                        UserNeed.N2_POSITIONING_ACCURACY,
                        "positioning_error",
                        1.0 if hit_top1 == "True" else 2.0  # Simplified error estimation
                    )
                    
                    user_needs_validator.record_validation_data(
                        session_id,
                        UserNeed.N2_POSITIONING_ACCURACY,
                        "task_completion_rate",
                        1.0 if hit_top1 == "True" else 0.5
                    )
                    
                except Exception as e:
                    print(f"âš ï¸ Failed to collect DG metrics for localization: {e}")
                
                # âœ… Detect language from caption and provider
                detected_lang = detect_language_from_caption(cap, provider)
                
                # âœ… Generate dynamic navigation response based on hierarchical fusion results
                navigation_response = generate_dynamic_navigation_response(
                    site_id, top1_id, final_confidence, low_conf, matching_data, detected_lang, top1
                )
                
                # Format response with detailed scoring and navigation
                response = {
                    "req_id": req_id,
                    "caption": cap,
                    "node_id": top1_id,
                    "confidence": final_confidence,  # ğŸ”§ Use calibrated + boosted confidence
                    "low_conf": low_conf,
                    "bearing": bearing,
                    "margin": final_margin,  # ğŸ”§ Use calibrated margin
                    "clarification_id": clarification_id,  # âœ… New: clarification session ID
                    "navigation_instruction": navigation_response,  # âœ… New: dynamic navigation instruction
                    "current_location": get_location_description(top1_id, site_id) if 'get_location_description' in globals() else f"Current location: {top1_id}",  # âœ… New: current location description
                    "next_action": get_next_action(top1_id, site_id, detected_lang),  # âœ… New: next action instruction
                    "candidates": [
                        {
                            "id": c["id"],
                            "score": float(c["score"]),  # âœ… Convert numpy types to Python native types
                            "s_nl": float(c["score_nl"]),
                            "s_struct": float(c["score_struct"]),
                            "provider": c["provider"],
                            "bonus_keywords": float(c["bonus_keywords"]),
                            "bonus_bearing": float(c["bonus_bearing"]),
                            "alpha_used": float(c["alpha_used"]),
                            "retrieval_method": c.get("retrieval_method", "unknown"),  # âœ… New: retrieval method info
                            "detailed_match_score": c.get("detailed_match_score", None)  # âœ… New: detailed matching score
                        }
                        for c in candidates[:10]
                    ],
                    "retrieval_method": "enhanced_ft_dual_retrieval" if provider.lower() == "ft" and site_id == "SCENE_A_MS" else "unified_dual_channel_fusion",
                    # ğŸ”§ NEW: Add calibration and boost information
                    "calibration_info": {
                        "raw_scores": {"top1": raw_top1_score, "top2": raw_top2_score},
                        "calibrated": {"confidence": calibrated_confidence, "margin": calibrated_margin},
                        "continuity_boost": {"amount": boost_amount, "reason": boost_reason},
                        "final": {"confidence": final_confidence, "margin": final_margin}
                    }
                }
                
                # Write comprehensive log with RQ3 data
                server_resp_ms = _now_ms()
                
                # âœ… Only write when logging is enabled
                enabled, run_id = _is_logging(session_id, provider)
                if enabled:
                    # ğŸ”§ NEW: Enhanced logging with similarity distribution analysis
                    top3_score = float(candidates[2]["score"]) if len(candidates) > 2 else 0.0
                    top4_score = float(candidates[3]["score"]) if len(candidates) > 3 else 0.0
                    top5_score = float(candidates[4]["score"]) if len(candidates) > 4 else 0.0
                    
                    # Calculate similarity distribution metrics
                    score_range = raw_top1_score - top5_score if len(candidates) > 4 else raw_top1_score - top2_score
                    score_variance = np.var([raw_top1_score, raw_top2_score, top3_score, top4_score, top5_score]) if len(candidates) > 4 else np.var([raw_top1_score, raw_top2_score])
                    
                    with open(paths["locate"], "a", newline="", encoding="utf-8") as f:
                        csv.writer(f).writerow([
                            site_id, run_id, datetime.utcnow().isoformat(), req_id, session_id, provider,
                            "trial",  # phase: trial phase for subsequent photos
                            cap,
                            top1_id, f"{final_confidence:.6f}",  # ğŸ”§ Use final calibrated confidence
                            top2_id, f"{raw_top2_score:.6f}",
                            f"{final_margin:.6f}",  # ğŸ”§ Use final calibrated margin
                            gt_node_id or "",
                            str(hit_top1).lower(), str(hit_top2).lower(), str(hit_hop1).lower(),
                            str(low_conf).lower(), low_conf_rule if low_conf else "",
                            client_start_ms or "", server_recv_ms, server_resp_ms
                        ])
                    
                    # ğŸ”§ NEW: Log detailed similarity distribution for analysis
                    similarity_log_path = os.path.join(os.path.dirname(paths["locate"]), "similarity_distribution.csv")
                    similarity_headers = [
                        "site_id", "run_id", "ts_iso", "req_id", "session_id", "provider",
                        "raw_top1", "raw_top2", "raw_top3", "raw_top4", "raw_top5",
                        "calibrated_conf", "calibrated_margin", "boost_amount", "boost_reason",
                        "final_conf", "final_margin", "score_range", "score_variance",
                        "low_conf", "low_conf_rule", "gt_node_id", "hit_top1"
                    ]
                    
                    # Ensure similarity distribution log headers
                    if not os.path.exists(similarity_log_path):
                        with open(similarity_log_path, "w", newline="", encoding="utf-8") as f:
                            csv.writer(f).writerow(similarity_headers)
                    
                    # Log similarity distribution data
                    with open(similarity_log_path, "a", newline="", encoding="utf-8") as f:
                        csv.writer(f).writerow([
                            site_id, run_id, datetime.utcnow().isoformat(), req_id, session_id, provider,
                            f"{raw_top1_score:.6f}", f"{raw_top2_score:.6f}", f"{top3_score:.6f}", f"{top4_score:.6f}", f"{top5_score:.6f}",
                            f"{calibrated_confidence:.6f}", f"{calibrated_margin:.6f}", f"{boost_amount:.6f}", boost_reason,
                            f"{final_confidence:.6f}", f"{final_margin:.6f}", f"{score_range:.6f}", f"{score_variance:.6f}",
                            str(low_conf).lower(), low_conf_rule if low_conf else "", gt_node_id or "", str(hit_top1).lower()
                        ])
                    
                    print(f"ğŸ“ Trial phase logged to {paths['locate']} (run_id: {run_id})")
                    print(f"ğŸ“Š Similarity distribution logged to {similarity_log_path}")
                    print(f"   Raw scores: [{raw_top1_score:.4f}, {raw_top2_score:.4f}, {top3_score:.4f}, {top4_score:.4f}, {top5_score:.4f}]")
                    print(f"   Score range: {score_range:.4f}, Variance: {score_variance:.4f}")
                else:
                    print(f"ğŸ“ Logging disabled for session={session_id}, provider={provider}")
                
                print(f"âœ“ Unified dual-channel retrieval successful for {site_id}")
                print(f"  Top candidate: {top1['text'][:50]}... (score: {final_confidence:.3f})")
                print(f"  Provider: {top1['provider']}, Alpha: {top1['alpha_used']:.2f}")
                print(f"  Margin: {final_margin:.3f}, Low conf: {low_conf}")
                if gt_node_id:
                    print(f"  Ground truth: {gt_node_id}")
                    print(f"  Hit Top1: {hit_top1}, Hit Top2: {hit_top2}, Hit Â±1-Hop: {hit_hop1}")
                    print(f"  Misbelief: {misbelief}, Clarification: {clarification_triggered}")
                    if clarification_id:
                        print(f"  Clarification session: {clarification_id}")
                
                return response
            else:
                # No candidates found
                server_resp_ms = _now_ms()
                
                # âœ… Only write when logging is enabled
                enabled, run_id = _is_logging(session_id, provider)
                if enabled:
                    with open(paths["locate"], "a", newline="", encoding="utf-8") as f:
                        csv.writer(f).writerow([
                            site_id, run_id, datetime.utcnow().isoformat(), req_id, session_id, provider,
                            cap,
                            "", "0.0",
                            "", "0.0",
                            "0.0",
                            gt_node_id or "",
                            "", "", "",
                            "true", "no_candidates",
                            client_start_ms or "", server_recv_ms, server_resp_ms
                        ])
                
                return {
                    "req_id": req_id,
                    "caption": cap,
                    "node_id": None,
                    "confidence": 0.0,
                    "low_conf": True,
                    "candidates": [],
                    "retrieval_method": "unified_dual_channel_fusion"
                }
                
        except Exception as e:
            print(f"âš  Unified dual-channel retrieval failed: {e}")
            print("âš  å¼‚å¸¸â†’æ²¿ç”¨å·²ç®—å‡ºçš„fused top-1ï¼Œä¸å›é€€åˆ°legacy")
            print("âš  ä¿æŒä¸Šä¸€ä¸ªç¨³å®šä½ç½®çŠ¶æ€ï¼Œä¸æ›´æ–°ä¼šè¯ä½ç½®")
            # ä¸å›é€€åˆ°legacyï¼Œé¿å…ä½ç½®æ ‡ç­¾æ¼‚ç§»æ‰“æ–­continuity
            use_legacy = False
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å›é€€åˆ°legacy
    if 'use_legacy' in locals() and not use_legacy:
        print("ğŸ”§ è·³è¿‡legacyå›é€€ï¼Œä¿æŒfused top-1ç»“æœ")
        # ğŸ”§ NEW: å¦‚æœæœ‰æˆåŠŸçš„æ£€ç´¢ç»“æœï¼Œè¿”å›å®é™…ç»“æœè€Œä¸æ˜¯é»˜è®¤å€¼
        if 'candidates' in locals() and candidates and len(candidates) > 0:
            top1 = candidates[0]
            top1_id = top1.get("id", "unknown")
            top1_score = float(top1.get("score", 0.0))
            
            # ğŸ”§ NEW: è®¡ç®—marginå€¼
            top2_score = 0.0
            if len(candidates) > 1:
                top2_score = float(candidates[1].get("score", 0.0))
            margin = max(0.0, top1_score - top2_score)
            
            # ğŸ”§ NEW: å†…å®¹ç›¸å…³æ€§æ£€æŸ¥
            caption_lower = cap.lower()
            top1_node = None
            for candidate in candidates:
                if candidate.get("id") == top1_id:
                    top1_node = candidate
                    break
            
            # å¦‚æœtop1ä¸captionå†…å®¹ä¸åŒ¹é…ï¼Œé™ä½ç½®ä¿¡åº¦
            if top1_node and 'text' in top1_node:
                node_text = top1_node['text'].lower()
                content_match_score = 0
                
                # æ£€æŸ¥å…³é”®è¯åŒ¹é…
                caption_words = set(caption_lower.split())
                node_words = set(node_text.split())
                common_words = caption_words.intersection(node_words)
                
                if len(common_words) > 0:
                    content_match_score = len(common_words) / max(len(caption_words), len(node_words))
                
                            # å¦‚æœå†…å®¹åŒ¹é…åº¦ä½ï¼Œé™ä½ç½®ä¿¡åº¦
            if content_match_score < 0.15:  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼åˆ°0.15
                adjusted_confidence = top1_score * 0.6  # é™ä½40%ï¼ˆæ›´ä¸¥æ ¼ï¼‰
                print(f"âš ï¸ å†…å®¹åŒ¹é…åº¦ä½({content_match_score:.2f})ï¼Œè°ƒæ•´ç½®ä¿¡åº¦: {top1_score:.3f} â†’ {adjusted_confidence:.3f}")
                top1_score = adjusted_confidence
            
            # ğŸ”§ NEW: é¢å¤–çš„è¯­ä¹‰æ£€æŸ¥ - å¦‚æœcaptionåŒ…å«"desk"ä½†top1ä¸æ˜¯deskç›¸å…³ï¼Œå¤§å¹…é™ä½ç½®ä¿¡åº¦
            caption_lower = cap.lower()
            if "desk" in caption_lower and "desk" not in top1_node.get('text', '').lower():
                # å¦‚æœå›¾ç‰‡æè¿°åŒ…å«"desk"ä½†è¯†åˆ«ç»“æœä¸æ˜¯deskç›¸å…³ï¼Œå¤§å¹…é™ä½ç½®ä¿¡åº¦
                adjusted_confidence = top1_score * 0.5  # é™ä½50%
                print(f"âš ï¸ è¯­ä¹‰ä¸åŒ¹é…ï¼šå›¾ç‰‡åŒ…å«'desk'ä½†è¯†åˆ«ä¸º'{top1_id}'ï¼Œå¤§å¹…é™ä½ç½®ä¿¡åº¦: {top1_score:.3f} â†’ {adjusted_confidence:.3f}")
                top1_score = adjusted_confidence
            
            print(f"ğŸ”§ è¿”å›æˆåŠŸçš„fused top-1ç»“æœ: {top1_id} (confidence: {top1_score:.3f}, margin: {margin:.3f})")
            return {
                "req_id": req_id,
                "caption": cap,
                "node_id": top1_id,
                "confidence": top1_score,
                "margin": margin,
                "low_conf": top1_score < LOWCONF_SCORE_TH,
                "candidates": candidates,
                "retrieval_method": "fused_top1_success"
            }
        else:
            # å¦‚æœæ²¡æœ‰å€™é€‰ç»“æœï¼Œæ‰è¿”å›é»˜è®¤å€¼
            print("ğŸ”§ æ²¡æœ‰å€™é€‰ç»“æœï¼Œè¿”å›é»˜è®¤å“åº”")
            return {
                "req_id": req_id,
                "caption": cap,
                "node_id": "unknown",
                "confidence": 0.0,
                "low_conf": True,
                "candidates": [],
                "retrieval_method": "fused_top1_no_candidates"
            }
    
    # Fallback to legacy retrieval
    print("Using legacy retrieval system")
    from sentence_transformers import SentenceTransformer
    def embed_text(t: str):
        return EMB.encode([t], normalize_embeddings=True, convert_to_numpy=True)[0].astype(np.float32).reshape(1,-1)
    item = SCENE[site_id]
    v = embed_text(cap)
    if "faiss" in str(type(item["index"])).lower():
        D, I = item["index"].search(v, 4)
        sims, idxs = D[0].tolist(), I[0].tolist()
    else:
        D, I = item["index"].kneighbors(v, n_neighbors=4, return_distance=True)
        sims = (1 - D[0]).tolist(); idxs = I[0].tolist()
    cands = []
    for s,i in zip(sims,idxs):
        meta = item["ids"][i]
        cands.append({"id": meta["id"], "type": meta["type"], "text": meta["text"], "score": float(s)})
    
    # Calculate confidence metrics for legacy system
    top1_score = cands[0]["score"] if cands else 0.0
    top2_score = cands[1]["score"] if len(cands) > 1 else 0.0
    margin = top1_score - top2_score
    # ğŸ”§ FIX: ä½¿ç”¨æ–°çš„é˜ˆå€¼ï¼šconfidence > 40% ä¸” margin > 5% å°±ä¸è§¦å‘low_conf
    low_conf = (len(cands)==0) or (top1_score < LOWCONF_SCORE_TH or margin < LOWCONF_MARGIN_TH)
    low_conf_rule = f"score<{LOWCONF_SCORE_TH*100:.0f}% OR margin<{LOWCONF_MARGIN_TH*100:.0f}%" if low_conf else f"confidence>{LOWCONF_SCORE_TH*100:.0f}% AND margin>{LOWCONF_MARGIN_TH*100:.0f}%"
    
    # Calculate hit metrics for legacy system
    top1_id = cands[0]["id"] if cands else ""
    top2_id = cands[1]["id"] if len(cands) > 1 else ""
    hit_top1 = (gt_node_id == top1_id) if gt_node_id else ""
    hit_top2 = (gt_node_id in [top1_id, top2_id]) if gt_node_id and top2_id else ""
    hit_hop1 = is_hop1(site_id, top1_id, gt_node_id) if gt_node_id else ""
    
    bearing = "ahead"
    t = cap.lower()
    if "left" in t: bearing = "left"
    elif "right" in t: bearing = "right"
    elif "behind" in t or "back" in t: bearing = "behind"
    
    # âœ… New: Track orientation and update session location for legacy system
    orientation_info = track_orientation(session_id, cap, top1_id)
    update_session_location(session_id, top1_id, top1_score, orientation_info)
    
    # Write comprehensive log for legacy system
    server_resp_ms = _now_ms()
    
    # âœ… Only write when logging is enabled
    enabled, run_id = _is_logging(session_id, provider)
    if enabled:
        with open(paths["locate"], "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                site_id, run_id, datetime.utcnow().isoformat(), req_id, session_id, provider,
                cap,
                top1_id, f"{top1_score:.6f}",
                top2_id, f"{top2_score:.6f}",
                f"{margin:.6f}",
                gt_node_id or "",
                str(hit_top1).lower(), str(hit_top2).lower(), str(hit_hop1).lower(),
                str(low_conf).lower(), low_conf_rule if low_conf else "",
                client_start_ms or "", server_recv_ms, server_resp_ms
            ])
    
    return {
        "req_id": req_id,
        "caption": cap, 
        "node_id": top1_id, 
        "confidence": top1_score,
        "low_conf": low_conf,
        "bearing": bearing, 
        "margin": margin,
        "candidates": cands,
        "retrieval_method": "legacy_fallback"
    }

@app.post("/api/asr")
async def api_asr(audio: UploadFile = File(...)):
    try:
        b = await audio.read()
        print(f"Received audio file: {audio.filename}, size: {len(b)} bytes")
        
        if not b or len(b) == 0:
            print("Empty audio file received")
            return {"text": "", "error": "empty_audio"}
        
        # Check file size (reasonable limit: 10MB)
        if len(b) > 10 * 1024 * 1024:
            print(f"Audio file too large: {len(b)} bytes")
            return {"text": "", "error": "file_too_large"}
        
        text = asr_bytes_to_text(b)
        if not text.strip():
            print("No speech detected in audio")
            return {"text": "", "error": "no_speech"}
        
        print(f"ASR successful: '{text}'")
        return {"text": text}
        
    except Exception as e:
        print(f"ASR API error: {e}")
        import traceback
        traceback.print_exc()
        return {"text": "", "error": f"asr_error: {str(e)}"}

class QAIn(BaseModel):
    session_id: str
    text: str
    lang: str = "en"

def generate_navigation_context(site_id: str, lang: str = "en") -> str:
    """Generate dynamic navigation context based on current site and language"""
    if site_id == "SCENE_A_MS":
        if lang == "zh":
            return """ä½ æ˜¯ä¸€ä¸ªå®¤å†…å¯¼èˆªåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·åœ¨ Maker Space ä¸­å¯¼èˆªã€‚

å½“å‰ä½ç½®ï¼šMaker Space å…¥å£å†…ä¾§
å¯ç”¨åœ°æ ‡ï¼š
- 3Dæ‰“å°æœºå·¥ä½œå°ï¼šç›´è¡Œçº¦6æ­¥ï¼Œçº¦4ç±³
- ç»ç’ƒé—¨åˆ°ä¸­åº­ï¼šç»§ç»­ç›´è¡Œçº¦7æ­¥ï¼Œçº¦5ç±³
- äºŒç»´ç ä¹¦æ¶ï¼šåœ¨ä½ çš„å·¦ä¾§
- ç»„ä»¶æŠ½å±‰å¢™ï¼šåœ¨ä½ çš„å³ä¾§
- ç»¿è‰²å›æ”¶ç®±ï¼šå…¥å£é™„è¿‘
- åœ°é¢çº¸ç®±ï¼šç¬¬5æ­¥é™„è¿‘ï¼Œéœ€è¦å‡é€Ÿç»•è¡Œ

ç”¨æˆ·å¯ä»¥é€šè¿‡æ‹ç…§æ¥æ›´æ–°å½“å‰ä½ç½®ä¿¡æ¯ã€‚"""
        else:
            return """You are an indoor navigation assistant, specifically helping users navigate in the Maker Space.

Current location: Inside the Maker Space entrance
Available landmarks:
- 3D printer table: Walk straight about 6 steps, roughly 4 meters
- Glass doors to atrium: Continue straight about 7 more steps, roughly 5 meters
- QR-code bookshelf: To your left
- Component drawer wall: To your right
- Green recycling bin: Near the entrance
- Box hazard: Near step 5, slow down and bypass

Users can take photos to update their current location information."""
    
    elif site_id == "SCENE_B_STUDIO":
        if lang == "zh":
            return """ä½ æ˜¯ä¸€ä¸ªå®¤å†…å¯¼èˆªåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·åœ¨å·¥ä½œå®¤å†…å¯¼èˆªã€‚

å½“å‰ä½ç½®ï¼šå·¥ä½œå®¤å…¥å£
å¯ç”¨åœ°æ ‡ï¼š
- å¤§çª—ï¼šå‘å‰5æ­¥ï¼Œçº¦3.5ç±³
- æ©™è‰²æ²™å‘æ—çš„æ¤…å­ï¼šå·¦è½¬åèµ°5æ­¥ï¼Œçº¦3.5ç±³
- åœ°é¢ç”µç¼†ï¼šç¬¬3æ­¥é™„è¿‘ï¼Œéœ€è¦å‡é€Ÿ

ç”¨æˆ·å¯ä»¥é€šè¿‡æ‹ç…§æ¥æ›´æ–°å½“å‰ä½ç½®ä¿¡æ¯ã€‚"""
        else:
            return """You are an indoor navigation assistant, specifically helping users navigate in the studio.

Current location: Studio entrance
Available landmarks:
- Large window: Walk forward 5 steps, roughly 3.5 meters
- Chair beside orange sofa: Turn left and walk 5 steps, roughly 3.5 meters
- Floor cable: Near step 3, slow down

Users can take photos to update their current location information."""
    
    else:
        if lang == "zh":
            return "ä½ æ˜¯ä¸€ä¸ªå®¤å†…å¯¼èˆªåŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·è¿›è¡Œå®¤å†…å¯¼èˆªã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›æ¸…æ™°çš„æŒ‡å¯¼ã€‚"
        else:
            return "You are an indoor navigation assistant that can help users with indoor navigation. Please provide clear guidance based on user questions."

@app.post("/api/qa")
async def api_qa(body: QAIn):
    """Dynamic QA using GPT with enhanced location context"""
    try:
        sess = SESSIONS.get(body.session_id, {})
        site_id = sess.get("site_id", "SCENE_A_MS")
        lang = "zh" if body.lang.lower().startswith("zh") else "en"
        
        print(f"QA request: session={body.session_id}, site={site_id}, lang={lang}, text='{body.text}'")
        
        # âœ… New: Generate enhanced location context with secondary location verification
        location_context = generate_location_context_prompt(body.session_id, body.text, site_id, lang)
        print(f"ğŸ“ Generated location context: {location_context[:200]}...")
        
        # Create enhanced prompt for GPT with location verification
        if lang == "zh":
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¤å†…å¯¼èˆªåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·åœ¨ {site_id} ä¸­å¯¼èˆªã€‚

{location_context}

è¯·æ ¹æ®ä»¥ä¸Šè¯¦ç»†çš„ä½ç½®ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›ï¼š
1. åŸºäºå½“å‰ä½ç½®çš„æŠ½è±¡å¯¼èˆªæŒ‡å¯¼ï¼ˆä¸è¦æåŠå…·ä½“åœ°ç‚¹åç§°ï¼‰
2. è€ƒè™‘ç”¨æˆ·å½“å‰æœå‘çš„è½¬å‘å»ºè®®
3. å¦‚æœä½ç½®ä¿¡æ¯ä¸æ˜ç¡®ï¼Œå»ºè®®ç”¨æˆ·é‡æ–°æ‹ç…§ç¡®è®¤
4. æä¾›é€šç”¨çš„ç§»åŠ¨æŒ‡å¯¼ï¼Œå¦‚"å‘å‰èµ°å‡ æ­¥"ã€"å‘å·¦è½¬"ç­‰
5. åŸºäºä½ç½®ç¨³å®šæ€§ç»™å‡ºç›¸åº”çš„å»ºè®®

å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé€‚åˆè¯­éŸ³æ’­æŠ¥ï¼Œä½¿ç”¨æŠ½è±¡çš„æ–¹å‘å’Œè·ç¦»æè¿°ã€‚"""
        else:
            prompt = f"""You are a professional indoor navigation assistant, specifically helping users navigate in {site_id}.

{location_context}

Please provide:
1. Abstract navigation guidance based on current location (avoid specific location names)
2. Turn-by-turn guidance considering user's current orientation
3. Suggestion to retake photo if location is unclear
4. General movement guidance like "walk forward a few steps", "turn left", etc.
5. Recommendations based on location stability

Keep your answer concise and suitable for voice output, using abstract direction and distance descriptions."""
        
        print(f"GPT enhanced prompt: {prompt[:300]}...")
        
        # Call GPT for dynamic response with location context
        response = OAI.chat.completions.create(
            model=LLM_MODEL,
            temperature=LLM_TEMP,
            messages=[
                {"role": "system", "content": "You are a helpful indoor navigation assistant with precise location awareness."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200  # Increased for more detailed navigation guidance
        )
        
        answer = response.choices[0].message.content.strip()
        print(f"GPT response with location context: {answer}")
        
        # âœ… New: Log the location-aware QA interaction
        try:
            session = SESSIONS.get(body.session_id, {})
            current_location = session.get("current_location", "unknown")
            
            # Log to clarification log if available
            paths = _log_paths(session.get("opening_provider", "ft"))
            _ensure_headers(paths)
            
            enabled, run_id = _is_logging(body.session_id, session.get("opening_provider", "ft"))
            if enabled:
                with open(paths["clar"], "a", newline="", encoding="utf-8") as f:
                    csv.writer(f).writerow([
                        site_id, run_id, datetime.utcnow().isoformat(), 
                        f"qa_{uuid.uuid4()}", body.session_id, session.get("opening_provider", "ft"),
                        "qa_location_aware",  # phase
                        f"qa_{uuid.uuid4()}", 1, "location_qa",  # req_id, round_idx, event
                        body.text, answer,  # user_text, system_text
                        current_location, "",  # resolved_node_id, gt_node_id
                        "success"  # success
                    ])
                print(f"ğŸ“ Location-aware QA logged to {paths['clar']}")
        except Exception as log_error:
            print(f"âš ï¸ Failed to log location-aware QA: {log_error}")
        
        # âœ… New: Collect DG metrics for QA interaction
        try:
            enhanced_metrics_collector.collect_real_time_data(
                MetricType.USER_BEHAVIOR,
                body.session_id,
                {
                    "action": "qa_interaction",
                    "site_id": site_id,
                    "lang": lang,
                    "user_question": body.text,
                    "system_response": answer,
                    "current_location": current_location,
                    "response_source": "gpt_location_aware"
                },
                priority=DataPriority.NORMAL,
                tags=["navigation", "qa", "gpt", "location_aware"]
            )
            
            # Record DG4 evaluation (Segmentable and Repeatable Instructions)
            if dg_evaluator:
                dg_evaluator.dg4_evaluator.record_task_completion(
                    session_id=body.session_id,
                    task_id=f"qa_{uuid.uuid4()}",
                    task_type="navigation_qa",
                    status="completed",
                    completion_time=0,  # Could be enhanced with actual timing
                    veering_count=0
                )
            
            # Record user needs validation data
            user_needs_validator.record_validation_data(
                body.session_id,
                UserNeed.N3_SEGREGATED_INSTRUCTIONS,
                "instruction_clarity",
                4  # Assuming good clarity for GPT responses
            )
            
        except Exception as e:
            print(f"âš ï¸ Failed to collect DG metrics for QA: {e}")
        
        # Return response in the same format as before
        return {
            "mode": "qa",
            "say": [answer],
            "meta": {"source": "gpt_location_aware", "site_id": site_id, "lang": lang, "current_location": current_location},
            "source": "gpt"
        }
        
    except Exception as e:
        print(f"QA API error: {e}")
        import traceback
        traceback.print_exc()
        
        # Fallback response
        if lang == "zh":
            fallback = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·ç¨åå†è¯•ã€‚"
        else:
            fallback = "Sorry, I cannot answer your question right now. Please try again later."
        
        return {
            "mode": "qa",
            "say": [fallback],
            "meta": {"source": "fallback", "site_id": site_id, "lang": lang},
            "source": "fallback"
        }

@app.get("/api/session/location/{session_id}")
async def get_session_location(session_id: str):
    """Get current location and history for a session"""
    if session_id not in SESSIONS:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session = SESSIONS[session_id]
    
    return {
        "session_id": session_id,
        "current_location": session.get("current_location"),
        "site_id": session.get("site_id"),
        "provider": session.get("opening_provider"),
        "last_update": session.get("last_update_time"),
        "photo_count": session.get("photo_count", 0),
        "location_history": session.get("location_history", []),
        "orientation_history": session.get("orientation_history", []),
        "confidence_history": session.get("confidence_history", [])
    }

@app.get("/api/session/status/{session_id}")
async def get_session_status(session_id: str):
    """Get comprehensive session status including location tracking"""
    if session_id not in SESSIONS:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session = SESSIONS[session_id]
    session_key = f"{session_id}_{session.get('opening_provider', 'ft')}_{session.get('site_id', 'SCENE_A_MS')}"
    photo_count = SESSIONS.get("_photo_count", {}).get(session_key, 0)
    
    # Calculate location confidence trend
    confidence_history = session.get("confidence_history", [])
    confidence_trend = "stable"
    if len(confidence_history) >= 2:
        recent_avg = sum(confidence_history[-3:]) / min(3, len(confidence_history))
        earlier_avg = sum(confidence_history[:-3]) / max(1, len(confidence_history) - 3)
        if recent_avg > earlier_avg + 0.1:
            confidence_trend = "improving"
        elif recent_avg < earlier_avg - 0.1:
            confidence_trend = "declining"
    
    return {
        "session_id": session_id,
        "site_id": session.get("site_id"),
        "provider": session.get("opening_provider"),
        "current_location": session.get("current_location"),
        "photo_count": photo_count,
        "last_update": session.get("last_update_time"),
        "confidence_trend": confidence_trend,
        "location_stability": len(session.get("location_history", [])),
        "orientation_consistency": all(
            o.get("consistent", True) for o in session.get("orientation_history", [])
        )
    }

@app.get("/api/location/verify/{session_id}")
async def verify_location_and_distance(session_id: str, destination: str = None):
    """Verify user location and calculate distance to destination"""
    if session_id not in SESSIONS:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session = SESSIONS[session_id]
    current_location = session.get("current_location")
    site_id = session.get("site_id")
    
    if not current_location:
        return {
            "session_id": session_id,
            "current_location": None,
            "location_verified": False,
            "message": "No location information available. Please take a photo first.",
            "suggestion": "Take a new photo to establish your current location"
        }
    
    # Get location verification details
    location_history = session.get("location_history", [])
    recent_locations = [h["location"] for h in location_history[-3:]] if location_history else []
    location_consistency = len(set(recent_locations)) <= 2 if recent_locations else True
    
    # Calculate distance to destination if specified
    distance_info = None
    if destination and current_location:
        distance_info = get_location_distance(current_location, destination, site_id)
    
    # Generate verification summary
    verification_summary = {
        "session_id": session_id,
        "current_location": current_location,
        "site_id": site_id,
        "location_verified": True,
        "location_consistency": "consistent" if location_consistency else "inconsistent",
        "recent_locations": recent_locations,
        "confidence": session.get("confidence_history", [0])[-1] if session.get("confidence_history") else 0,
        "last_update": session.get("last_update_time"),
        "suggestion": "Location looks good" if location_consistency else "Consider retaking photo for better accuracy"
    }
    
    if distance_info and "error" not in distance_info:
        verification_summary["destination"] = destination
        verification_summary["distance"] = distance_info
        verification_summary["navigation_ready"] = True
    elif destination:
        verification_summary["destination"] = destination
        verification_summary["distance"] = {"error": "Route not found"}
        verification_summary["navigation_ready"] = False
    
    return verification_summary

@app.get("/api/location/navigate/{session_id}")
async def get_navigation_instructions(session_id: str, destination: str):
    """Get detailed navigation instructions from current location to destination"""
    if session_id not in SESSIONS:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session = SESSIONS[session_id]
    current_location = session.get("current_location")
    site_id = session.get("site_id")
    
    if not current_location:
        raise HTTPException(status_code=400, detail="Current location not available. Please take a photo first.")
    
    # Get distance information
    distance_info = get_location_distance(current_location, destination, site_id)
    
    if "error" in distance_info:
        return {
            "session_id": session_id,
            "from": current_location,
            "to": destination,
            "error": distance_info["error"],
            "message": f"Cannot find route from {current_location} to {destination}",
            "suggestion": "Please check the destination name or take a new photo to update your location"
        }
    
    # Generate navigation instructions
    if site_id == "SCENE_A_MS":
        # Get language from session
        lang = session.get("lang", "en")
        
        if lang == "zh":
            instructions = f"""ä»{current_location}åˆ°{destination}çš„å¯¼èˆªæŒ‡å¯¼ï¼š
            
1. å½“å‰ä½ç½®ï¼š{current_location}
2. ç›®æ ‡ä½ç½®ï¼š{destination}
3. è·ç¦»ï¼šçº¦{distance_info['steps']}æ­¥ ({distance_info['meters']}ç±³)
4. æ–¹å‘ï¼š{distance_info['direction']}
5. é¢„è®¡æ—¶é—´ï¼š{distance_info['estimated_time']}

å¯¼èˆªæ­¥éª¤ï¼š
- é¢å‘{distance_info['direction']}æ–¹å‘
- ç¼“æ…¢å‰è¿›ï¼Œæ³¨æ„åœ°é¢éšœç¢ç‰©
- æ¯æ­¥çº¦0.7ç±³ï¼Œä¿æŒç¨³å®šèŠ‚å¥
- åˆ°è¾¾ç›®æ ‡ä½ç½®åæ‹ç…§ç¡®è®¤"""
        else:
            instructions = f"""Navigation from {current_location} to {destination}:
            
1. Current Location: {current_location}
2. Destination: {destination}
3. Distance: About {distance_info['steps']} steps ({distance_info['meters']} meters)
4. Direction: {distance_info['direction']}
5. Estimated Time: {distance_info['estimated_time']}

Navigation Steps:
- Face {distance_info['direction']}
- Walk slowly, watch for ground obstacles
- Each step is about 0.7 meters, maintain steady pace
- Take photo to confirm arrival at destination"""
    else:
        instructions = f"Navigation instructions for {site_id} are not yet implemented."
    
    return {
        "session_id": session_id,
        "from": current_location,
        "to": destination,
        "distance": distance_info,
        "instructions": instructions,
        "navigation_ready": True
    }

# Startup instructions (execute in project root directory):
# uvicorn backend.app:app --reload --host 0.0.0.0 --port 8000

# âœ… New: Language Detection and Dynamic Navigation Functions
def detect_language_from_caption(caption: str, provider: str) -> str:
    """Detect language from caption text and provider type"""
    caption_lower = caption.lower()
    
    # Check for Chinese characters in caption
    chinese_chars = ['çš„', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æ˜¯', 'äº†', 'åˆ°', 'ä»', 'å‘', 'ä¸Š', 'ä¸‹', 'å·¦', 'å³', 'å‰', 'å']
    if any(char in caption for char in chinese_chars):
        return "zh"
    
    # Check for common English words
    english_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']
    if any(word in caption_lower for word in english_words):
        return "en"
    
    # Default based on provider (ft mode often uses Chinese, base mode often uses English)
    if provider.lower() == "ft":
        return "zh"  # Default to Chinese for ft mode
    else:
        return "en"  # Default to English for base mode

def generate_dynamic_navigation_response(site_id: str, node_id: str, confidence: float, low_conf: bool, matching_data: dict, lang: str = "en", candidate_info: dict = None) -> str:
    """ä¿®å¤ï¼šæ”¹è¿›çš„åŠ¨æ€å¯¼èˆªå“åº”ç”Ÿæˆ"""
    print(f"ğŸ—ï¸ Layered Fusion navigation for {site_id} - {node_id} (conf: {confidence:.3f}, low_conf: {low_conf})")
    
    try:
        # è·å–ç»“æ„ä¿¡æ¯
        structure_info = get_structure_based_location_info(site_id, node_id, lang)
        
        # è·å–detailå¢å¼ºä¿¡æ¯
        detail_info = "None"
        if candidate_info and candidate_info.get("detail_metadata"):
            detail_info = get_detail_based_conversation_enhancement(node_id, candidate_info["detail_metadata"], lang)
        
        print(f"ğŸ” Layered fusion info: structure={confidence:.3f}, detail_items={len(candidate_info.get('detail_metadata', []) if candidate_info else [])}, method={candidate_info.get('retrieval_method', 'unknown') if candidate_info else 'unknown'}")
        print(f"ğŸ§­ Layered fusion response: structure='{structure_info[:50]}...', detail='{detail_info[:50] if detail_info else 'None'}...'")
        
        # ç»„åˆå“åº”
        if detail_info and detail_info != "None":
            response = f"{structure_info} {detail_info}"
        else:
            response = structure_info
            
        return response
        
    except Exception as e:
        print(f"âš ï¸ Layered fusion response generation failed: {e}")
        # å›é€€åˆ°ç®€å•å“åº”
        return f"You are at {node_id}. Please describe what you see around you."

def get_structure_based_location_info(site_id: str, node_id: str, lang: str = "en") -> str:
    """è·å–ä½ç½®ä¿¡æ¯ï¼Œé¿å…NameError"""
    try:
        if site_id == "SCENE_A_MS":
            return generate_scene_a_structure_info(node_id, lang)
        elif site_id == "SCENE_B_STUDIO":
            return generate_scene_b_structure_info(node_id, lang)
        else:
            if lang == "zh":
                return f"å½“å‰ä½ç½®ï¼š{node_id}ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨è¦å»å“ªé‡Œã€‚"
            else:
                return f"Current location: {node_id}. Please tell me where you want to go."
    except NameError:
        # å¦‚æœå‡½æ•°ä¸å­˜åœ¨ï¼Œè¿”å›ç®€å•æè¿°
        if lang == "zh":
            return f"å½“å‰ä½ç½®ï¼š{node_id}ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨è¦å»å“ªé‡Œã€‚"
        else:
            return f"Current location: {node_id}. Please tell me where you want to go."

def get_detail_based_conversation_enhancement(node_id: str, detail_metadata: list, lang: str = "en") -> str:
    """è·å–å¯¹è¯å¢å¼ºä¿¡æ¯ï¼Œé¿å…NameError"""
    try:
        if not detail_metadata:
            return ""
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªdetailé¡¹è¿›è¡Œå¯¹è¯å¢å¼º
        detail_item = detail_metadata[0]
        
        # æå–ç©ºé—´å…³ç³»å’Œç‹¬ç‰¹ç‰¹å¾
        spatial_relations = detail_item.get("spatial_relations", {})
        unique_features = detail_item.get("unique_features", [])
        
        enhancement_parts = []
        
        # æ·»åŠ ç©ºé—´ä¸Šä¸‹æ–‡
        if spatial_relations:
            if "front" in spatial_relations and spatial_relations["front"] != "n/a":
                enhancement_parts.append(f"å‰æ–¹ï¼š{spatial_relations['front']}")
            if "left" in spatial_relations and spatial_relations["left"] != "n/a":
                enhancement_parts.append(f"å·¦ä¾§ï¼š{spatial_relations['left']}")
            if "right" in spatial_relations and spatial_relations["right"] != "n/a":
                enhancement_parts.append(f"å³ä¾§ï¼š{spatial_relations['right']}")
            if "back" in spatial_relations and spatial_relations["back"] != "n/a":
                enhancement_parts.append(f"åæ–¹ï¼š{spatial_relations['back']}")
        
        # æ·»åŠ ç‹¬ç‰¹ç‰¹å¾
        if unique_features:
            features = [f for f in unique_features if f and f != ""]
            if features:
                enhancement_parts.append(f"ç‰¹è‰²ï¼š{', '.join(features)}")
        
        if enhancement_parts:
            if lang == "zh":
                return f"ç¯å¢ƒæè¿°ï¼š{'ï¼›'.join(enhancement_parts)}ã€‚"
            else:
                return f"Environment: {'; '.join(enhancement_parts)}."
        
        return ""
    except Exception as e:
        print(f"âš ï¸ Detail enhancement failed: {e}")
        return ""

def generate_ai_spatial_reasoning(caption: str, provider: str, site_id: str, matching_data: dict, detailed_data: list = None) -> str:
    """Generate AI-powered spatial reasoning based on BLIP caption and textmap analysis"""
    
    print(f"ğŸ” generate_ai_spatial_reasoning called with: provider={provider}, site_id={site_id}")
    
    # Detect language
    lang = detect_language_from_caption(caption, provider)
    print(f"ğŸŒ Detected language: {lang}")
    
    # Get spatial context from textmap
    spatial_context = extract_spatial_context_from_textmap(matching_data, detailed_data, site_id)
    print(f"ğŸ“Š Spatial context keys: {list(spatial_context.keys()) if spatial_context else 'None'}")
    
    # Generate AI reasoning prompt
    reasoning_prompt = create_spatial_reasoning_prompt(caption, spatial_context, site_id, lang)
    print(f"ğŸ“ Generated prompt (first 200 chars): {reasoning_prompt[:200]}...")
    
    # Use AI to generate spatial reasoning (simulated for now)
    ai_reasoning = simulate_ai_spatial_reasoning(reasoning_prompt, lang)
    print(f"ğŸ¤– Final AI reasoning output: {ai_reasoning[:100]}...")
    
    return ai_reasoning

def extract_spatial_context_from_textmap(matching_data: dict, detailed_data: list, site_id: str) -> dict:
    """Extract spatial context from textmap data"""
    context = {
        "topology": {},
        "landmarks": {},
        "spatial_relationships": {},
        "navigation_policy": {},
        "current_environment": {}
    }
    
    if matching_data:
        # Extract topology information
        if "topology" in matching_data:
            context["topology"] = matching_data["topology"]
        
        # Extract landmarks
        if "landmarks" in matching_data:
            context["landmarks"] = matching_data["landmarks"]
        
        # Extract navigation policy
        if "navigation_policy" in matching_data:
            context["navigation_policy"] = matching_data["navigation_policy"]
    
    if detailed_data and site_id == "SCENE_A_MS":
        # Extract detailed spatial information
        context["detailed_descriptions"] = []
        for item in detailed_data:
            if "nl_text" in item and "struct_text" in item:
                context["detailed_descriptions"].append({
                    "id": item["id"],
                    "natural_language": item["nl_text"],
                    "structured": item["struct_text"]
                })
    
    return context

def create_spatial_reasoning_prompt(caption: str, spatial_context: dict, site_id: str, lang: str) -> str:
    """Create AI prompt for spatial reasoning"""
    
    if lang == "zh":
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç©ºé—´æ¨ç†AIåŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›æ™ºèƒ½çš„ç©ºé—´åˆ†æå’Œå¯¼èˆªæŒ‡å¯¼ï¼š

**ç”¨æˆ·æ‹æ‘„çš„å›¾åƒæè¿°**: {caption}

**å½“å‰åœºæ™¯**: {site_id}

**ç©ºé—´æ‹“æ‰‘ä¿¡æ¯**: {json.dumps(spatial_context.get('topology', {}), ensure_ascii=False, indent=2)}

**åœ°æ ‡ä¿¡æ¯**: {json.dumps(spatial_context.get('landmarks', {}), ensure_ascii=False, indent=2)}

**å¯¼èˆªç­–ç•¥**: {json.dumps(spatial_context.get('navigation_policy', {}), ensure_ascii=False, indent=2)}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯è¿›è¡Œç©ºé—´æ¨ç†ï¼Œå¹¶ç»™å‡ºï¼š
1. å½“å‰ä½ç½®åˆ†æ
2. å‘¨å›´ç¯å¢ƒæè¿°
3. å¯ç”¨çš„å¯¼èˆªé€‰é¡¹
4. ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®

å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé€‚åˆè¯­éŸ³æ’­æŠ¥ã€‚"""
    else:
        prompt = f"""You are a professional spatial reasoning AI assistant. Based on the following information, provide intelligent spatial analysis and navigation guidance for the user:

**User's Image Description**: {caption}

**Current Scene**: {site_id}

**Spatial Topology**: {json.dumps(spatial_context.get('topology', {}), indent=2)}

**Landmark Information**: {json.dumps(spatial_context.get('landmarks', {}), indent=2)}

**Navigation Policy**: {json.dumps(spatial_context.get('navigation_policy', {}), indent=2)}

Please perform spatial reasoning based on the above information and provide:
1. Current location analysis
2. Surrounding environment description
3. Available navigation options
4. Next action recommendations

Keep your answer concise and suitable for voice output."""
    
    return prompt

def simulate_ai_spatial_reasoning(prompt: str, lang: str) -> str:
    """Simulate AI spatial reasoning (placeholder for actual AI integration)"""
    
    # This is a simulation - in production, you would call an actual AI service
    # For now, we'll use intelligent rule-based reasoning
    
    # ğŸ”§ FIXED: More precise scene detection using regex pattern matching
    import re
    
    # Extract scene_id from the prompt more precisely
    scene_match = re.search(r'\*\*å½“å‰åœºæ™¯\*\*:\s*(\w+)|Current Scene.*?:\s*(\w+)', prompt)
    if scene_match:
        detected_scene = scene_match.group(1) or scene_match.group(2)
        print(f"ğŸ” Detected scene from prompt: {detected_scene}")
    else:
        # Fallback: check if prompt contains scene information
        if "SCENE_A_MS" in prompt and "SCENE_B_STUDIO" not in prompt:
            detected_scene = "SCENE_A_MS"
        elif "SCENE_B_STUDIO" in prompt and "SCENE_A_MS" not in prompt:
            detected_scene = "SCENE_B_STUDIO"
        else:
            # Default to SCENE_A_MS if ambiguous
            detected_scene = "SCENE_A_MS"
            print(f"âš ï¸ Ambiguous scene detection, defaulting to: {detected_scene}")
    
    print(f"ğŸ¯ Final scene determination: {detected_scene}")
    
    if detected_scene == "SCENE_A_MS":
        if lang == "zh":
            return """åŸºäºæ‚¨çš„ç…§ç‰‡å’Œç©ºé—´åˆ†æï¼Œæˆ‘è¯†åˆ«å‡ºæ‚¨å½“å‰åœ¨Maker Spaceç¯å¢ƒä¸­ã€‚

**ç©ºé—´åˆ†æ**ï¼š
- æ‚¨ä½äºä¸€ä¸ªç°ä»£åŒ–çš„åˆ¶é€ åˆ›æ–°å·¥ä½œç©ºé—´
- å‘¨å›´æœ‰3Dæ‰“å°è®¾å¤‡ã€å·¥ä½œå°å’Œå­˜å‚¨ç³»ç»Ÿ
- ç©ºé—´å¸ƒå±€å¼€æ”¾ï¼Œä¾¿äºåä½œå’Œåˆ¶ä½œ

**ç¯å¢ƒç‰¹å¾**ï¼š
- æ˜äº®çš„ç…§æ˜ç³»ç»Ÿ
- å·¥ä¸šé£æ ¼çš„å¤©èŠ±æ¿ï¼Œæš´éœ²çš„ç®¡é“å’Œè£…ç½®
- ç°è‰²ä¹™çƒ¯åŸºåœ°æ¿ï¼Œå¸¦æœ‰é»„è‰²çº¿æ¡æ ‡è®°æ´»åŠ¨åŒºåŸŸ

**å¯ç”¨å¯¼èˆªé€‰é¡¹**ï¼š
1. å‘å‰ç›´è¡Œçº¦4æ­¥åˆ°è¾¾3Dæ‰“å°æœºæ¡Œ
2. å³è½¬çº¦2æ­¥åˆ°è¾¾ç»„ä»¶æŠ½å±‰å¢™
3. å·¦è½¬çº¦2æ­¥åˆ°è¾¾äºŒç»´ç ä¹¦æ¶åŒºåŸŸ

**å»ºè®®è¡ŒåŠ¨**ï¼šæ ¹æ®æ‚¨çš„ç›®æ ‡ï¼Œæˆ‘å»ºè®®å…ˆç›´è¡Œåˆ°3Dæ‰“å°æœºæ¡Œï¼Œé‚£é‡Œæ˜¯ç©ºé—´çš„æ ¸å¿ƒå·¥ä½œåŒºåŸŸã€‚"""
        else:
            return """Based on your photo and spatial analysis, I've identified that you're currently in a Maker Space environment.

**Spatial Analysis**:
- You're located in a modern manufacturing and innovation workspace
- Surrounded by 3D printing equipment, workbenches, and storage systems
- Open space layout conducive to collaboration and fabrication

**Environmental Features**:
- Bright lighting system
- Industrial-style ceiling with exposed pipes and fixtures
- Gray vinyl flooring with yellow lines marking activity areas

**Available Navigation Options**:
1. Walk straight forward about 4 steps to reach the 3D printer table
2. Turn right about 2 steps to reach the component drawer wall
3. Turn left about 2 steps to reach the QR code bookshelf area

**Recommended Action**: Based on your goal, I suggest walking straight to the 3D printer table, which is the core work area of the space."""
    
    elif detected_scene == "SCENE_B_STUDIO":
        if lang == "zh":
            return """åŸºäºæ‚¨çš„ç…§ç‰‡å’Œç©ºé—´åˆ†æï¼Œæˆ‘è¯†åˆ«å‡ºæ‚¨å½“å‰åœ¨å·¥ä½œå®¤å·¥ä½œç¯å¢ƒä¸­ã€‚

**ç©ºé—´åˆ†æ**ï¼š
- æ‚¨ä½äºä¸€ä¸ªå¤šåŠŸèƒ½å·¥ä½œå’Œä¼šè®®ç¯å¢ƒ
- ç»“åˆäº†åŠå…¬ã€ç ”å‘å’Œä¼‘é—²åŒºåŸŸ
- ç©ºé—´å¸ƒå±€çµæ´»ï¼Œæ”¯æŒåˆ›æ„åä½œ

**ç¯å¢ƒç‰¹å¾**ï¼š
- å¤§è½åœ°çª—è®©é˜³å…‰å……è¶³
- ç‹¬ç‰¹è®¾è®¡çš„ç»¿è‰²å’Œè“ç»¿è‰²ä¼‘é—²æ¤…
- å¤šæ˜¾ç¤ºå™¨å·¥ä½œç«™å’ŒåŠå…¬æ¤…

**å¯ç”¨å¯¼èˆªé€‰é¡¹**ï¼š
1. å‘å‰ç›´è¡Œçº¦5æ­¥åˆ°è¾¾å¤§çª—åŒºåŸŸ
2. å·¦è½¬çº¦5æ­¥åˆ°è¾¾æ©™è‰²æ²™å‘æ—çš„æ¤…å­
3. ç›´è¡Œçº¦3æ­¥åˆ°è¾¾å·¥ä½œå®¤ä¸­å¤®åŒºåŸŸ

**å»ºè®®è¡ŒåŠ¨**ï¼šæ ¹æ®æ‚¨çš„ç›®æ ‡ï¼Œæˆ‘å»ºè®®å…ˆå‘å‰ç›´è¡Œåˆ°å¤§çª—åŒºåŸŸï¼Œé‚£é‡Œè§†é‡å¼€é˜”ï¼Œé€‚åˆè§‚å¯Ÿå’Œæ€è€ƒã€‚"""
        else:
            return """Based on your photo and spatial analysis, I've identified that you're currently in a studio workspace environment.

**Spatial Analysis**:
- You're located in a multifunctional work and meeting environment
- Combines office, research and development, and leisure areas
- Flexible space layout supporting creative collaboration

**Environmental Features**:
- Large floor-to-ceiling windows allowing abundant sunlight
- Uniquely designed green and teal lounge chairs
- Multi-monitor workstations and office chairs

**Available Navigation Options**:
1. Walk straight forward about 5 steps to reach the large window area
2. Turn left about 5 steps to reach the chair beside the orange sofa
3. Walk straight about 3 steps to reach the central studio area

**Recommended Action**: Based on your goal, I suggest walking straight forward to the large window area, which offers an open view and is ideal for observation and reflection."""
    
    else:
        print(f"âš ï¸ Unknown scene detected: {detected_scene}, using default response")
        if lang == "zh":
            return "åŸºäºæ‚¨çš„ç…§ç‰‡ï¼Œæˆ‘æ­£åœ¨åˆ†æå½“å‰ç©ºé—´ç¯å¢ƒã€‚è¯·ç¨ç­‰ï¼Œæˆ‘å°†ä¸ºæ‚¨æä¾›è¯¦ç»†çš„ç©ºé—´åˆ†æå’Œå¯¼èˆªæŒ‡å¯¼ã€‚"
        else:
            return "Based on your photo, I'm analyzing the current spatial environment. Please wait while I provide you with detailed spatial analysis and navigation guidance."

def get_ai_enhanced_preset_output(caption: str, provider: str, site_id: str) -> str:
    """Get AI-enhanced preset output based on BLIP caption and textmap analysis"""
    
    print(f"ğŸ” get_ai_enhanced_preset_output called with: provider={provider}, site_id={site_id}")
    
    # Get matching data from textmap
    matching_data = get_matching_data(provider, site_id)
    print(f"ğŸ“Š Matching data keys: {list(matching_data.keys()) if matching_data else 'None'}")
    
    # Get detailed data if available
    detailed_data = get_detailed_matching_data(site_id) if site_id == "SCENE_A_MS" else []
    print(f"ğŸ“Š Detailed data count: {len(detailed_data)}")
    
    # Generate AI spatial reasoning
    ai_output = generate_ai_spatial_reasoning(caption, provider, site_id, matching_data, detailed_data)
    print(f"ğŸ¤– AI spatial reasoning output: {ai_output[:100]}...")
    
    return ai_output

# âœ… Enhanced preset output function that uses AI spatial reasoning
def get_enhanced_preset_output(caption: str, provider: str, site_id: str, use_ai: bool = True) -> str:
    """Get enhanced preset output with option to use AI spatial reasoning"""
    
    print(f"ğŸ” get_enhanced_preset_output called with: provider={provider}, site_id={site_id}, use_ai={use_ai}")
    
    if use_ai and caption:
        # Use AI spatial reasoning
        print("ğŸ¤– Using AI spatial reasoning for enhanced output")
        result = get_ai_enhanced_preset_output(caption, provider, site_id)
        print(f"ğŸ¤– AI-enhanced output result: {result[:100]}...")
        return result
    else:
        # Fall back to traditional preset output
        print("ğŸ“š Using traditional preset output")
        result = get_preset_output(provider, site_id)
        print(f"ğŸ“š Traditional preset output result: {result[:100]}...")
        return result

# ============================================================================
# âœ… New: DG Optimization API Endpoints
# ============================================================================

@app.post("/api/dg/metrics/collect")
async def collect_dg_metrics(
    session_id: str,
    metric_type: str,
    data: Dict[str, Any],
    priority: str = "normal",
    tags: List[str] = None
):
    """Collect DG optimization metrics"""
    try:
        # Convert string to enum
        metric_type_enum = MetricType(metric_type)
        priority_enum = DataPriority(priority)
        
        success = enhanced_metrics_collector.collect_real_time_data(
            metric_type_enum,
            session_id,
            data,
            priority_enum,
            tags or []
        )
        
        if not success:
            raise HTTPException(status_code=500, detail="Failed to collect metrics")
        
        return {
            "session_id": session_id,
            "metric_type": metric_type,
            "status": "collected",
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Metrics collection failed: {str(e)}")

@app.get("/api/dg/metrics/export/{session_id}")
async def export_session_metrics(session_id: str, format: str = "csv"):
    """Export session metrics"""
    try:
        if format.lower() == "csv":
            filename = f"session_{session_id}_metrics.csv"
            success = enhanced_metrics_collector.export_data_to_csv(filename, session_id=session_id)
        elif format.lower() == "json":
            filename = f"session_{session_id}_metrics.json"
            success = enhanced_metrics_collector.export_data_to_json(filename, session_id=session_id)
        else:
            raise HTTPException(status_code=400, detail="Invalid format. Use 'csv' or 'json'")
        
        if not success:
            raise HTTPException(status_code=500, detail="Export failed")
        
        return {
            "session_id": session_id,
            "format": format,
            "filename": filename,
            "status": "exported",
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Export failed: {str(e)}")

@app.get("/api/dg/metrics/analytics/{session_id}")
async def get_metrics_analytics(session_id: str):
    """Get analytics report for a session"""
    try:
        report = enhanced_metrics_collector.generate_analytics_report(session_id)
        return report
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analytics generation failed: {str(e)}")

@app.get("/api/dg/metrics/stats")
async def get_metrics_collection_stats():
    """Get metrics collection statistics"""
    try:
        stats = enhanced_metrics_collector.get_collection_stats()
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get collection stats: {str(e)}")

@app.post("/api/dg/metrics/session/{session_id}/close")
async def close_metrics_session(session_id: str):
    """Close a metrics collection session"""
    try:
        enhanced_metrics_collector.close_session(session_id)
        return {
            "session_id": session_id,
            "status": "closed",
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to close session: {str(e)}")

@app.post("/api/dg/evaluation/record")
async def record_dg_evaluation(
    session_id: str,
    design_goal: str,
    evaluation_data: Dict[str, Any]
):
    """Record DG evaluation data"""
    try:
        if not dg_evaluator:
            raise HTTPException(status_code=503, detail="DG evaluation not enabled")
        
        # Record evaluation data based on design goal
        if design_goal == "DG1":
            dg_evaluator.dg1_evaluator.record_setup_process(
                session_id=session_id,
                steps=evaluation_data.get("steps", []),
                time_taken=evaluation_data.get("time_taken", 0),
                success=evaluation_data.get("success", False)
            )
        elif design_goal == "DG2":
            dg_evaluator.dg2_evaluator.record_landmark_recall(
                session_id=session_id,
                task_id=evaluation_data.get("task_id", ""),
                landmarks_presented=evaluation_data.get("landmarks_presented", []),
                landmarks_recalled=evaluation_data.get("landmarks_recalled", []),
                time_delay=evaluation_data.get("time_delay", 0)
            )
        elif design_goal == "DG3":
            dg_evaluator.dg3_evaluator.record_interaction_behavior(
                session_id=session_id,
                photo_count=evaluation_data.get("photo_count", 0),
                first_photo_success=evaluation_data.get("first_photo_success", False),
                adjustment_actions=evaluation_data.get("adjustment_actions", [])
            )
        elif design_goal == "DG4":
            dg_evaluator.dg4_evaluator.record_task_completion(
                session_id=session_id,
                task_id=evaluation_data.get("task_id", ""),
                task_type=evaluation_data.get("task_type", ""),
                status=evaluation_data.get("status", "not_started"),
                completion_time=evaluation_data.get("completion_time", 0),
                veering_count=evaluation_data.get("veering_count", 0)
            )
        elif design_goal == "DG5":
            dg_evaluator.dg5_evaluator.record_clarification_dialogue(
                session_id=session_id,
                dialogue_id=evaluation_data.get("dialogue_id", ""),
                ambiguity_before=evaluation_data.get("ambiguity_before", 0),
                ambiguity_after=evaluation_data.get("ambiguity_after", 0),
                dialogue_count=evaluation_data.get("dialogue_count", 0)
            )
        elif design_goal == "DG6":
            dg_evaluator.dg6_evaluator.record_wcag_compliance(
                session_id=session_id,
                wcag_guideline=evaluation_data.get("wcag_guideline", ""),
                compliance_status=evaluation_data.get("compliance_status", ""),
                issues_found=evaluation_data.get("issues_found", [])
            )
        else:
            raise HTTPException(status_code=400, detail="Invalid design goal")
        
        return {
            "session_id": session_id,
            "design_goal": design_goal,
            "status": "recorded",
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Evaluation recording failed: {str(e)}")

@app.get("/api/dg/evaluation/report/{session_id}")
async def get_dg_evaluation_report(session_id: str):
    """Get comprehensive DG evaluation report"""
    try:
        if not dg_evaluator:
            raise HTTPException(status_code=503, detail="DG evaluation not enabled")
        
        report = dg_evaluator.generate_evaluation_report(session_id)
        return report
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Report generation failed: {str(e)}")

@app.post("/api/dg/accessibility/check")
async def check_accessibility_compliance(
    interface_elements: Dict[str, Any],
    target_level: str = "AA"
):
    """Check WCAG accessibility compliance"""
    try:
        if not accessibility_checker:
            raise HTTPException(status_code=503, detail="Accessibility checking not enabled")
        
        # Convert string to enum
        from accessibility_checker import WCAGLevel
        level_enum = WCAGLevel(target_level.upper())
        
        results = accessibility_checker.check_wcag_compliance(interface_elements, level_enum)
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Accessibility check failed: {str(e)}")

@app.post("/api/dg/accessibility/voiceover")
async def test_voiceover_compatibility(features: Dict[str, Any]):
    """Test VoiceOver compatibility"""
    try:
        if not accessibility_checker:
            raise HTTPException(status_code=503, detail="Accessibility checking not enabled")
        
        results = accessibility_checker.test_voiceover_compatibility(features)
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"VoiceOver test failed: {str(e)}")

@app.post("/api/dg/indoor_gml/generate")
async def generate_indoor_gml_map(
    site_data: Dict[str, Any],
    landmarks: List[Dict[str, Any]],
    connections: List[Dict[str, Any]]
):
    """Generate IndoorGML standard map"""
    try:
        if not indoor_gml_generator:
            raise HTTPException(status_code=503, detail="IndoorGML generation not enabled")
        
        gml_content = indoor_gml_generator.generate_indoor_gml(site_data, landmarks, connections)
        return {
            "status": "generated",
            "content_length": len(gml_content),
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"IndoorGML generation failed: {str(e)}")

@app.post("/api/dg/indoor_gml/validate")
async def validate_indoor_gml_compliance(gml_content: str):
    """Validate IndoorGML standard compliance"""
    try:
        if not indoor_gml_generator:
            raise HTTPException(status_code=503, detail="IndoorGML validation not enabled")
        
        validation_results = indoor_gml_generator.validate_standard_compliance(gml_content)
        return validation_results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"IndoorGML validation failed: {str(e)}")

@app.get("/api/dg/user_needs/validation/{session_id}")
async def get_user_needs_validation(session_id: str):
    """Get user needs validation report"""
    try:
        report = user_needs_validator.generate_comprehensive_report(session_id)
        return report
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"User needs validation failed: {str(e)}")

@app.post("/api/dg/user_needs/record")
async def record_user_needs_data(
    session_id: str,
    user_need: str,
    metric_name: str,
    value: Any
):
    """Record user needs validation data"""
    try:
        # Convert string to enum
        need_enum = UserNeed(user_need)
        
        user_needs_validator.record_validation_data(session_id, need_enum, metric_name, value)
        return {
            "session_id": session_id,
            "user_need": user_need,
            "metric_name": metric_name,
            "status": "recorded",
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"User needs recording failed: {str(e)}")

@app.get("/api/dg/user_needs/matrix")
async def get_user_needs_matrix():
    """Get user needs to design goals mapping matrix"""
    try:
        matrix = user_needs_validator.get_requirement_goal_matrix()
        return {
            "matrix": matrix,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get matrix: {str(e)}")

# ============================================================================
# âœ… Enhanced Health Check with DG Optimization Status
# ============================================================================

@app.get("/health/enhanced")
async def enhanced_health_check():
    """Enhanced health check with DG optimization status"""
    base_health = await health_check()
    
    dg_status = {
        "dg_evaluation": {
            "enabled": ENABLE_DG_EVALUATION,
            "status": "available" if dg_evaluator else "disabled"
        },
        "accessibility_checking": {
            "enabled": ENABLE_ACCESSIBILITY_CHECKING,
            "status": "available" if accessibility_checker else "disabled"
        },
        "indoor_gml": {
            "enabled": ENABLE_INDOOR_GML,
            "status": "available" if indoor_gml_generator else "disabled"
        },
        "enhanced_metrics": {
            "status": "available" if enhanced_metrics_collector else "disabled",
            "storage_path": METRICS_STORAGE_PATH
        },
        "user_needs_validation": {
            "status": "available" if user_needs_validator else "disabled"
        }
    }
    
    return {
        **base_health,
        "dg_optimization": dg_status
    }

def generate_scene_a_structure_info(node_id: str, lang: str = "en") -> str:
    """Generate structure-based location information for SCENE_A_MS from Sense_A_Finetuned.fixed.jsonl"""
    if lang == "zh":
        # ä¸­æ–‡å¯¼èˆªæŒ‡ä»¤ - åŸºäºStructureæ–‡ä»¶çš„æ‹“æ‰‘ä¿¡æ¯
        if node_id == "dp_ms_entrance":
            return "æ‚¨åœ¨Maker Spaceå…¥å£ã€‚ç›´è¡Œçº¦6æ­¥åˆ°è¾¾3Dæ‰“å°æœºæ¡Œï¼Œç„¶åå·¦è½¬ç»§ç»­å‰è¿›è¿›å…¥ä¸­åº­ã€‚"
        
        elif node_id == "yline_start":
            return "æ‚¨åœ¨é»„è‰²å¼•å¯¼çº¿èµ·ç‚¹ã€‚ç›´è¡Œçº¦3æ­¥åˆ°è¾¾æ¤…å­ä½ç½®ï¼Œç„¶åç»§ç»­æ²¿å¼•å¯¼çº¿å‰è¿›ã€‚"
        
        elif node_id == "chair_on_yline":
            return "æ‚¨åœ¨é»„è‰²å¼•å¯¼çº¿ä¸Šçš„æ¤…å­æ—ã€‚ç»§ç»­ç›´è¡Œçº¦2æ­¥ï¼Œå¼•å¯¼çº¿å°†å‘å·¦å¼¯æ›²ã€‚"
        
        elif node_id == "yline_bend_mid":
            return "æ‚¨åœ¨é»„è‰²å¼•å¯¼çº¿å¼¯æ›²å¤„ã€‚ç›´è¡Œçº¦7æ­¥åˆ°è¾¾çª—æˆ·å’Œè½¯åº§åŒºåŸŸï¼Œç„¶åè¿›å…¥ä¸­åº­ã€‚"
        
        elif node_id == "atrium_edge":
            return "æ‚¨åœ¨ä¸­åº­è¾¹ç¼˜ï¼Œé è¿‘çª—æˆ·å’Œè½¯åº§ã€‚å³è½¬çº¦5æ­¥åˆ°è¾¾ç”µè§†åŒºåŸŸã€‚"
        
        elif node_id == "tv_zone":
            return "æ‚¨åœ¨ç”µè§†åŒºåŸŸã€‚å‰æ–¹çº¦4æ­¥åˆ°è¾¾å°ä¼šè®®æ¡Œï¼Œç„¶åç»§ç»­å‰è¿›åˆ°æ©™è‰²æ²™å‘ã€‚"
        
        elif node_id == "small_table_mid":
            return "æ‚¨åœ¨ä½çŸ®ä¼šè®®æ¡Œæ—ã€‚ç›´è¡Œçº¦3æ­¥åˆ°è¾¾æ©™è‰²æ²™å‘è§’è½ã€‚"
        
        elif node_id == "orange_sofa_corner":
            return "æ‚¨å·²åˆ°è¾¾æ©™è‰²æ²™å‘è§’è½ï¼Œé å¢™æ”¾ç½®ã€‚å¯¼èˆªä»»åŠ¡å®Œæˆï¼"
        
        else:
            return f"å½“å‰ä½ç½®ï¼š{node_id}ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨è¦å»å“ªé‡Œã€‚"
    
    else:
        # English navigation instructions - based on Structure file topology
        if node_id == "dp_ms_entrance":
            return "You are at the Maker Space entrance. Walk straight about 6 steps to reach the 3D printer table, then turn left to continue into the atrium."
        
        elif node_id == "yline_start":
            return "You are at the yellow line start. Walk straight about 3 steps to reach the chair position, then continue along the guide line."
        
        elif node_id == "chair_on_yline":
            return "You are at the chair on the yellow line. Continue straight about 2 steps, the line will bend left."
        
        elif node_id == "yline_bend_mid":
            return "You are at the yellow line bend. Walk straight about 7 steps to reach the windows and soft seats, then enter the atrium."
        
        elif node_id == "atrium_edge":
            return "You are at the atrium edge, near windows and soft seats. Turn right about 5 steps to reach the TV zone."
        
        elif node_id == "tv_zone":
            return "You are in the TV zone. Walk forward about 4 steps to reach the small meeting table, then continue to the orange sofa."
        
        elif node_id == "small_table_mid":
            return "You are at the low meeting table. Walk straight about 3 steps to reach the orange sofa corner."
        
        elif node_id == "orange_sofa_corner":
            return "You have reached the orange sofa corner, against the wall. Navigation task completed!"
        
        else:
            return f"Current location: {node_id}. Please tell me where you want to go."

def generate_scene_b_structure_info(node_id: str, lang: str = "en") -> str:
    """Generate structure-based location information for SCENE_B_STUDIO from Sense_B_Finetuned.fixed.jsonl"""
    if lang == "zh":
        # ä¸­æ–‡å¯¼èˆªæŒ‡ä»¤ - åŸºäºStructureæ–‡ä»¶çš„æ‹“æ‰‘ä¿¡æ¯
        if node_id == "atrium_desks_hub":
            return "æ‚¨åœ¨ä¸­å¤®å·¥ä½œå°åŒºåŸŸï¼Œé¢å‘å¤§ç”µè§†å±å¹•ã€‚å·¦ä¾§æ˜¯çª—æˆ·å¢™ï¼Œå³ä¾§æ˜¯æ©™è‰²æ²™å‘åŒºåŸŸã€‚"
        
        elif node_id == "node_left_to_windows":
            return "æ‚¨æ­£åœ¨å‘å·¦è½¬å‘çª—æˆ·æ–¹å‘ã€‚ç›´è¡Œçº¦2æ­¥åˆ°è¾¾çª—æˆ·è¾¹ç¼˜çš„è½¯åº§åŒºåŸŸã€‚"
        
        elif node_id == "atrium_windows_edge":
            return "æ‚¨åœ¨çª—æˆ·è¾¹ç¼˜çš„è½¯åº§åŒºåŸŸã€‚å‘åè½¬çº¦3æ­¥åˆ°è¾¾å°ç™½è‰²ä¼šè®®æ¡Œï¼Œç„¶åç»§ç»­å‰è¿›åˆ°æ©™è‰²æ²™å‘ã€‚"
        
        elif node_id == "poi_small_table":
            return "æ‚¨åœ¨ç™½è‰²ä¼šè®®æ¡Œæ—ï¼Œæ¡Œä¸Šæœ‰ç´«è‰²æ¤…å­ã€‚ç›´è¡Œçº¦2æ­¥åˆ°è¾¾æ©™è‰²æ²™å‘åŒºåŸŸã€‚"
        
        elif node_id == "poi_orange_green_sofa":
            return "æ‚¨å·²åˆ°è¾¾æ©™è‰²æ²™å‘åŒºåŸŸï¼Œæ²™å‘é å¢™æ”¾ç½®ã€‚é™„è¿‘æœ‰ç»¿è‰²é«˜èƒŒæ‰¶æ‰‹æ¤…å’Œä¸¤ä¸ªé»‘è‰²è¾¹æ¡Œã€‚"
        
        else:
            return f"å½“å‰ä½ç½®ï¼š{node_id}ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨è¦å»å“ªé‡Œã€‚"
    
    else:
        # English navigation instructions - based on Structure file topology
        if node_id == "atrium_desks_hub":
            return "You are at the central desks hub, facing the large TV screen. To your left is the windows wall, to your right is the orange sofa area."
        
        elif node_id == "node_left_to_windows":
            return "You are turning left toward the windows. Walk straight about 2 steps to reach the soft seats area at the window edge."
        
        elif node_id == "atrium_windows_edge":
            return "You are at the soft seats area by the windows. Turn around and walk about 3 steps to reach the small white meeting table, then continue to the orange sofa."
        
        elif node_id == "poi_small_table":
            return "You are at the white meeting table with purple chairs. Walk straight about 2 steps to reach the orange sofa area."
        
        elif node_id == "poi_orange_green_sofa":
            return "You have reached the orange sofa area, with the sofa against the wall. Nearby are a green high-back armchair and two black side tables."
        
        else:
            return f"Current location: {node_id}. Please tell me where you want to go."

# âœ… New: AI Spatial Reasoning System to Replace Preset Outputs