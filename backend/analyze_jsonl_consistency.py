#!/usr/bin/env python3
"""
åˆ†æå››ä¸ªJSONLæ–‡ä»¶çš„ä¸€è‡´æ€§å’Œæ½œåœ¨é—®é¢˜
æ£€æŸ¥æ˜¯å¦ä¸ä¹‹å‰çš„User Needså’ŒDGsä¿®æ”¹ä¿æŒä¸€è‡´
"""

import json
import os
from typing import Dict, List, Any

def load_jsonl_file(filepath: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ–‡ä»¶"""
    try:
        data = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        data.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
        return data
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        return []

def analyze_structure_file(data: Dict[str, Any], filename: str) -> Dict[str, Any]:
    """åˆ†æStructureæ–‡ä»¶çš„ç»“æ„å’Œå†…å®¹"""
    print(f"\nğŸ” åˆ†æStructureæ–‡ä»¶: {filename}")
    print("=" * 60)
    
    analysis = {
        "filename": filename,
        "has_topology": False,
        "has_nodes": False,
        "has_edges": False,
        "has_landmarks": False,
        "has_retrieval": False,
        "node_count": 0,
        "edge_count": 0,
        "landmark_count": 0,
        "retrieval_fields": [],
        "evaluation_hooks": [],
        "accessibility_fields": [],
        "issues": []
    }
    
    # æ£€æŸ¥åŸºæœ¬ç»“æ„
    if "topology" in data:
        analysis["has_topology"] = True
        topology = data["topology"]
        
        # æ£€æŸ¥èŠ‚ç‚¹
        if "nodes" in topology:
            analysis["has_nodes"] = True
            analysis["node_count"] = len(topology["nodes"])
            
            # åˆ†æç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯
            if topology["nodes"]:
                first_node = topology["nodes"][0]
                print(f"ğŸ“Š èŠ‚ç‚¹æ•°é‡: {analysis['node_count']}")
                print(f"ğŸ“ ç¤ºä¾‹èŠ‚ç‚¹: {first_node.get('id', 'Unknown')}")
                
                # æ£€æŸ¥æ£€ç´¢å­—æ®µ
                if "retrieval" in first_node:
                    analysis["has_retrieval"] = True
                    retrieval = first_node["retrieval"]
                    analysis["retrieval_fields"] = list(retrieval.keys())
                    
                    print(f"ğŸ” æ£€ç´¢å­—æ®µ: {analysis['retrieval_fields']}")
                    if "index_terms" in retrieval:
                        print(f"   index_terms: {retrieval['index_terms']}")
                    if "tags" in retrieval:
                        print(f"   tags: {retrieval['tags']}")
                
                # æ£€æŸ¥è¯„ä¼°é’©å­
                if "evaluation_hooks" in first_node:
                    hooks = first_node["evaluation_hooks"]
                    analysis["evaluation_hooks"] = hooks.get("tags", [])
                    print(f"ğŸ·ï¸ è¯„ä¼°æ ‡ç­¾: {analysis['evaluation_hooks']}")
                
                # æ£€æŸ¥æ— éšœç¢å­—æ®µ
                if "accessibility" in first_node:
                    accessibility = first_node["accessibility"]
                    analysis["accessibility_fields"] = list(accessibility.keys())
                    print(f"â™¿ æ— éšœç¢å­—æ®µ: {analysis['accessibility_fields']}")
        
        # æ£€æŸ¥è¾¹
        if "edges" in topology:
            analysis["has_edges"] = True
            analysis["edge_count"] = len(topology["edges"])
            print(f"ğŸ”„ è¾¹æ•°é‡: {analysis['edge_count']}")
        
        # æ£€æŸ¥åœ°æ ‡
        if "landmarks" in topology:
            analysis["has_landmarks"] = True
            analysis["landmark_count"] = len(topology["landmarks"])
            print(f"ğŸ›ï¸ åœ°æ ‡æ•°é‡: {analysis['landmark_count']}")
    
    # æ£€æŸ¥å…¶ä»–é‡è¦å­—æ®µ
    if "retrieval" in data:
        global_retrieval = data["retrieval"]
        print(f"ğŸŒ å…¨å±€æ£€ç´¢å­—æ®µ: {list(global_retrieval.keys())}")
        if "cnl_index" in global_retrieval:
            print(f"   cnl_index: {len(global_retrieval['cnl_index'])} é¡¹")
        if "keywords" in global_retrieval:
            print(f"   keywords: {global_retrieval['keywords']}")
    
    # æ£€æŸ¥è¾“å‡ºå­—æ®µ
    if "output" in data:
        output = data["output"]
        print(f"ğŸ“ è¾“å‡ºé•¿åº¦: {len(output)} å­—ç¬¦")
        print(f"   å‰100å­—ç¬¦: {output[:100]}...")
    
    return analysis

def analyze_detail_file(data: List[Dict[str, Any]], filename: str) -> Dict[str, Any]:
    """åˆ†æDetailæ–‡ä»¶çš„ç»“æ„å’Œå†…å®¹"""
    print(f"\nğŸ” åˆ†æDetailæ–‡ä»¶: {filename}")
    print("=" * 60)
    
    if not data:
        return {"filename": filename, "error": "æ–‡ä»¶ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥"}
    
    analysis = {
        "filename": filename,
        "total_entries": len(data),
        "has_node_hint": 0,
        "has_spatial_relations": 0,
        "has_unique_features": 0,
        "has_fusion": 0,
        "node_hint_values": set(),
        "issues": []
    }
    
    # åˆ†æç¬¬ä¸€ä¸ªæ¡ç›®
    first_entry = data[0]
    print(f"ğŸ“Š æ€»æ¡ç›®æ•°: {analysis['total_entries']}")
    print(f"ğŸ“ ç¤ºä¾‹æ¡ç›®ID: {first_entry.get('id', 'Unknown')}")
    
    # æ£€æŸ¥å…³é”®å­—æ®µ
    for entry in data:
        if "node_hint" in entry:
            analysis["has_node_hint"] += 1
            analysis["node_hint_values"].add(entry["node_hint"])
        
        if "spatial_relations" in entry:
            analysis["has_spatial_relations"] += 1
        
        if "unique_features" in entry:
            analysis["has_unique_features"] += 1
        
        if "fusion" in entry:
            analysis["has_fusion"] += 1
    
    print(f"ğŸ”— åŒ…å«node_hintçš„æ¡ç›®: {analysis['has_node_hint']}/{analysis['total_entries']}")
    print(f"ğŸ§­ åŒ…å«spatial_relationsçš„æ¡ç›®: {analysis['has_spatial_relations']}/{analysis['total_entries']}")
    print(f"â­ åŒ…å«unique_featuresçš„æ¡ç›®: {analysis['has_unique_features']}/{analysis['total_entries']}")
    print(f"ğŸ”„ åŒ…å«fusionçš„æ¡ç›®: {analysis['has_fusion']}/{analysis['total_entries']}")
    
    if analysis["node_hint_values"]:
        print(f"ğŸ¯ node_hintå€¼: {sorted(list(analysis['node_hint_values']))}")
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    if analysis["has_node_hint"] < analysis["total_entries"]:
        analysis["issues"].append("éƒ¨åˆ†æ¡ç›®ç¼ºå°‘node_hintå­—æ®µ")
    
    if analysis["has_spatial_relations"] < analysis["total_entries"]:
        analysis["issues"].append("éƒ¨åˆ†æ¡ç›®ç¼ºå°‘spatial_relationså­—æ®µ")
    
    return analysis

def check_consistency_between_files(structure_analysis: Dict, detail_analysis: Dict) -> Dict[str, Any]:
    """æ£€æŸ¥Structureå’ŒDetailæ–‡ä»¶ä¹‹é—´çš„ä¸€è‡´æ€§"""
    print(f"\nğŸ”— æ£€æŸ¥Structureå’ŒDetailæ–‡ä»¶çš„ä¸€è‡´æ€§")
    print("=" * 60)
    
    consistency = {
        "node_hint_coverage": 0.0,
        "potential_mismatches": [],
        "recommendations": []
    }
    
    # æ£€æŸ¥node_hintè¦†ç›–ç‡
    if "node_hint_values" in detail_analysis and "node_count" in structure_analysis:
        detail_nodes = detail_analysis["node_hint_values"]
        structure_nodes = structure_analysis["node_count"]
        
        if structure_nodes > 0:
            consistency["node_hint_coverage"] = len(detail_nodes) / structure_nodes
            print(f"ğŸ“Š node_hintè¦†ç›–ç‡: {consistency['node_hint_coverage']:.2%} ({len(detail_nodes)}/{structure_nodes})")
        
        # æ£€æŸ¥æ½œåœ¨çš„èŠ‚ç‚¹IDä¸åŒ¹é…
        if "node_ids" in structure_analysis:
            structure_node_ids = set(structure_analysis["node_ids"])
            missing_nodes = structure_node_ids - detail_nodes
            extra_nodes = detail_nodes - structure_node_ids
            
            if missing_nodes:
                consistency["potential_mismatches"].append(f"Detailæ–‡ä»¶ç¼ºå°‘èŠ‚ç‚¹: {missing_nodes}")
                print(f"âš ï¸ Detailæ–‡ä»¶ç¼ºå°‘èŠ‚ç‚¹: {missing_nodes}")
            
            if extra_nodes:
                consistency["potential_mismatches"].append(f"Detailæ–‡ä»¶åŒ…å«æœªçŸ¥èŠ‚ç‚¹: {extra_nodes}")
                print(f"âš ï¸ Detailæ–‡ä»¶åŒ…å«æœªçŸ¥èŠ‚ç‚¹: {extra_nodes}")
    
    # ç”Ÿæˆå»ºè®®
    if consistency["node_hint_coverage"] < 0.8:
        consistency["recommendations"].append("å»ºè®®å¢åŠ Detailæ–‡ä»¶çš„node_hintè¦†ç›–ç‡")
    
    if detail_analysis.get("has_spatial_relations", 0) < detail_analysis.get("total_entries", 0):
        consistency["recommendations"].append("å»ºè®®ä¸ºæ‰€æœ‰Detailæ¡ç›®æ·»åŠ spatial_relations")
    
    if detail_analysis.get("has_unique_features", 0) < detail_analysis.get("total_entries", 0):
        consistency["recommendations"].append("å»ºè®®ä¸ºæ‰€æœ‰Detailæ¡ç›®æ·»åŠ unique_features")
    
    return consistency

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åˆ†æJSONLæ–‡ä»¶çš„ä¸€è‡´æ€§")
    print("=" * 80)
    
    data_dir = "data"
    
    # åˆ†æStructureæ–‡ä»¶
    structure_files = [
        "Sense_A_Finetuned.fixed.jsonl",
        "Sense_B_Finetuned.fixed.jsonl"
    ]
    
    # åˆ†æDetailæ–‡ä»¶
    detail_files = [
        "Sense_A_MS.jsonl",
        "Sense_B_Studio.jsonl"
    ]
    
    all_analyses = {}
    
    # åˆ†æStructureæ–‡ä»¶
    for filename in structure_files:
        filepath = os.path.join(data_dir, filename)
        if os.path.exists(filepath):
            data = load_jsonl_file(filepath)
            if data:
                analysis = analyze_structure_file(data[0], filename)
                all_analyses[filename] = analysis
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    
    # åˆ†æDetailæ–‡ä»¶
    for filename in detail_files:
        filepath = os.path.join(data_dir, filename)
        if os.path.exists(filepath):
            data = load_jsonl_file(filepath)
            if data:
                analysis = analyze_detail_file(data, filename)
                all_analyses[filename] = analysis
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    
    # æ£€æŸ¥ä¸€è‡´æ€§
    print(f"\nğŸ“‹ åˆ†ææ€»ç»“")
    print("=" * 80)
    
    for filename, analysis in all_analyses.items():
        if "error" not in analysis:
            print(f"âœ… {filename}: åˆ†æå®Œæˆ")
        else:
            print(f"âŒ {filename}: {analysis['error']}")
    
    # ç”Ÿæˆå»ºè®®
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®")
    print("=" * 80)
    
    recommendations = []
    for filename, analysis in all_analyses.items():
        if "issues" in analysis and analysis["issues"]:
            print(f"ğŸ”§ {filename}:")
            for issue in analysis["issues"]:
                print(f"   - {issue}")
                recommendations.append(f"{filename}: {issue}")
    
    if not recommendations:
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶éƒ½ç¬¦åˆé¢„æœŸç»“æ„ï¼")
    
    print(f"\nğŸ“Š åˆ†æå®Œæˆï¼å…±åˆ†æäº† {len(all_analyses)} ä¸ªæ–‡ä»¶")

if __name__ == "__main__":
    main()
